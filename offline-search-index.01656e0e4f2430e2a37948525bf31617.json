[{"body":" This section contains detailed brainstorming and analysis on various documentation topics, strategies, and technologies. It serves as a space for exploring ideas, evaluating options, and documenting thought processes related to the development and maintenance of this documentation site.\n1. Available Guides Static Site Generation Migration Analysis - Analysis of migrating from Docsify to an SEO-optimized static site generator 2. Getting Started Select a guide from the sidebar to begin.\n","categories":"","description":"In-depth brainstorming and analysis of documentation topics and strategies","excerpt":"In-depth brainstorming and analysis of documentation topics and …","ref":"/my-documents/docs/brainstorming/","tags":"","title":"Brainstorming"},{"body":" Learn how to write efficient, maintainable, and robust Bash scripts with these comprehensive guides covering basic practices, Linux commands, and testing.\n1. What You’ll Learn This section covers:\nBasic Best Practices - Foundational best practices for writing Bash scripts Linux Commands Best Practices - Effective use of Linux commands in scripts Bats Testing Framework - Testing Bash scripts with the Bats framework 2. Getting Started Choose a topic from the sidebar to begin learning about Bash scripting best practices.\n","categories":"","description":"Best practices for writing Bash scripts","excerpt":"Best practices for writing Bash scripts","ref":"/my-documents/docs/bash-scripts/","tags":"","title":"Bash Scripts"},{"body":" 1. External references 2. General best practices 3. escape quotes 4. Bash environment options 4.1. errexit (set -e | set -o errexit) 4.1.1. Caveats with command substitution 4.1.2. Caveats with process substitution 4.1.3. Process substitution is asynchronous 4.2. pipefail (set -o pipefail) 4.3. errtrace (set -E | set -o errtrace) 4.4. nounset (set -u | set -o nounset) 4.5. inherit error exit code in sub shells 4.6. posix (set -o posix) 5. Main function 6. Arguments 7. some commands default options to use 8. Variables 8.1. Variable declaration 8.2. variable naming convention 8.3. Variable expansion 8.3.1. Examples 8.4. Check if a variable is defined 8.5. Variable default value 8.6. Passing variable by reference to function 8.6.1. Example 1 8.6.2. Example 2 9. Capture output 9.1. Capture output and test result 9.2. Capture output and retrieve status code 10. Array 11. Temporary directory 12. Traps 13. Deal with SIGPIPE - exit code 141 14. Performances analysis 15. Bash Performance tips 15.1. Array::wrap2 performance improvement 1. External references pure bash bible pure sh(posix) bible 2. General best practices cat \u003c\u003c 'EOF' avoid to interpolate variables\nuse builtin cd instead of cd, builtin pwd instead of pwd, … to avoid using customized aliased commands by the user In this framework, I added the command unalias -a || true to remove all eventual aliases and also ensure to disable aliases expansion by using shopt -u expand_aliases. Because aliases have a very special way to load. In a script file changing an alias doesn’t occur immediately, it depends if script evaluated has been parsed yet or not. And alias changed in a function, will be applied outside of the function. But I experienced some trouble with this last rule, so I give up using aliases.\nuse the right shebang, avoid #!/bin/bash as bash binary could be in another folder (especially on alpine), use this instead #!/usr/bin/env bash\nprefer to use printf vs echo\navoid global variables whenever possible, prefer using local\ncheck that every lowercase variable is used as local in functions https://github.com/bats-core/bats-core/issues/726 https://github.com/koalaman/shellcheck/issues/1395 https://github.com/koalaman/shellcheck/issues/468 avoid to export variables whenever possible\n3. escape quotes help='quiet mode, doesn'\\''t display any output' # alternative help=\"quiet mode, doesn't display any output\" 4. Bash environment options See Set bash builtin documentation\nThis framework uses these mode by default:\nerrexit pipefail errtrace 4.1. errexit (set -e | set -o errexit) Check official doc but it can be summarized like this:\nExit immediately command returns a non-zero status.\nI was considering this as a best practice because every non controlled command failure will stop your program. But actually\nsometimes you need or expect a command to fail Eg1: delete a folder that actually doesn’t exists. Use || true to ignore the error.\nrm -Rf folder || true Eg2: a command that expects to fail if conditions are not met. Using if will not stop the program on non-zero exit code.\nif git diff-index --quiet HEAD --; then Log::displayInfo \"Pull git repository '${dir}' as no changes detected\" git pull --progress return 0 else Log::displayWarning \"Pulling git repository '${dir}' avoided as changes detected\" fi actually this feature is not well implemented everywhere\nsometimes some commands that should fail doesn’t fail the feature is not homogeneous across implementations some commands expects to have non zero exit code some commands exits with non zero error code but does not necessarily needs the program to exit Finally it is preferable to check every command status code manually instead of relying to an automatic management.\n4.1.1. Caveats with command substitution #!/bin/bash set -o errexit echo $(exit 1) echo $? Output:\n0 it is because echo has succeeded. the same result occurs even with shopt -s inherit_errexit (see below).\nThe best practice is to always assign command substitution to variable:\n#!/bin/bash set -o errexit declare cmdOut cmdOut=$(exit 1) echo \"${cmdOut}\" echo $? Outputs nothing because the script stopped before variable affectation, return code is 1.\n4.1.2. Caveats with process substitution Consider this example that reads each line of the output of the command passed using process substitution in \u003c(...)\nparse() { local scriptFile=\"$1\" local implementDirective while IFS='' read -r implementDirective; do echo \"${implementDirective}\" done \u003c \u003c(grep -E -e \"^# IMPLEMENT .*$\" \"${scriptFile}\") } If we execute this command with a non existent file, even if errexit, pipefail and inherit_errexit are set, the command will actually succeed.\nIt is because process substitution launch the command as as separated process. I didn’t find any clean way to manage this using process substitution (only workaround I found was to pass by file to pass the exit code to parent process).\nSo here the solution removing process substitution\nparse() { local scriptFile=\"$1\" local implementDirective grep -E -e \"^# IMPLEMENT .*$\" \"${scriptFile}\" | while IFS='' read -r implementDirective; do echo \"${implementDirective}\" done } But how to use readarray without using process substitution. Old code was:\ndeclare -a interfacesFunctions readarray -t interfacesFunctions \u003c \u003c(Compiler::Implement::mergeInterfacesFunctions \"${COMPILED_FILE2}\") Compiler::Implement::validateInterfaceFunctions \\ \"${COMPILED_FILE2}\" \"${INPUT_FILE}\" \"${interfacesFunctions[@]}\" I first think about doing this\ndeclare -a interfacesFunctions Compiler::Implement::mergeInterfacesFunctions \"${COMPILED_FILE2}\" | readarray -t interfacesFunctions But interfacesFunctions was empty because readarray is run in another process, to avoid this issue, I could have used shopt -s lastpipe\nBut I finally transformed it to (the array in the same sub-shell so no issue):\nCompiler::Implement::mergeInterfacesFunctions \"${COMPILED_FILE2}\" | { declare -a interfacesFunctions readarray -t interfacesFunctions Compiler::Implement::validateInterfaceFunctions \\ \"${COMPILED_FILE2}\" \"${INPUT_FILE}\" \"${interfacesFunctions[@]}\" } The issue with this previous solution is that commands runs in a sub-shell but using shopt -s lastpipe could solve this issue.\nAnother solution would be to simply read the array from stdin:\ndeclare -a interfacesFunctions readarray -t interfacesFunctions \u003c\u003c\u003c\"$( Compiler::Implement::mergeInterfacesFunctions \"${COMPILED_FILE2}\" )\" Compiler::Implement::validateInterfaceFunctions \\ \"${COMPILED_FILE2}\" \"${INPUT_FILE}\" \"${interfacesFunctions[@]}\" 4.1.3. Process substitution is asynchronous it is why you cannot retrieve the status code, a way to do that is to wait the process to finish\nwhile read -r line; do echo \"$line\" \u0026 done \u003c \u003c( echo 1 sleep 1 echo 2 sleep 1 exit 77 ) could be rewritten in\nmapfile -t lines \u003c \u003c( echo 1 sleep 1 echo 2 sleep 1 exit 77 ) wait $! for line in \"${lines[@]}\"; do echo \"$line\" \u0026 done sleep 1 wait $! echo done 4.2. pipefail (set -o pipefail) https://dougrichardson.us/notes/fail-fast-bash-scripting.html\nIf set, the return value of a pipeline is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands in the pipeline exit successfully. This option is disabled by default.\nIt is complementary with errexit, as if it not activated, the failure of command in pipe could hide the error.\nEg: without pipefail this command succeed\n#!/bin/bash set -o errexit set +o pipefail # deactivate pipefail mode foo | echo \"a\" # 'foo' is a non-existing command # Output: # a # bash: foo: command not found # echo $? # exit code is 0 # 0 4.3. errtrace (set -E | set -o errtrace) https://dougrichardson.us/notes/fail-fast-bash-scripting.html\nIf set, any trap on ERR is inherited by shell functions, command substitutions, and commands executed in a subShell environment. The ERR trap is normally not inherited in such cases.\n4.4. nounset (set -u | set -o nounset) https://dougrichardson.us/notes/fail-fast-bash-scripting.html\nTreat unset variables and parameters other than the special parameters ‘@’ or ‘’, or array variables subscripted with ‘@’ or ‘’, as an error when performing parameter expansion. An error message will be written to the standard error, and a non-interactive shell will exit.\n4.5. inherit error exit code in sub shells https://dougrichardson.us/notes/fail-fast-bash-scripting.html\nlet’s see why using shopt -s inherit_errexit ?\nset -e does not affect subShells created by Command Substitution. This rule is stated in Command Execution Environment:\nsubShells spawned to execute command substitutions inherit the value of the -e option from the parent shell. When not in POSIX mode, Bash clears the -e option in such subShells.\nThis rule means that the following script will run to completion, in spite of INVALID_COMMAND.\n#!/bin/bash # command-substitution.sh set -e MY_VAR=$( echo -n Start INVALID_COMMAND echo -n End ) echo \"MY_VAR is $MY_VAR\" Output:\n./command-substitution.sh: line 4: INVALID_COMMAND: command not found MY_VAR is StartEnd shopt -s inherit_errexit, added in Bash 4.4 allows you to have command substitution parameters inherit your set -e from the parent script.\nFrom the Shopt Builtin documentation:\nIf set, command substitution inherits the value of the errexit option, instead of unsetting it in the subShell environment. This option is enabled when POSIX mode is enabled.\nSo, modifying command-substitution.sh above, we get:\n#!/bin/bash # command-substitution-inherit_errexit.sh set -e shopt -s inherit_errexit MY_VAR=$( echo -n Start INVALID_COMMAND echo -n End ) echo \"MY_VAR is $MY_VAR\" Output:\n./command-substitution-inherit_errexit.sh: line 5: INVALID_COMMAND: command not found 4.6. posix (set -o posix) Change the behavior of Bash where the default operation differs from the POSIX standard to match the standard (see Bash POSIX Mode). This is intended to make Bash behave as a strict superset of that standard.\n5. Main function An important best practice is to always encapsulate all your script inside a main function. One reason for this technique is to make sure the script does not accidentally do anything nasty in the case where the script is truncated. I often had this issue because when I change some of my bash framework functions, the pre-commit runs buildBinFiles command that can be recompiled itself. In this case the script fails.\nanother reason for doing this is to not execute the file at all if there is a syntax error.\nAdditionally you can add a snippet in order to avoid your function to be executed in the case where it is being source. The following code will execute main function if called as a script passing arguments, or will just import the main function if the script is sourced. See this stack overflow for more details\n#!/usr/bin/env bash main() { # main script set -eo pipefail } BASH_SOURCE=\".$0\" [[ \".$0\" != \".$BASH_SOURCE\" ]] || main \"$@\" 6. Arguments to construct complex command line, prefer to use an array declare -a cmd=(git push origin :${branch}) then you can display the result using echo \"${cmd[*]}\" you can execute the command using \"${cmd[@]}\" boolean arguments, to avoid seeing some calls like this myFunction 0 1 0 with 3 boolean values. prefer to provide constants(using readonly) to make the call more clear like myFunction arg1False arg2True arg3False of course replacing argX with the real argument name. Eg: Filters::directive \"${FILTER_DIRECTIVE_REMOVE_HEADERS}\" You have to prefix all your constants to avoid conflicts. instead of adding a new arg to the function with a default value, consider using an env variable that can be easily overridden before calling the function. Eg: SUDO=sudo Github::upgradeRelease ... It avoids to have to pass previous arguments that were potentially defaulted. 7. some commands default options to use Check out 10-LinuxCommands-BestPractices.md\n8. Variables 8.1. Variable declaration ensure we don’t have any globals, all variables should be passed to the functions declare all variables as local in functions to avoid making them global local or declare multiple local a z export readonly does not work, first readonly then export avoid using export most of the times, export is needed only when variables has to be passed to child process. 8.2. variable naming convention env variable that aims to be exported should be capitalized with underscore local variables should conform to camelCase 8.3. Variable expansion Shell Parameter Expansion\n${PARAMETER:-WORD} vs ${PARAMETER-WORD}:\nIf the parameter PARAMETER is unset (was never defined) or null (empty), ${PARAMETER:-WORD} expands to WORD, otherwise it expands to the value of PARAMETER, as if it just was ${PARAMETER}.\nIf you omit the :(colon) like in ${PARAMETER-WORD}, the default value is only used when the parameter is unset, not when it was empty.\n:warning: use this latter syntax when using function arguments in order to be able to reset a value to empty string, otherwise default value would be applied.\n8.3.1. Examples Extract directory from full file path: directory=\"${REAL_SCRIPT_FILE%/*}\"\nExtract file name from full file path: fileName=\"${REAL_SCRIPT_FILE##*/}\"\n8.4. Check if a variable is defined if [[ -z ${varName+xxx} ]]; then # varName is not set fi Alternatively you can use this framework function Assert::varExistsAndNotEmpty\n8.5. Variable default value Always consider to set a default value to the variable that you are using.\nEg.: Let’s see this dangerous example\n# Don't Do that !!!! rm -Rf \"${TMPDIR}/etc\" || true This could end very badly if your script runs as root and if ${TMPDIR} is not set, this script will result to do a rm -Rf /etc\nInstead you can do that\nrm -Rf \"${TMPDIR:-/tmp}/etc\" || true 8.6. Passing variable by reference to function Always “scope” variables passed by reference. Scoping in bash means to find a name that is a low probability that the caller of the method names the parameter with the same name as in the function.\n8.6.1. Example 1 Array::setArray() { local -n arr=$1 local IFS=$2 - # set no glob feature set -f # shellcheck disable=SC2206,SC2034 arr=($3) } Array::setArray arr , \"1,2,3,\" this example results to the following error messages\nbash: local: warning: arr: circular name reference bash: warning: arr: circular name reference bash: warning: arr: circular name reference Tis example should be fixed by renaming local arr to a more “scoped” name.\nArray::setArray() { local -n setArray_array=$1 local IFS=$2 - # set no glob feature set -f # shellcheck disable=SC2206,SC2034 setArray_array=($3) } Array::setArray arr , \"1,2,3,\" # declare -p arr # # output: declare -a arr=([0]=\"1\" [1]=\"2\" [2]=\"3\") 8.6.2. Example 2 A more tricky example, here the references array is affected to local array, this local array has a conflicting name. This example does not produce any error messages.\nPostman::Model::getValidCollectionRefs() { local configFile=\"$1\" local -n getValidCollectionRefs=$2 shift 2 || true local -a refs=(\"$@\") # ... getValidCollectionRefs=(\"${refs[@]}\") } local -a refs Postman::Model::getValidCollectionRefs \"file\" refs a b c declare -p refs # =\u003e declare -a refs In Previous example, getValidCollectionRefs is well “scoped” but there is a conflict with the local refs array inside the function resulting in affectation not working. The correct way to do it is to scope also the variables affected to referenced variables\nPostman::Model::getValidCollectionRefs() { local configFile=\"$1\" local -n getValidCollectionRefsResult=$2 shift 2 || true local -a getValidCollectionRefsSelection=(\"$@\") # ... getValidCollectionRefsResult=(\"${getValidCollectionRefsSelection[@]}\") } local -a refs Postman::Model::getValidCollectionRefs \"file\" refs a b c declare -p refs # =\u003e declare -a refs=([0]=\"a\" [1]=\"b\" [2]=\"c\") 9. Capture output You can use command substitution.\nEg:\nlocal output output=\"$(functionThatOutputSomething \"${arg1}\")\" 9.1. Capture output and test result local output output=\"$(functionThatOutputSomething \"${arg1}\")\" || { echo \"error\" exit 1 } 9.2. Capture output and retrieve status code It’s advised to put it on the same line using ;. If it was on 2 lines, other commands could be put between the command and the status code retrieval, the status would not be the same command status.\n10. Array read each line of a file to an array readarray -t var \u003c /path/to/filename 11. Temporary directory use ${TMPDIR:-/tmp}, TMPDIR variable does not always exist. or when mktemp is available, use dirname $(mktemp -u --tmpdir)\nThe variable TMPDIR is initialized in src/_includes/_commonHeader.sh used by all the binaries used in this framework.\n12. Traps when trapping EXIT do not forget to throw back same exit code otherwise exit code of last command executed in the trap is thrown\nIn this example rc variable contains the original exit code\ncleanOnExit() { local rc=$? if [[ \"${KEEP_TEMP_FILES:-0}\" = \"1\" ]]; then Log::displayInfo \"KEEP_TEMP_FILES=1 temp files kept here '${TMPDIR}'\" elif [[ -n \"${TMPDIR+xxx}\" ]]; then Log::displayDebug \"KEEP_TEMP_FILES=0 removing temp files '${TMPDIR}'\" rm -Rf \"${TMPDIR:-/tmp/fake}\" \u003e/dev/null 2\u003e\u00261 fi exit \"${rc}\" } trap cleanOnExit EXIT HUP QUIT ABRT TERM 13. Deal with SIGPIPE - exit code 141 related stackoverflow post\nset -o pipefail makes exit code 141 being sent in some cases\nEg: with grep\nbin/postmanCli --help | grep -q DESCRIPTION echo \"$? ${PIPESTATUS[@]}\" This is because grep -q exits immediately with a zero status as soon as a match is found. The zfs command is still writing to the pipe, but there is no reader (because grep has exited), so it is sent a SIGPIPE signal from the kernel and it exits with a status of 141.\nEg: or with head\necho \"${longMultilineString}\" | head -n 1 Finally I found this elegant stackoverflow solution:\nhandle_pipefail() { # ignore exit code 141 from simple command pipes # - use with: cmd1 | cmd2 || handle_pipefail $? (($1 == 141)) \u0026\u0026 return 0 return $1 } # then use it or test it as: yes | head -n 1 || handle_pipefail $? echo \"ec=$?\" I added handle_pipefail as Bash::handlePipelineFailure in bash-tools-framework.\n14. Performances analysis generate a csv file with milliseconds measures\ncodeToMeasureStart=$(date +%s%3N) # ... the code to measure echo \u003e\u00262 \"printCurrentLine;$(($(date +%s%3N) - codeToMeasureStart))\" 15. Bash Performance tips 15.1. Array::wrap2 performance improvement Commit with performance improvement\nmanualTests/Array::wrap2Perf.sh:\ndisplaying 12 lines (558 characters) 100 times passed from ~10s to \u003c1s (improved by 90%) performance improvement using:\necho instead of string concatenation string substitution instead of calling sed on each element echo -e removed the need to do a loop on each character to parse ansi code and the need of Filters::removeAnsiCodes ","categories":["Bash"],"description":"Foundational best practices for writing Bash scripts","excerpt":"Foundational best practices for writing Bash scripts","ref":"/my-documents/docs/bash-scripts/basic-best-practices/","tags":["bash","scripts","best-practices"],"title":"Basic Best Practices"},{"body":" Welcome to the documentation! Browse through guides organized by topic covering Bash scripting, Docker, Jenkins, and more.\n1. Browse Documentation Use the sidebar navigation to explore:\nBash Scripts - Best practices for writing efficient Bash scripts How-To Guides - Step-by-step guides for Docker, Jenkins, and other technologies Reference Lists - Curated lists of tools and resources Other Projects - Related documentation sites and tools 2. Getting Started Choose a section from the sidebar to begin exploring the documentation. Each section contains detailed guides, best practices, and real-world examples.\n","categories":"","description":"Comprehensive guides and best practices","excerpt":"Comprehensive guides and best practices","ref":"/my-documents/docs/","tags":"","title":"Documentation"},{"body":" 1. Jenkins Master Slave Architecture 1.1. Jenkins controller/Jenkins master node 1.2. Nodes 1.3. Agents 1.4. Executors 1.5. Jobs 2. Jenkins dynamic node Source: https://www.jenkins.io/doc/book/managing/nodes/\nSource glossary: https://www.jenkins.io/doc/book/glossary/\n1. Jenkins Master Slave Architecture The Jenkins controller is the master node which is able to launch jobs on different nodes (machines) directed by an Agent. The Agent can the use one or several executors to execute the job(s) depending on configuration.\nJenkins is using Master/Slave architecture with the following components:\n1.1. Jenkins controller/Jenkins master node The central, coordinating process which stores configuration, loads plugins, and renders the various user interfaces for Jenkins.\nThe Jenkins controller is the Jenkins service itself and is where Jenkins is installed. It is a webserver that also acts as a “brain” for deciding how, when and where to run tasks. Management tasks (configuration, authorization, and authentication) are executed on the controller, which serves HTTP requests. Files written when a Pipeline executes are written to the filesystem on the controller unless they are off-loaded to an artifact repository such as Nexus or Artifactory.\n1.2. Nodes A machine which is part of the Jenkins environment and capable of executing Pipelines or jobs. Both the Controller and Agents are considered to be Nodes.\nNodes are the “machines” on which build agents run. Jenkins monitors each attached node for disk space, free temp space, free swap, clock time/sync and response time. A node is taken offline if any of these values go outside the configured threshold.\nThe Jenkins controller itself runs on a special built-in node. It is possible to run agents and executors on this built-in node although this can degrade performance, reduce scalability of the Jenkins instance, and create serious security problems and is strongly discouraged, especially for production environments.\n1.3. Agents An agent is typically a machine, or container, which connects to a Jenkins controller and executes tasks when directed by the controller.\nAgents manage the task execution on behalf of the Jenkins controller by using executors. An agent is actually a small (170KB single jar) Java client process that connects to a Jenkins controller and is assumed to be unreliable. An agent can use any operating system that supports Java. Tools required for builds and tests are installed on the node where the agent runs; they can be installed directly or in a container (Docker or Kubernetes). Each agent is effectively a process with its own PID (Process Identifier) on the host machine.\nIn practice, nodes and agents are essentially the same but it is good to remember that they are conceptually distinct.\n1.4. Executors A slot for execution of work defined by a Pipeline or job on a Node. A Node may have zero or more Executors configured which corresponds to how many concurrent Jobs or Pipelines are able to execute on that Node.\nAn executor is a slot for execution of tasks; effectively, it is a thread in the agent. The number of executors on a node defines the number of concurrent tasks that can be executed on that node at one time. In other words, this determines the number of concurrent Pipeline stages that can execute on that node at one time.\nThe proper number of executors per build node must be determined based on the resources available on the node and the resources required for the workload. When determining how many executors to run on a node, consider CPU and memory requirements as well as the amount of I/O and network activity:\nOne executor per node is the safest configuration. One executor per CPU core may work well if the tasks being run are small. Monitor I/O performance, CPU load, memory usage, and I/O throughput carefully when running multiple executors on a node. 1.5. Jobs A user-configured description of work which Jenkins should perform, such as building a piece of software, etc.\n2. Jenkins dynamic node Jenkins has static slave nodes and can trigger the generation of dynamic slave nodes\n","categories":["Jenkins"],"description":"Understanding Jenkins architecture and concepts","excerpt":"Understanding Jenkins architecture and concepts","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/01-how-jenkins-works/","tags":["jenkins","jenkinsfile","ci-cd"],"title":"How Jenkins Works"},{"body":"This section provides comprehensive guides for writing Jenkinsfiles and working with Jenkins pipelines.\nThis is a complete guide covering Jenkins architecture, pipeline syntax, shared libraries, best practices, and annotated examples.\n1. What You’ll Learn How Jenkins works and its architecture Declarative and scripted pipeline syntax Creating and using Jenkins shared libraries Jenkins best practices and configuration Real-world Jenkinsfile examples with detailed annotations Common recipes and troubleshooting tips ","categories":"","description":"Comprehensive guide to writing Jenkins pipelines and Jenkinsfiles","excerpt":"Comprehensive guide to writing Jenkins pipelines and Jenkinsfiles","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/","tags":"","title":"How to Write Jenkinsfiles"},{"body":"1. Test tools 1.1. TestContainers TestContainers\nUnit tests with real dependencies\nTestContainers is an open source framework for providing throwaway, lightweight instances of databases, message brokers, web browsers, or just about anything that can run in a Docker container.\nTest dependencies as code\nNo more need for mocks or complicated environment configurations. Define your test dependencies as code, then simply run your tests and containers will be created and then deleted.\nWith support for many languages and testing frameworks, all you need is Docker.\nSupported languages: python, nodejs, …\nSupported modules: redis, mysql, …\n","categories":"","description":"Reference list of testing tools and frameworks","excerpt":"Reference list of testing tools and frameworks","ref":"/my-documents/docs/lists/test/","tags":"","title":"Test"},{"body":" 1. Dockerfile best practices 2. Basic best practices 2.1. Best Practice #1: Merge the image layers 2.1.1. Bad practice #1 2.1.2. Best practice #1 2.2. Best Practice #2: trace commands and fail on error 2.2.1. Bad practice #2 2.2.2. Best Practice #2 2.3. Best practice #3: packages ordering and versions 2.3.1. Bad practice #3 2.3.2. Best Practice #3 2.3.2.1. Order packages alphabetically and one package by line 2.3.2.2. Always specify packages versions 2.3.2.3. Ensure non interactive 2.4. Best practice #4: ensure image receives latest security updates 2.4.1. Bad practice #4 2.4.2. Best Practice #4 2.5. Conclusion: image size comparison 2.5.1. Dockerfile without best practices 2.5.2. Dockerfile with all optimizations 3. Docker Buildx best practices 3.1. Optimize image size 3.1.1. Dockerfile not optimized 3.1.2. Dockerfile optimized 1. Dockerfile best practices Follow official best practices and you can follow these specific best practices\nBut The worst so-called “best practice” for Docker\nBackup, explains why you should actually also use apt-get upgrade\nUse hadolint\nUse ;\\ to separate each command line\nsome Dockerfiles are using \u0026\u0026 to separate commands in the same RUN instruction (I was doing it too ;-), but I strongly discourage it because it breaks the checks done by set -o errexit set -o errexit makes the whole RUN instruction to fail if one of the commands has failed, but it is not the same when using \u0026\u0026 One package by line, packages sorted alphabetically to ease readability and merges\nAlways specify the most exact version possible of your packages (to avoid to get major version that would break your build or software)\ndo not usage docker image with latest tag, always specify the right version to use\n2. Basic best practices 2.1. Best Practice #1: Merge the image layers in a Dockerfile each RUN command will create an image layer.\n2.1.1. Bad practice #1 Here a bad practice that you shouldn’t follow\n2.1.2. Best practice #1 Best practice #1 merge the RUN layers to avoid cache issue and gain on total image size\nFROM ubuntu:20.04 RUN apt-get update \\ \u0026\u0026 apt-get install -y apache2 \\ \u0026\u0026 rm -rf \\ /var/lib/apt/lists/* \\ /tmp/* \\ /var/tmp/* \\ /usr/share/doc/* 2.2. Best Practice #2: trace commands and fail on error from previous example we want to trace each command that is executed\n2.2.1. Bad practice #2 when building complex layer and one of the command fails, it’s interesting to know which command makes the build to fail\nFROM ubuntu:20.04 RUN apt-get update \\ \u0026\u0026 [ -d badFolder ] \\ \u0026\u0026 apt-get install -y apache2 \\ \u0026\u0026 rm -rf \\ /var/lib/apt/lists/* \\ /tmp/* \\ /var/tmp/* \\ /usr/share/doc/* docker build . gives the following log output(partly truncated)\nNot easy here to know that the command [ -d badFolder ] makes the build failing\nWithout the best practice #2, the following code build successfully\nFROM ubuntu:20.04 RUN set -x ;\\ apt-get update ;\\ [ -d badFolder ] ;\\ ls -al 2.2.2. Best Practice #2 Best Practice #2: Override SHELL options of the RUN command and use ;\\ instead of \u0026\u0026\nThe following options are set on the shell to override the default behavior:\nset -o pipefail: The return status of a pipeline is the exit status of the last command, unless the pipefail option is enabled. If pipefail is enabled, the pipeline’s return status is the value of the last (rightmost) command to exit with a non-zero status, or zero if all commands exit successfully. without it, a command failure could be masked by the command piped after it set -o errexit (same as set -e): Exit immediately if a pipeline (which may consist of a single simple command), a list, or a compound command (see SHELL GRAMMAR above), exits with a non-zero status. set -o xtrace(same as set -x): After expanding each simple command, for command, case command, select command, or arithmetic for command, display the expanded value of PS4, followed by the command and its expanded arguments or associated word list. Those options are not mandatory but are strongly advised. Although there are some workaround to know:\nif a command can fail and you want to ignore it, you can use commandThatCanFail || true These options can be used with /bin/sh as well.\nAlso it is strongly advised to use ;\\ to separate commands because it could happen that some errors are ignored when \u0026\u0026 is used in conjunction with ||\nFROM ubuntu:20.04 # The SHELL instructions will be applied to all the subsequent RUN instructions SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"xtrace\", \"-c\"] RUN apt-get update ;\\ [ -d badFolder ] ;\\ apt-get install -y apache2 ;\\ rm -rf \\ /var/lib/apt/lists/* \\ /tmp/* \\ /var/tmp/* \\ /usr/share/doc/* docker build . gives the following log output(partly truncated)\nHere the command line displayed just above the error indicates clearly from where the error comes from:\n#5 6.172 + '[' -d badFolder ']' 2.3. Best practice #3: packages ordering and versions Best Practice #3: order packages alphabetically, always specify packages versions, ensure non interactive\nFrom previous example we want to install several packages\n2.3.1. Bad practice #3 let’s add some packages on our previous example (errors removed)\nThe following docker has the following issues:\nit doesn’t set the package versions the installation will install also the recommended packages it’s using apt instead of apt-get (hadolint warning DL3027 Do not use apt as it is meant to be a end-user tool, use apt-get or apt-cache instead) the packages are not ordered alphabetically FROM ubuntu:20.04 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"xtrace\", \"-c\"] RUN apt update ;\\ apt install -y php7.4 apache2 php7.4-curl redis-tools ;\\ rm -rf \\ /var/lib/apt/lists/* \\ /tmp/* \\ /var/tmp/* \\ /usr/share/doc/* 2.3.2. Best Practice #3 Best Practice #3: order packages alphabetically, always specify packages versions, ensure non interactive\n2.3.2.1. Order packages alphabetically and one package by line one package by line allows packages to be simpler ordered alphabetically\none package by line and ordering alphabetically allows :\nto merge branches changes more easily to detect redundancies more easily to improve readability 2.3.2.2. Always specify packages versions over the time your build’s dependencies could be updated on the remote repositories and your packages be unattended upgraded to the latest version making your software breaks because it doesn’t manage the changes of the new package.\nIt happens several times for me, for example, in 2021, xdebug has been automatically upgraded on one of my docker image from version 2.8 to 3.0 making all the dev environments broken. It happens also on a build pipeline with a version of npm gulp that has been upgraded to latest version. In both cases we resolved the issue by downgrading the version to the one we were using.\n2.3.2.3. Ensure non interactive some apt-get packages could ask for interactive questions, you can avoid this using the env variable DEBIAN_FRONTEND=noninteractive\nNote: ARG instruction allows to set env variable available only during build time\nFROM ubuntu:20.04 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"xtrace\", \"-c\"] ARG DEBIAN_FRONTEND=noninteractive RUN apt-get update ;\\ apt-get install -y -q --no-install-recommends \\ # Mind to use quotes to avoid shell to try to expand * with some files apache2='2.4.*' \\ php7.4='7.4.*' \\ php7.4-curl='7.4.*' \\ # Notice the ':'(colon) redis-tools='5:5.*' \\ ;\\ # cleaning apt-get autoremove -y ;\\ apt-get -y clean ;\\ rm -rf \\ /var/lib/apt/lists/* \\ /tmp/* \\ /var/tmp/* \\ /usr/share/doc/* # use the following command to know the current version of the packages # using another RUN instead of using previous one will avoid the whole # previous layer to be rebuilt # RUN apt-cache policy \\ # apache2 \\ # php7.4 \\ # php7.4-curl \\ # redis-tools # Gives the following output #6 0.387 + apt-cache policy apache2 #6 0.399 apache2: #6 0.399 Installed: 2.4.41-4ubuntu3.14 #6 0.399 Candidate: 2.4.41-4ubuntu3.14 #6 0.399 Version table: #6 0.399 *** 2.4.41-4ubuntu3.14 100 #6 0.399 100 /var/lib/dpkg/status #6 0.400 + apt-cache policy php7.4 #6 0.409 php7.4: #6 0.409 Installed: 7.4.3-4ubuntu2.18 #6 0.409 Candidate: 7.4.3-4ubuntu2.18 #6 0.409 Version table: #6 0.409 *** 7.4.3-4ubuntu2.18 100 #6 0.409 100 /var/lib/dpkg/status #6 0.409 + apt-cache policy php7.4-curl #6 0.420 php7.4-curl: #6 0.420 Installed: 7.4.3-4ubuntu2.18 #6 0.420 Candidate: 7.4.3-4ubuntu2.18 #6 0.420 Version table: #6 0.420 *** 7.4.3-4ubuntu2.18 100 #6 0.421 100 /var/lib/dpkg/status #6 0.421 + apt-cache policy redis-tools #6 0.431 redis-tools: #6 0.431 Installed: 5:5.0.7-2ubuntu0.1 #6 0.431 Candidate: 5:5.0.7-2ubuntu0.1 #6 0.431 Version table: #6 0.431 *** 5:5.0.7-2ubuntu0.1 100 #6 0.432 100 /var/lib/dpkg/status 2.4. Best practice #4: ensure image receives latest security updates from previous example we want to ensure the image receives the latest security updates\n2.4.1. Bad practice #4 registry image are not always updated and latest apt security updates are not installed\nFROM ubuntu:20.04 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"xtrace\", \"-c\"] ARG DEBIAN_FRONTEND=noninteractive RUN apt-get update ;\\ apt-get install -y -q --no-install-recommends \\ apache2='2.4.*' \\ php7.4='7.4.*' \\ php7.4-curl='7.4.*' \\ redis-tools='5:5.*' \\ ;\\ # cleaning apt-get autoremove -y ;\\ apt-get -y clean ;\\ rm -rf \\ /var/lib/apt/lists/* \\ /tmp/* \\ /var/tmp/* \\ /usr/share/doc/* 2.4.2. Best Practice #4 be sure to apply latest security updates, to install the latest security updates in the image, keep sure to call apt-get upgrade -y\nHere the updated Dockerfile:\nFROM ubuntu:20.04 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"xtrace\", \"-c\"] ARG DEBIAN_FRONTEND=noninteractive RUN apt-get update ;\\ # be sure to apply latest security updates # https://pythonspeed.com/articles/security-updates-in-docker/ apt-get upgrade -y ;\\ apt-get install -y -q --no-install-recommends \\ apache2='2.4.*' \\ php7.4='7.4.*' \\ php7.4-curl='7.4.*' \\ redis-tools='5:5.*' \\ ;\\ # cleaning apt-get autoremove -y ;\\ apt-get -y clean ;\\ rm -rf \\ /var/lib/apt/lists/* \\ /tmp/* \\ /var/tmp/* \\ /usr/share/doc/* 2.5. Conclusion: image size comparison from previous example we want to ensure the image receives the latest security updates\n2.5.1. Dockerfile without best practices FROM ubuntu:20.04 ARG DEBIAN_FRONTEND=noninteractive RUN apt-get update RUN apt-get install -y apache2 php7.4 php7.4-curl redis-tools # cleaning RUN apt-get autoremove -y ;\\ apt-get -y clean ;\\ rm -rf \\ /var/lib/apt/lists/* \\ /tmp/* \\ /var/tmp/* \\ /usr/share/doc/* 2.5.2. Dockerfile with all optimizations FROM ubuntu:20.04 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"xtrace\", \"-c\"] ARG DEBIAN_FRONTEND=noninteractive RUN apt-get update ;\\ apt-get upgrade -y ;\\ apt-get install -y -q --no-install-recommends \\ apache2='2.4.*' \\ php7.4='7.4.*' \\ php7.4-curl='7.4.*' \\ redis-tools='5:5.*' \\ ;\\ # cleaning apt-get autoremove -y ;\\ apt-get -y clean ;\\ rm -rf \\ /var/lib/apt/lists/* \\ /tmp/* \\ /var/tmp/* \\ /usr/share/doc/* 3. Docker Buildx best practices 3.1. Optimize image size Source: https://askubuntu.com/questions/628407/removing-man-pages-on-ubuntu-docker-installation\nLet’s consider this example\n3.1.1. Dockerfile not optimized FROM ubuntu:20.04 as stage1 ARG DEBIAN_FRONTEND=noninteractive SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"xtrace\", \"-c\"] RUN \\ apt-get update ;\\ apt-get install -y -q --no-install-recommends \\ htop FROM stage1 as stage2 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"xtrace\", \"-c\"] RUN \\ # here we just test that the ARG DEBIAN_FRONTEND has been inherited from # previous stage (it is the case) echo \"DEBIAN_FRONTEND=${DEBIAN_FRONTEND}\" Now let’s build and check the image size, the best way to do this is to export the image to a file\ndocker build and save:\ndocker build -f Dockerfile1 -t test1 . docker save test1 -o test1.tar Now we will optimize this image by removing man pages (you can still find man pages on the web) and removing apt cache\n3.1.2. Dockerfile optimized FROM ubuntu:20.04 as stage1 ARG DEBIAN_FRONTEND=noninteractive COPY 01-noDoc /etc/dpkg/dpkg.cfg.d/ COPY 02-aptNoCache /etc/apt/apt.conf.d/ SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"xtrace\", \"-c\"] RUN \\ # remove apt cache and man/doc rm -rf /var/cache/apt/archives /usr/share/{doc,man,locale}/ ;\\ \\ apt-get update ;\\ apt-get install -y -q --no-install-recommends \\ htop \\ ;\\ # clean apt packages apt-get autoremove -y ;\\ ls -al /var/cache/apt ;\\ rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/doc/* FROM stage1 as stage2 SHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-o\", \"errexit\", \"-o\", \"xtrace\", \"-c\"] RUN \\ echo \"DEBIAN_FRONTEND=${DEBIAN_FRONTEND}\" Here the content of /etc/dpkg/dpkg.cfg.d/01-noDoc, it will tell apt to not install man docs and translations\n# /etc/dpkg/dpkg.cfg.d/01_nodoc # Delete locales path-exclude=/usr/share/locale/* # Delete man pages path-exclude=/usr/share/man/* # Delete docs path-exclude=/usr/share/doc/* path-include=/usr/share/doc/*/copyright Here the content of /etc/apt/apt.conf.d/02-aptNoCache, it will instruct apt to not store any cache (note that apt-get clean will not work after that change but you don’t need to use it anymore)\nDir::Cache \"\"; Dir::Cache::archives \"\"; Now let’s build and check the image size, the best way to do this is to export the image to a file\ndocker build and save:\ndocker build -f Dockerfile2 -t test2 . docker save test2 -o test2.tar Here the size of the files\ntest1.tar 117 020 672 bytes test2.tar 76 560 896 bytes We passed from ~117MB to ~76MB so we gain ~41MB Please note also that we used --no-install-recommends option in both example that allows us to save some other MB\n","categories":"","description":"Best practices for writing efficient and secure Dockerfiles","excerpt":"Best practices for writing efficient and secure Dockerfiles","ref":"/my-documents/docs/howtos/howto-write-dockerfile/","tags":"","title":"How to Write Dockerfiles"},{"body":" In-depth tutorials and how-to guides for Docker, Jenkins, and other development technologies.\n1. Available Guides How to Write Dockerfiles - Best practices for efficient Dockerfiles How to Write Docker Compose Files - Organizing multi-container applications How to Write Jenkinsfiles - Complete Jenkins pipeline guide (10 articles) Saml2Aws Setup - AWS access with SAML authentication 2. Getting Started Select a guide from the sidebar to begin.\n","categories":"","description":"Step-by-step guides for various technologies","excerpt":"Step-by-step guides for various technologies","ref":"/my-documents/docs/howtos/","tags":"","title":"How-To Guides"},{"body":" 1. What is a pipeline ? 2. Pipeline creation via UI 3. Groovy 4. Difference between scripted pipeline (freestyle) and declarative pipeline syntax 5. Declarative pipeline example 6. Scripted pipeline example 7. Why Pipeline? 1. What is a pipeline ? https://www.jenkins.io/doc/book/pipeline/\nJenkins Pipeline (or simply “Pipeline” with a capital “P”) is a suite of plugins which supports implementing and integrating continuous delivery pipelines into Jenkins.\nA continuous delivery (CD) pipeline is an automated expression of your process for getting software from version control right through to your users and customers. Every change to your software (committed in source control) goes through a complex process on its way to being released. This process involves building the software in a reliable and repeatable manner, as well as progressing the built software (called a “build”) through multiple stages of testing and deployment.\nPipeline provides an extensible set of tools for modeling simple-to-complex delivery pipelines “as code” via the Pipeline domain-specific language (DSL) syntax. View footnote 1\nThe definition of a Jenkins Pipeline is written into a text file (called a Jenkinsfile) which in turn can be committed to a project’s source control repository. View footnote 2 This is the foundation of “Pipeline-as-code”; treating the CD pipeline a part of the application to be versioned and reviewed like any other code.\n2. Pipeline creation via UI it’s not recommended but it’s possible to create a pipeline via the UI.\nThere are several drawbacks:\nno code revision difficult to read, understand 3. Groovy Scripted and declarative pipelines are using groovy language.\nCheckout https://www.guru99.com/groovy-tutorial.html to have a quick overview of this derived language check Wikipedia\n4. Difference between scripted pipeline (freestyle) and declarative pipeline syntax What are the main differences ? Here are some of the most important things you should know:\nBasically, declarative and scripted pipelines differ in terms of the programmatic approach. One uses a declarative programming model and the second uses an imperative programming mode. Declarative pipelines break down stages into multiple steps, while in scripted pipelines there is no need for this. Example below Declarative and Scripted Pipelines are constructed fundamentally differently. Declarative Pipeline is a more recent feature of Jenkins Pipeline which:\nprovides richer syntactical features over Scripted Pipeline syntax, and is designed to make writing and reading Pipeline code easier. By default automatically checkout stage Many of the individual syntactical components (or “steps”) written into a Jenkinsfile, however, are common to both Declarative and Scripted Pipeline. Read more about how these two types of syntax differ in Pipeline concepts and Pipeline syntax overview.\n5. Declarative pipeline example Pipeline syntax documentation\npipeline { agent { // executed on an executor with the label 'some-label' // or 'docker', the label normally specifies: // - the size of the machine to use // (eg.: Docker-C5XLarge used for build that needs a powerful machine) // - the features you want in your machine // (eg.: docker-base-ubuntu an image with docker command available) label \"some-label\" } stages { stage(\"foo\") { steps { // variable assignment and Complex global // variables (with properties or methods) // can only be done in a script block script { foo = docker.image('ubuntu') env.bar = \"${foo.imageName()}\" echo \"foo: ${foo.imageName()}\" } } } stage(\"bar\") { steps{ echo \"bar: ${env.bar}\" echo \"foo: ${foo.imageName()}\" } } } } 6. Scripted pipeline example Scripted pipelines permit a developer to inject code, while the declarative Jenkins pipeline doesn’t. should be avoided actually, try to use jenkins library instead\nnode { git url: 'https://github.com/jfrogdev/project-examples.git' // Get Artifactory server instance, defined in the Artifactory Plugin // administration page. def server = Artifactory.server \"SERVER_ID\" // Read the upload spec and upload files to Artifactory. def downloadSpec = '''{ \"files\": [ { \"pattern\": \"libs-snapshot-local/*.zip\", \"target\": \"dependencies/\", \"props\": \"p1=v1;p2=v2\" } ] }''' def buildInfo1 = server.download spec: downloadSpec // Read the upload spec which was downloaded from github. def uploadSpec = '''{ \"files\": [ { \"pattern\": \"resources/Kermit.*\", \"target\": \"libs-snapshot-local\", \"props\": \"p1=v1;p2=v2\" }, { \"pattern\": \"resources/Frogger.*\", \"target\": \"libs-snapshot-local\" } ] }''' // Upload to Artifactory. def buildInfo2 = server.upload spec: uploadSpec // Merge the upload and download build-info objects. buildInfo1.append buildInfo2 // Publish the build to Artifactory server.publishBuildInfo buildInfo1 } 7. Why Pipeline? Jenkins is, fundamentally, an automation engine which supports a number of automation patterns. Pipeline adds a powerful set of automation tools onto Jenkins, supporting use cases that span from simple continuous integration to comprehensive CD pipelines. By modeling a series of related tasks, users can take advantage of the many features of Pipeline:\nCode: Pipelines are implemented in code and typically checked into source control, giving teams the ability to edit, review, and iterate upon their delivery pipeline. Durable: Pipelines can survive both planned and unplanned restarts of the Jenkins controller. Pausable: Pipelines can optionally stop and wait for human input or approval before continuing the Pipeline run. Versatile: Pipelines support complex real-world CD requirements, including the ability to fork/join, loop, and perform work in parallel. Extensible: The Pipeline plugin supports custom extensions to its DSL see jenkins doc and multiple options for integration with other plugins. While Jenkins has always allowed rudimentary forms of chaining Freestyle Jobs together to perform sequential tasks, see jenkins doc Pipeline makes this concept a first-class citizen in Jenkins.\nMore information on Official jenkins documentation - Pipeline\n","categories":["Jenkins"],"description":"Declarative and scripted pipeline syntax","excerpt":"Declarative and scripted pipeline syntax","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/02-jenkins-pipelines/","tags":["jenkins","jenkinsfile","ci-cd"],"title":"Jenkins Pipelines"},{"body":" 1. some commands default options to use 2. Bash and grep regular expressions 1. some commands default options to use https://dougrichardson.us/notes/fail-fast-bash-scripting.html but set -o nounset is not usable because empty array are considered unset always use sed -E avoid using grep -P as it is not supported on alpine, prefer using -E 2. Bash and grep regular expressions grep regular expression [A-Za-z] matches by default accentuated character, if you don’t want to match them, use the environment variable LC_ALL=POSIX, Eg: LC_ALL=POSIX grep -E -q '^[A-Za-z_0-9:]+$' I added export LC_ALL=POSIX in all my headers, it can be overridden using a subShell ","categories":["Bash"],"description":"Best practices for using Linux commands in Bash scripts","excerpt":"Best practices for using Linux commands in Bash scripts","ref":"/my-documents/docs/bash-scripts/linux-commands-best-practices/","tags":["bash","scripts","best-practices"],"title":"Linux Commands Best Practices"},{"body":"1. OSINT tools What Are Open Source Intelligence (OSINT) Tools? Open-source intelligence software, abbreviated as OSINT software, are tools that allow the collection of information that is publicly available or open-source. The goal of using OSINT software is mainly to learn more about an individual or a business.\n1.1. Lissy93/web-check All-in-one OSINT tool for analyzing any website Comprehensive, on-demand open source intelligence for any website\ngithub project Lissy93/web-check\n","categories":"","description":"Reference list of web analysis and OSINT tools","excerpt":"Reference list of web analysis and OSINT tools","ref":"/my-documents/docs/lists/web/","tags":"","title":"Web Tools"},{"body":" 1. use of default temp directory created by bats 2. avoid boilerplate code 3. Override an environment variable when using bats run 4. Override a bash framework function 1. use of default temp directory created by bats Instead of creating yourself your temp directory, you can use the special variable BATS_TEST_TMPDIR, this directory is automatically destroyed at the end of the test except if the option --no-tempdir-cleanup is provided to bats command.\nException: if you are testing bash traps, you would need to create your own directories to avoid unexpected errors.\n2. avoid boilerplate code using this include, includes most of the features needed when using bats\n# shellcheck source=src/batsHeaders.sh source \"$(cd \"${BATS_TEST_DIRNAME}/..\" \u0026\u0026 pwd)/batsHeaders.sh\" It sets those bash features:\nset -o errexit set -o pipefail It imports several common files like some additional bats features.\nAnd makes several variables available:\nBASH_TOOLS_ROOT_DIR vendorDir srcDir FRAMEWORK_ROOT_DIR (same as BASH_TOOLS_ROOT_DIR but used by some bash framework functions) LC_ALL=POSIX see Bash and grep regular expressions best practices 3. Override an environment variable when using bats run SUDO=\"\" run Linux::Apt::update 4. Override a bash framework function using stub is not possible because it does not support executable with special characters like ::. So the solution is just to override the function inside your test function without importing the original function of course. In tearDown method do not forget to use unset -f yourFunction\n","categories":["Bash"],"description":"Best practices for testing Bash scripts with Bats framework","excerpt":"Best practices for testing Bash scripts with Bats framework","ref":"/my-documents/docs/bash-scripts/bats-best-practices/","tags":["bash","scripts","best-practices"],"title":"Bats Testing Framework"},{"body":" 1. platform 1. platform as not everyone is using the same environment (some are using MacOS for example which is targeting arm64 instead of amd64), it is advised to add this option to target the right architecture\ndocker-compose platform:\nservices: serviceName: platform: linux/x86_64 # ... ","categories":"","description":"Guide to writing and organizing Docker Compose files","excerpt":"Guide to writing and organizing Docker Compose files","ref":"/my-documents/docs/howtos/howto-write-docker-compose/","tags":"","title":"How to Write Docker Compose Files"},{"body":" 1. What is a jenkins shared library ? 2. Loading libraries dynamically 3. jenkins library directory structure 4. Jenkins library 5. Jenkins library structure 6. external resource usage 1. What is a jenkins shared library ? As Pipeline is adopted for more and more projects in an organization, common patterns are likely to emerge. Oftentimes it is useful to share parts of Pipelines between various projects to reduce redundancies and keep code “DRY”\nfor more information check pipeline shared libraries\n2. Loading libraries dynamically As of version 2.7 of the Pipeline: Shared Groovy Libraries plugin, there is a new option for loading (non-implicit) libraries in a script: a library step that loads a library dynamically, at any time during the build.\nIf you are only interested in using global variables/functions (from the vars/ directory), the syntax is quite simple:\nlibrary 'my-shared-library' Thereafter, any global variables from that library will be accessible to the script.\n3. jenkins library directory structure The directory structure of a Shared Library repository is as follows:\n(root) +- src # Groovy source files | +- org | +- foo | +- Bar.groovy # for org.foo.Bar class | +- vars # The vars directory hosts script # files that are exposed as a variable in Pipelines | +- foo.groovy # for global 'foo' variable | +- foo.txt # help for 'foo' variable | +- resources # resource files (external libraries only) | +- org | +- foo | +- bar.json # static helper data for org.foo.Bar 4. Jenkins library remember that jenkins library code is executed on master node\nif you want to execute code on the node, you need to use jenkinsExecutor\nusage of jenkins executor\nString credentialsId = 'babee6c1-14fe-4d90-9da0-ffa7068c69af' def lib = library( identifier: 'jenkins_library@v1.0', retriever: modernSCM([ $class: 'GitSCMSource', remote: 'git@github.com:fchastanet/jenkins-library.git', credentialsId: credentialsId ]) ) // this is the jenkinsExecutor instance def docker = lib.fchastanet.Docker.new(this) Then in the library, it is used like this:\ndef status = this.jenkinsExecutor.sh( script: \"docker pull ${cacheTag}\", returnStatus: true ) 5. Jenkins library structure I remarked that a lot of code was duplicated between all my Jenkinsfiles so I created this library https://github.com/fchastanet/jenkins-library\n(root) +- doc # markdown files automatically generated # from groovy files by generateDoc.sh +- src # Groovy source files | +- fchastanet | +- Cloudflare.groovy # zonePurge | +- Docker.groovy # getTagCompatibleFromBranch # pullBuildPushImage, ... | +- Git.groovy # getRepoURL, getCommitSha, # getLastPusherEmail, # updateConditionalGithubCommitStatus | +- Kubernetes.groovy # deployHelmChart, ... | +- Lint.groovy # dockerLint, # transform lighthouse report # to Warnings NG issues format | +- Mail.groovy # sendTeamsNotification, # sendConditionalEmail, ... | +- Utils.groovy # deepMerge, isCollectionOrArray, # deleteDirAsRoot, # initAws (could be moved to Aws class) +- vars # The vars directory hosts script files that # are exposed as a variable in Pipelines | +- dockerPullBuildPush.groovy # | +- whenOrSkip.groovy # 6. external resource usage If you need you check out how I used this repository https://github.com/fchastanet/jenkins-library-resources in jenkins_library (Linter) that hosts some resources to parse result files.\n","categories":["Jenkins"],"description":"Creating and using Jenkins shared libraries","excerpt":"Creating and using Jenkins shared libraries","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/03-jenkins-library/","tags":["jenkins","jenkinsfile","ci-cd"],"title":"Jenkins Library"},{"body":" Curated lists of tools, resources, and references for development and testing.\n1. Available Lists Test Tools - Testing frameworks and tools Web Tools - Web analysis and OSINT tools 2. Browse Lists Select a list from the sidebar to explore resources.\n","categories":"","description":"Curated reference lists and collections","excerpt":"Curated reference lists and collections","ref":"/my-documents/docs/lists/","tags":"","title":"Reference Lists"},{"body":" 1. Pipeline best practices 2. Shared library best practices 1. Pipeline best practices Official Jenkins pipeline best practices\nSummary:\nMake sure to use Groovy code in Pipelines as glue Externalize shell scripts from Jenkins Pipeline for better jenkinsfile readability in order to test the scripts isolated from jenkins Avoid complex Groovy code in Pipelines Groovy code always executes on controller which means using controller resources(memory and CPU) it is not the case for shell scripts eg1: prefer using jq inside shell script instead of groovy JsonSlurper eg2: prefer calling curl instead of groovy http request Reducing repetition of similar Pipeline steps (eg: one sh step instead of severals) group similar steps together to avoid step creation/destruction overhead Avoiding calls to Jenkins.getInstance 2. Shared library best practices Official Jenkins shared libraries best practices\nSummary:\nDo not override built-in Pipeline steps Avoiding large global variable declaration files Avoiding very large shared libraries And:\nimport jenkins library using a tag like in docker build, npm package with package-lock.json or python pip lock, it’s advised to target a given version of the library because some changes could break The missing part: we miss on this library unit tests but each pipeline is a kind of integration test Because a pipeline can be resumed, your library’s classes should implement Serializable class and the following attribute has to be provided: private static final long serialVersionUID = 1L ","categories":["Jenkins"],"description":"Best practices for Jenkins configuration","excerpt":"Best practices for Jenkins configuration","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/04-jenkins-best-practices/","tags":["jenkins","jenkinsfile","ci-cd"],"title":"Jenkins Best Practices"},{"body":"Links to related documentation and projects in this documentation suite.\nBash Tools Framework Bash Tools Bash Dev Env Bash Compiler ","categories":"","description":"Related documentation sites","excerpt":"Related documentation sites","ref":"/my-documents/docs/other-projects/","tags":"","title":"Other Projects"},{"body":"Configure saml2aws accounts\nsaml2aws configure \\ --idp-account='\u003caccount_alias\u003e' \\ --idp-provider='AzureAD' \\ --mfa='Auto' \\ --profile='\u003cprofile\u003e' \\ --url='https://account.activedirectory.windowsazure.com' \\ --username='\u003cusername\u003e@microsoft.com' \\ --app-id='\u003capp_id\u003e' \\ --skip-prompt \u003capp_id\u003e is a unique identifier for the application we want credentials for (in this case an AWS environment). \u003caccount_alias\u003e serves as a name to identify the saml2aws configuration (see your ~/.saml2aws file \u003cprofile\u003e serves as the name of the aws cli profile that will be created when you log in. This will automatically identify your tenant ID based on the AppID and will create a configuration based on the provided information. Configuration will be created in ~/.saml2aws\n1. Use saml2aws login command to configure the AWS CLI profile Run saml2aws login to add or refresh your profile for the aws cli.\nsaml2aws login -a ${account_alias} Follow the prompts to enter your SSO credentials and complete the multi-factor authentication step.\nNote: if you are part of multiple roles you can use –role flag to configure the required role.\nAbove steps have been taken from below GitHub Repo. They have been tried in MacOS, Windows, Linux and Windows WSL https://github.com/Versent/saml2aws\n2. Kubernetes connection Adding a newly created Technology Convergence EKS cluster to your ~/.kube/config:\nAdd EKS Cluster to ~/.kube/config\naws eks update-kubeconfig --name $clusterName --region us-east-1 3. Common issues 3.1. Error - error authenticating to IdP: unable to locate IDP OIDC form submit URL This is very likely because you changed your account password. Reenter your password when prompted at saml2aws login\n3.2. Error - error authenticating to IdP: unable to locate SAMLRequest URL This is very likely because you do not have access to this AWS account.\nMultifactor authentication asks for a number, but the terminal doesn’t provide a number.\nSolution 1: We’ve found that going to your Microsoft account security info and deleting and re-adding the sign-in method seems to fix the issue. You should then be able to just enter a Time-based one-time password from your Microsoft Authenticator app.\nSolution 2: You can change the MFA option for your saml2aws config either with PhoneAppOTP, PhoneAppNotification, or OneWaySMS. Something like this in your ~/.saml2aws file\nname = tc-dev app_id = 83cffb56-1d1b-400c-ad47-345c58e378dc url = https://account.activedirectory.windowsazure.com username = \u003c\u003e@microsoft.com provider = AzureAD mfa = OneWaySMS skip_verify = false timeout = 0 aws_urn = urn:amazon:webservices aws_session_duration = 3600 aws_profile = dev resource_id = subdomain = role_arn = region = http_attempts_count = http_retry_delay = credentials_file = saml_cache = false saml_cache_file = target_url = disable_remember_device = false disable_sessions = false prompter = for more reference, follow this page https://github.com/Versent/saml2aws/blob/master/doc/provider/aad/README.md#configure\n","categories":"","description":"Guide to setting up and using Saml2Aws for AWS access","excerpt":"Guide to setting up and using Saml2Aws for AWS access","ref":"/my-documents/docs/howtos/saml2aws/","tags":"","title":"Saml2Aws Setup"},{"body":"Pipeline example\n1. Simple one This build is used to generate docker images used to build production code and launch phpunit tests. This pipeline is parameterized in the Jenkins UI directly with the parameters:\nbranch (git branch to use) environment(select with 3 options: build, phpunit or all) it would have been better to use simply 2 checkboxes phpunit/build project_branch Here the source code with inline comments:\nAnnotated jenkinsfile Expand source\n// This method allows to convert the branch name to a docker image tag. // This method is generally used by most of my jenkins pipelines, it's why it has been added to https://github.com/fchastanet/jenkins-library/blob/master/src/fchastanet/Docker.groovy#L31 def getTagCompatibleFromBranch(String branchName) { def String tag = branchName.toLowerCase() tag = tag.replaceAll(\"^origin/\", \"\") return tag.replaceAll('/', '_') } // we declare here some variables that will be used in next stages def String deploymentBranchTagCompatible = '' pipeline { agent { node { // the pipeline is executed on a machine with docker daemon // available label 'docker-ubuntu' } } stages { stage ('checkout') { steps { // this command is actually not necessary because checkout is // done automatically when using declarative pipeline sh 'echo \"pulling ... ${GIT_BRANCH#origin/}\"' checkout scm // this particular build needs to access to some private github // repositories, so here we are copying the ssh key // it would be better to use new way of injecting ssh key // inside docker using sshagent // check https://stackoverflow.com/a/66897280 withCredentials([ sshUserPrivateKey( credentialsId: '855aad9f-1b1b-494c-aa7f-4de881c7f659', keyFileVariable: 'sshKeyFile' ) ]) { // best practice similar steps should be merged into one sh 'rm -f ./phpunit/id_rsa' sh 'rm -f ./build/id_rsa' // here we are escaping '$' so the variable will be // interpolated on the jenkins slave and not the jenkins // master node instead of escaping, we could have used // single quotes sh \"cp \\$sshKeyFile ./phpunit/id_rsa\" sh \"cp \\$sshKeyFile ./build/id_rsa\" } script { // as actually scm is already done before executing the // first step, this call could have been done during // declaration of this variable deploymentBranchTagCompatible = getTagCompatibleFromBranch(GIT_BRANCH) } } } stage(\"build Build env\") { when { // the build can be launched with the parameter environment // defined in the configuration of the jenkins job, these // parameters could have been defined directly in the pipeline // see https://www.jenkins.io/doc/book/pipeline/syntax/#parameters expression { return params.environment != \"phpunit\"} } steps { // here we could have launched all this commands in the same sh // directive sh \"docker build --build-arg BRANCH=${params.project_branch} -t build build\" // use a constant for dockerRegistryId.dkr.ecr.eu-west-1.amazonaws.com sh \"docker tag build dockerRegistryId.dkr.ecr.eu-west-1.amazonaws.com/build:${deploymentBranchTagCompatible}\" sh \"docker push dockerRegistryId.dkr.ecr.eu-west-1.amazonaws.com/build:${deploymentBranchTagCompatible}\" } } stage(\"build PHPUnit env\") { when { // it would have been cleaner to use // expression { return params.environment = \"phpunit\"} expression { return params.environment != \"build\"} } steps { sh \"docker build --build-arg BRANCH=${params.project_branch} -t phpunit phpunit\" sh \"docker tag phpunit dockerRegistryId.dkr.ecr.eu-west-1.amazonaws.com/phpunit:${deploymentBranchTagCompatible}\" sh \"docker push dockerRegistryId.dkr.ecr.eu-west-1.amazonaws.com/phpunit:${deploymentBranchTagCompatible}\" } } } } without seeing the Dockerfile files, we can advise :\nto build these images in the same pipeline where build and phpunit are run the images are built at the same time so we are sure that we are using the right version apparently the docker build depend on the branch of the project, this should be avoided ssh key is used in docker image, that could lead to a security issue as ssh key is still in the history of images layers even if it has been removed in subsequent layers, check https://stackoverflow.com/a/66897280 for information on how to use ssh-agent instead we could use a single Dockerfile with 2 stages: one stage to generate production image one stage that inherits production stage, used to execute phpunit it has the following advantages : reduce the total image size because of the reuse different docker image layers only one Dockerfile to maintain 2. More advanced and annotated Jenkinsfiles php project with helm push javascript project with docker build and S3 push browser extension build and deployment jenkins library - reusable pipeline generation ","categories":["Jenkins"],"description":"Detailed Jenkinsfile examples with annotations","excerpt":"Detailed Jenkinsfile examples with annotations","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/05-annotated-jenkinsfiles--p1/","tags":["jenkins","jenkinsfile","ci-cd"],"title":"Annotated Jenkinsfiles - Part 1"},{"body":"1. Introduction This example is missing the use of parameters, jenkins library in order to reuse common code\nThis example uses :\npost conditions https://www.jenkins.io/doc/book/pipeline/syntax/#post github plugin to set commit status indicating the result of the build usage of several jenkins plugins, you can check here to get the full list installed on your server and even generate code snippets by adding pipeline-syntax/ to your jenkins server url But it misses:\nusage of inline parameters usage of jenkins library to reuse common code Git updateConditionalGithubCommitStatus Docker pullBuildPushImage check Pipeline syntax documentation\n2. Annotated Jenkinsfile // Define variables for QA environment def String registry_id = 'awsAccountId' def String registry_url = registry_id + '.dkr.ecr.us-east-1.amazonaws.com' def String image_name = 'project' def String image_fqdn_master = registry_url + '/' + image_name + ':master' def String image_fqdn_current_branch = image_fqdn_master // this method is used by several of my pipelines and has been added // to jenkins_library \u003chttps://github.com/fchastanet/jenkins-library/blob/master/src/fchastanet/Git.groovy#L156\u003e void publishStatusToGithub(String status) { step([ $class: \"GitHubCommitStatusSetter\", reposSource: [$class: \"ManuallyEnteredRepositorySource\", url: \"https://github.com/fchastanet/project\"], errorHandlers: [[$class: 'ShallowAnyErrorHandler']], statusResultSource: [ $class: 'ConditionalStatusResultSource', results: [ [$class: 'AnyBuildResult', state: status] ] ] ]); } pipeline { agent { node { // bad practice: try to indicate in your node labels, which feature it // includes for example, here we need docker, label could have been // 'eks-nonprod-docker' label 'eks-nonprod' } } stages { stage ('Checkout') { steps { // checkout is not necessary as it is automatically done checkout scm script { // 'wrap' allows to inject some useful variables like BUILD_USER, // BUILD_USER_FIRST_NAME // see https://www.jenkins.io/doc/pipeline/steps/build-user-vars-plugin/ wrap([$class: 'BuildUser']) { def String displayName = \"#${currentBuild.number}_${BRANCH}_${BUILD_USER}_${DEPLOYMENT}\" // params could have been defined inside the pipeline directly // instead of defining them in jenkins build configuration if (params.DEPLOYMENT == 'staging') { displayName = \"${displayName}_${INSTANCE}\" } // next line allows to change the build name, check addHtmlBadge // plugin function for more advanced usage of this feature, you // check this jenkinsfile 05-02-Annotated-Jenkinsfiles.md currentBuild.displayName = displayName } } } } stage ('Run tests') { steps { // all these sh directives could have been merged into one // it is best to use a separated sh file that could take some parameters // as it is simpler to read and to eventually test separately sh 'docker build -t project-test \"$PWD\"/docker/test' sh 'cp \"$PWD\"/app/config/parameters.yml.dist \"$PWD\"/app/config/parameters.yml' // for better readability and if separated script is not possible, use // continuation line for better readability sh 'docker run -i --rm -v \"$PWD\":/var/www/html/ -w /var/www/html/ project-test /bin/bash -c \"composer install -a \u0026\u0026 ./bin/phpunit -c /var/www/html/app/phpunit.xml --coverage-html /var/www/html/var/logs/coverage/ --log-junit /var/www/html/var/logs/phpunit.xml --coverage-clover /var/www/html/var/logs/clover_coverage.xml\"' } // Run the steps in the post section regardless of the completion status // of the Pipeline’s or stage’s run. // see https://www.jenkins.io/doc/book/pipeline/syntax/#post post { always { // report unit test reports (unit test should generate result using // using junit format) junit 'var/logs/phpunit.xml' // generate coverage page from test results step([ $class: 'CloverPublisher', cloverReportDir: 'var/logs/', cloverReportFileName: 'clover_coverage.xml' ]) // publish html page with the result of the coverage publishHTML( target: [ allowMissing: false, alwaysLinkToLastBuild: false, keepAll: true, reportDir: 'var/logs/coverage/', reportFiles: 'index.html', reportName: \"Coverage Report\" ] ) } } } // this stage will be executed only if previous stage is successful stage('Build image') { when { // this stage is executed only if these conditions returns true expression { return params.DEPLOYMENT == \"staging\" || ( params.DEPLOYMENT == \"prod\" \u0026\u0026 env.GIT_BRANCH == 'origin/master' ) } } steps { script { // this code is used in most of the pipeline and has been centralized // in https://github.com/fchastanet/jenkins-library/blob/master/src/fchastanet/Git.groovy#L39 env.IMAGE_TAG = env.GIT_COMMIT.substring(0, 7) // Update variable for production environment if ( params.DEPLOYMENT == 'prod' ) { registry_id = 'awsDockerRegistryId' registry_url = registry_id + '.dkr.ecr.eu-central-1.amazonaws.com' image_fqdn_master = registry_url + '/' + image_name + ':master' } image_fqdn_current_branch = registry_url + '/' + image_name + ':' + env.IMAGE_TAG } // As jenkins slave machine can be constructed on demand, // it doesn't always contains all docker image cache // here to avoid building docker image from scratch, we are trying to // pull an existing version of the docker image on docker registry // and then build using this image as cache, so all layers not updated // in Dockerfile will not be built again (gain of time) // It is again a recurrent usage in most of the pipelines // so the next 8 lines could be replaced by the call to this method // Docker // pullBuildPushImage https://github.com/fchastanet/jenkins-library/blob/master/src/fchastanet/Docker.groovy#L46 // Pull the master from repository (|| true avoids errors if the image // hasn't been pushed before) sh \"docker pull ${image_fqdn_master} || true\" // Build the image using pulled image as cache // instead of using concatenation, it is more readable to use variable interpolation // Eg: \"docker build --cache-from ${image_fqdn_master} -t ...\" sh 'docker build \\ --cache-from ' + image_fqdn_master + ' \\ -t ' + image_name + ' \\ -f \"$PWD/docker/prod/Dockerfile\" \\ .' } } stage('Deploy image (Staging)') { when { expression { return params.DEPLOYMENT == \"staging\" } } steps { script { // Actually we should always push the image in order to be able to // feed the docker cache for next builds // Again the method Docker pullBuildPushImage https://github.com/fchastanet/jenkins-library/blob/master/src/fchastanet/Docker.groovy#L46 // solves this issue and could be used instead of the next 6 lines // and \"Push image (Prod)\" stage // If building master, we should push the image with the tag master // to benefit from docker cache if ( env.GIT_BRANCH == 'origin/master' ) { sh label:\"Tag the image as master\", script:\"docker tag ${image_name} ${image_fqdn_master}\" sh label:\"Push the image as master\", script:\"docker push ${image_fqdn_master}\" } } sh label:\"Tag the image\", script:\"docker tag ${image_name} ${image_fqdn_current_branch}\" sh label:\"Push the image\", script:\"docker push ${image_fqdn_current_branch}\" // use variable interpolation instead of concatenation sh label:\"Deploy on cluster\", script:\" \\ helm3 upgrade project-\" + params.INSTANCE + \" -i \\ --namespace project-\" + params.INSTANCE + \" \\ --create-namespace \\ --cleanup-on-fail \\ --atomic \\ -f helm/values_files/values-\" + params.INSTANCE + \".yaml \\ --set deployment.php_container.image.pullPolicy=Always \\ --set image.tag=\" + env.IMAGE_TAG + \" \\ ./helm\" } } stage('Push image (Prod)') { when { expression { return params.DEPLOYMENT == \"prod\" \u0026\u0026 env.GIT_BRANCH == 'origin/master'} } // The method Docker pullBuildPushImage https://github.com/fchastanet/jenkins-library/blob/master/src/fchastanet/Docker.groovy#L46 // provides a generic way of managing the pull, build, push of the docker // images, by managing also a common way of tagging docker images steps { sh label:\"Tag the image as master\", script:\"docker tag ${image_name} ${image_fqdn_current_branch}\" sh label:\"Push the image as master\", script:\"docker push ${image_fqdn_current_branch}\" } } } post { always { // mark github commit as built publishStatusToGithub(\"${currentBuild.currentResult}\") } } } This directive is really difficult to read and eventually debug it\nsh 'docker run -i --rm -v \"$PWD\":/var/www/html/ -w /var/www/html/ project-test /bin/bash -c \"composer install -a \u0026\u0026 ./bin/phpunit -c /var/www/html/app/phpunit.xml --coverage-html /var/www/html/var/logs/coverage/ --log-junit /var/www/html/var/logs/phpunit.xml --coverage-clover /var/www/html/var/logs/clover_coverage.xml\"' Another way to write previous directive is to:\nuse continuation line avoid ‘\u0026\u0026’ as it can mask errors, use ‘;’ instead use ‘set -o errexit’ to fail on first error use ‘set -o pipefail’ to fail if eventual piped command is failing ‘set -x’ allows to trace every command executed for better debugging Here a possible refactoring:\nsh '''' docker run -i --rm \\ -v \"$PWD\":/var/www/html/ \\ -w /var/www/html/ \\ project-test \\ /bin/bash -c \"\\ set -x ;\\ set -o errexit ;\\ set -o pipefail ;\\ composer install -a ;\\ ./bin/phpunit \\ -c /var/www/html/app/phpunit.xml \\ --coverage-html /var/www/html/var/logs/coverage/ \\ --log-junit /var/www/html/var/logs/phpunit.xml \\ --coverage-clover /var/www/html/var/logs/clover_coverage.xml \" ''' Note however it is best to use a separated sh file(s) that could take some parameters as it is simpler to read and to eventually test separately. Here a refactoring using a separated sh file:\nrunTests.sh\n#!/bin/bash set -x -o errexit -o pipefail composer install -a ./bin/phpunit \\ -c /var/www/html/app/phpunit.xml \\ --coverage-html /var/www/html/var/logs/coverage/ \\ --log-junit /var/www/html/var/logs/phpunit.xml \\ --coverage-clover /var/www/html/var/logs/clover_coverage.xml jenkinsRunTests.sh\n#!/bin/bash set -x -o errexit -o pipefail docker build -t project-test \"${PWD}/docker/test\" docker run -i --rm \\ -v \"${PWD}:/var/www/html/\" \\ -w /var/www/html/ \\ project-test \\ runTests.sh Then the sh directive becomes simply\nsh 'jenkinsRunTests.sh' ","categories":["Jenkins"],"description":"More annotated Jenkinsfile examples","excerpt":"More annotated Jenkinsfile examples","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/06-annotated-jenkinsfiles--p2/","tags":["jenkins","jenkinsfile","ci-cd"],"title":"Annotated Jenkinsfiles - Part 2"},{"body":"1. Introduction This build will:\npull/build/push docker image used to generate project files lint run Unit tests with coverage build the SPA run accessibility tests build story book and deploy it deploy spa on s3 bucket and refresh cloudflare cache It allows to build for production and qa stages allowing different instances. Every build contains:\na summary of the build git branch git revision target environment all the available Urls: spa url storybook url 2. Annotated Jenkinsfile // anonymized parameters String credentialsId = 'jenkinsCredentialId' def lib = library( identifier: 'jenkins_library@v1.0', retriever: modernSCM([ $class: 'GitSCMSource', remote: 'git@github.com:fchastanet/jenkins-library.git', credentialsId: credentialsId ]) ) def docker = lib.fchastanet.Docker.new(this) def git = lib.fchastanet.Git.new(this) def mail = lib.fchastanet.Mail.new(this) def utils = lib.fchastanet.Utils.new(this) def cloudflare = lib.fchastanet.Cloudflare.new(this) // anonymized parameters String CLOUDFLARE_ZONE_ID = 'cloudflareZoneId' String CLOUDFLARE_ZONE_ID_PROD = 'cloudflareZoneIdProd' String REGISTRY_ID_QA = 'dockerRegistryId' String REACT_APP_PENDO_API_KEY = 'pendoApiKey' String REGISTRY_QA = REGISTRY_ID_QA + '.dkr.ecr.us-east-1.amazonaws.com' String IMAGE_NAME_SPA = 'project-ui' String STAGING_API_URL = 'https://api.host' String INSTANCE_URL = \"https://${params.instanceName}.host\" String REACT_APP_API_BASE_URL_PROD = 'https://ui.host' String REACT_APP_PENDO_SOURCE_DOMAIN = 'https://cdn.eu.pendo.io' String buildBucketPrefix String S3_PUBLIC_URL = 'qa-spa.s3.amazonaws.com/project' String S3_PROD_PUBLIC_URL = 'spa.s3.amazonaws.com/project' List\u003cString\u003e instanceChoices = (1..20).collect { 'project' + it } Map buildInfo = [ apiUrl: '', storyBookAvailable: false, storyBookUrl: '', storyBookDocsUrl: '', spaAvailable: false, spaUrl: '', instanceName: '', ] // add information on summary page def addBuildInfo(buildInfo) { String deployInfo = '' if (buildInfo.spaAvailable) { String formatInstanceName = buildInfo.instanceName ? \" (${buildInfo.instanceName})\" : ''; deployInfo += \"\u003ca href='${buildInfo.spaUrl}'\u003eSPA${formatInstanceName}\u003c/a\u003e\" } if (buildInfo.storyBookAvailable) { deployInfo += \" / \u003ca href='${buildInfo.storyBookUrl}'\u003eStorybook\u003c/a\u003e\" deployInfo += \" / \u003ca href='${buildInfo.storyBookDocsUrl}'\u003eStorybook docs\u003c/a\u003e\" } String summaryHtml = \"\"\" \u003cb\u003ebranch : \u003c/b\u003e${GIT_BRANCH}\u003cbr/\u003e \u003cb\u003erevision : \u003c/b\u003e${GIT_COMMIT}\u003cbr/\u003e \u003cb\u003etarget env : \u003c/b\u003e${params.targetEnv}\u003cbr/\u003e ${deployInfo} \"\"\" removeHtmlBadges id: \"htmlBadge${currentBuild.number}\" addHtmlBadge html: summaryHtml, id: \"htmlBadge${currentBuild.number}\" } pipeline { agent { node { // this image has the features docker and lighthouse label 'docker-base-ubuntu-lighthouse' } } parameters { gitParameter( branchFilter: 'origin/(.*)', defaultValue: 'main', quickFilterEnabled: true, sortMode: 'ASCENDING_SMART', name: 'BRANCH', type: 'PT_BRANCH' ) choice( name: 'targetEnv', choices: ['none', 'testing', 'production'], description: 'Where it should be deployed to? (Default: none - No deploy)' ) booleanParam( name: 'buildStorybook', defaultValue: false, description: 'Build Storybook (will only apply if selected targetEnv is testing)' ) choice( name: 'instanceName', choices: instanceChoices, description: 'Instance name to deploy the revision' ) } stages { stage('Build SPA image') { steps { script { // set build status to pending on github commit step([$class: 'GitHubSetCommitStatusBuilder']) wrap([$class: 'BuildUser']) { currentBuild.displayName = \"#${currentBuild.number}_${BRANCH}_${BUILD_USER}_${targetEnv}\" } branchName = docker.getTagCompatibleFromBranch(env.GIT_BRANCH) shortSha = git.getShortCommitSha(env.GIT_BRANCH) if (params.targetEnv == 'production') { buildBucketPrefix = GIT_COMMIT buildInfo.apiUrl = REACT_APP_API_BASE_URL_PROD s3BaseUrl = 's3://project-spa/project' } else { buildBucketPrefix = params.instanceName buildInfo.instanceName = params.instanceName buildInfo.spaUrl = \"${INSTANCE_URL}/index.html\" buildInfo.apiUrl = STAGING_API_URL s3BaseUrl = 's3://project-qa-spa/project' buildInfo.storyBookUrl = \"${INSTANCE_URL}/storybook/index.html\" buildInfo.storyBookDocsUrl = \"${INSTANCE_URL}/storybook-docs/index.html\" } addBuildInfo(buildInfo) // Setup .env sh \"\"\" set -x echo \"REACT_APP_API_BASE_URL = '${buildInfo.apiUrl}'\" \u003e ./.env echo \"REACT_APP_PENDO_SOURCE_DOMAIN = '${REACT_APP_PENDO_SOURCE_DOMAIN}'\" \u003e\u003e ./.env echo \"REACT_APP_PENDO_API_KEY = '${REACT_APP_PENDO_API_KEY}'\" \u003e\u003e ./.env \"\"\" withCredentials([ sshUserPrivateKey( credentialsId: 'sshCredentialsId', keyFileVariable: 'sshKeyFile') ]) { docker.pullBuildPushImage( buildDirectory: pwd(), // use safer way to inject ssh key during docker build buildArgs: \"--ssh default=\\$sshKeyFile --build-arg USER_ID=\\$(id -u)\", registryImageUrl: \"${REGISTRY_QA}/${IMAGE_NAME_SPA}\", tagPrefix: \"${IMAGE_NAME_SPA}:\", localTagName: \"latest\", tags: [ shortSha, branchName ], pullTags: ['main'] ) } } } } stage('Linting') { steps { sh \"\"\" docker run --rm \\ -v ${env.WORKSPACE}:/app \\ -v /app/node_modules \\ ${IMAGE_NAME_SPA} \\ npm run lint \"\"\" } } stage('UT') { steps { script { sh \"\"\"docker run --rm \\ -v ${env.WORKSPACE}:/app \\ -v /app/node_modules \\ ${IMAGE_NAME_SPA} \\ npm run test:coverage -- --ci \"\"\" junit 'output/junit.xml' // https://plugins.jenkins.io/clover/ step([ $class: 'CloverPublisher', cloverReportDir: 'output/coverage', cloverReportFileName: 'clover.xml', healthyTarget: [ methodCoverage: 70, conditionalCoverage: 70, statementCoverage: 70 ], // build will not fail but be set as unhealthy if coverage goes // below 60% unhealthyTarget: [ methodCoverage: 60, conditionalCoverage: 60, statementCoverage: 60 ], // build will fail if coverage goes below 50% failingTarget: [ methodCoverage: 50, conditionalCoverage: 50, statementCoverage: 50 ] ]) } } } stage('Build SPA') { steps { script { sh \"\"\" docker run --rm \\ -v ${env.WORKSPACE}:/app \\ -v /app/node_modules \\ ${IMAGE_NAME_SPA} \"\"\" } } } stage('Accessibility tests') { steps { script { // the pa11y-ci could have been made available in the node image // to avoid installation each time, the build is launched sh ''' sudo npm install -g serve pa11y-ci serve -s build \u003e /dev/null 2\u003e\u00261 \u0026 pa11y-ci --threshold 5 http://127.0.0.1:3000 ''' } } } stage('Build Storybook') { steps { whenOrSkip( params.targetEnv == 'testing' \u0026\u0026 params.buildStorybook == true ) { script { sh \"\"\" docker run --rm \\ -v ${env.WORKSPACE}:/app \\ -v /app/node_modules \\ ${IMAGE_NAME_SPA} \\ sh -c 'npm run storybook:build -- --output-dir build/storybook \\ \u0026\u0026 npm run storybook:build-docs -- --output-dir build/storybook-docs' \"\"\" buildInfo.storyBookAvailable = true } } } } stage('Artifacts to S3') { steps { whenOrSkip(params.targetEnv != 'none') { script { if (params.targetEnv == 'production') { utils.initAws('arn:aws:iam::awsIamId:role/JenkinsSlave') } sh \"aws s3 cp ${env.WORKSPACE}/build ${s3BaseUrl}/${buildBucketPrefix} --recursive --no-progress\" sh \"aws s3 cp ${env.WORKSPACE}/build ${s3BaseUrl}/project1 --recursive --no-progress\" if (params.targetEnv == 'production') { echo 'project SPA packages have been pushed to production bucket.' echo '''You can refresh the production indexes with the CD production pipeline.''' cloudflare.zonePurge(CLOUDFLARE_ZONE_ID_PROD, [prefixes:[ \"${S3_PROD_PUBLIC_URL}/project1/\" ]]) } else { cloudflare.zonePurge(CLOUDFLARE_ZONE_ID, [prefixes:[ \"${S3_PUBLIC_URL}/${buildBucketPrefix}/\" ]]) buildInfo.spaAvailable = true publishChecks detailsURL: buildInfo.spaUrl, name: 'projectSpaUrl', title: 'project SPA url' } addBuildInfo(buildInfo) } } } } } post { always { script { git.updateConditionalGithubCommitStatus() mail.sendConditionalEmail() } } } } ","categories":["Jenkins"],"description":"Additional Jenkinsfile pattern examples","excerpt":"Additional Jenkinsfile pattern examples","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/07-annotated-jenkinsfiles--p3/","tags":["jenkins","jenkinsfile","ci-cd"],"title":"Annotated Jenkinsfiles - Part 3"},{"body":"1. introduction The project aim is to create a browser extension available on chrome and firefox\nThis build allows to:\nlint the project using megalinter and phpstorm inspection build necessary docker images build firefox and chrome extensions deploy firefox extension on s3 bucket deploy chrome extension on google play store 2. Annotated Jenkinsfile def credentialsId = 'jenkinsSshCredentialsId' def lib = library( identifier: 'jenkins_library', retriever: modernSCM([ $class: 'GitSCMSource', remote: 'git@github.com:fchastanet/jenkins-library.git', credentialsId: credentialsId ]) ) def docker = lib.fchastanet.Docker.new(this) def git = lib.fchastanet.Git.new(this) def mail = lib.fchastanet.Mail.new(this) def String deploymentBranchTagCompatible = '' def String gitShortSha = '' def String REGISTRY_URL = 'dockerRegistryId.dkr.ecr.eu-west-1.amazonaws.com' def String ECR_BROWSER_EXTENSION_BUILD = 'browser_extension_lint' def String BUILD_TAG = 'build' def String PHPSTORM_TAG = 'phpstorm-inspections' def String REFERENCE_JOB_NAME = 'Browser_extension_deploy' def String FIREFOX_S3_BUCKET = 'browser-extensions' // it would have been easier to use checkboxes to avoid 'both'/'none' // complexity def DEPLOY_CHROME = (params.targetStore == 'both' || params.targetStore == 'chrome') def DEPLOY_FIREFOX = (params.targetStore == 'both' || params.targetStore == 'firefox') pipeline { agent { node { label 'docker-base-ubuntu' } } parameters { gitParameter branchFilter: 'origin/(.*)', defaultValue: 'master', quickFilterEnabled: true, sortMode: 'ASCENDING_SMART', name: 'BRANCH', type: 'PT_BRANCH' choice ( name: 'targetStore', choices: ['none', 'both', 'chrome', 'firefox'], description: 'Where it should be deployed to? (Default: none, has effect only on master branch)' ) } environment { GOOGLE_CREDS = credentials('GoogleApiChromeExtension') GOOGLE_TOKEN = credentials('GoogleApiChromeExtensionCode') GOOGLE_APP_ID = 'googleAppId' // provided by https://addons.mozilla.org/en-US/developers/addon/api/key/ FIREFOX_CREDS = credentials('MozillaApiFirefoxExtension') FIREFOX_APP_ID='{d4ce8a6f-675a-4f74-b2ea-7df130157ff4}' } stages { stage(\"Init\") { steps { script { deploymentBranchTagCompatible = docker.getTagCompatibleFromBranch(env.GIT_BRANCH) gitShortSha = git.getShortCommitSha(env.GIT_BRANCH) echo \"Branch ${env.GIT_BRANCH}\" echo \"Docker tag = ${deploymentBranchTagCompatible}\" echo \"git short sha = ${gitShortSha}\" } sh 'echo StrictHostKeyChecking=no \u003e\u003e ~/.ssh/config' } } stage(\"Lint\") { agent { docker { image 'megalinter/megalinter-javascript:v5' args \"-u root -v ${WORKSPACE}:/tmp/lint --entrypoint=''\" reuseNode true } } steps { sh 'npm install stylelint-config-rational-order' sh '/entrypoint.sh' } } stage(\"Build docker images\") { steps { // whenOrSkip directive is defined in https://github.com/fchastanet/jenkins-library/blob/master/vars/whenOrSkip.groovy whenOrSkip(currentBuild.currentResult == \"SUCCESS\") { script { docker.pullBuildPushImage( buildDirectory: 'build', registryImageUrl: \"${REGISTRY_URL}/${ECR_BROWSER_EXTENSION_BUILD}\", tagPrefix: \"${ECR_BROWSER_EXTENSION_BUILD}:\", tags: [ \"${BUILD_TAG}_${gitShortSha}\", \"${BUILD_TAG}_${deploymentBranchTagCompatible}\", ], pullTags: [\"${BUILD_TAG}_master\"] ) } } } } stage(\"Build firefox/chrome extensions\") { steps { whenOrSkip(currentBuild.currentResult == \"SUCCESS\") { script { sh \"\"\" docker run \\ -v \\$(pwd):/deploy \\ --rm '${ECR_BROWSER_EXTENSION_BUILD}' \\ /deploy/build/build-extensions.sh \"\"\" // multiple git statuses can be set on a given commit // you can configure github to authorize pull request merge // based on the presence of one or more github statuses git.updateGithubCommitStatus(\"BUILD_OK\") } } } } stage(\"Deploy extensions\") { // deploy both extensions in parallel parallel { stage(\"Deploy chrome\") { steps { whenOrSkip(currentBuild.currentResult == \"SUCCESS\" \u0026\u0026 DEPLOY_CHROME) { // do not fail the entire build if this stage fail // so firefox stage can be executed catchError(buildResult: 'SUCCESS', stageResult: 'FAILURE') { script { // best practice: complex sh files have been created outside // of this jenkinsfile deploy-chrome-extension.sh sh \"\"\" docker run \\ -v \\$(pwd):/deploy \\ -e APP_CREDS_USR='${GOOGLE_CREDS_USR}' \\ -e APP_CREDS_PSW='${GOOGLE_CREDS_PSW}' \\ -e APP_TOKEN='${GOOGLE_APP_TOKEN}' \\ -e APP_ID='${GOOGLE_APP_ID}' \\ --rm '${ECR_BROWSER_EXTENSION_BUILD}' \\ /deploy/build/deploy-chrome-extension.sh \"\"\" git.updateGithubCommitStatus(\"CHROME_DEPLOYED\") } } } } } stage(\"Deploy firefox\") { steps { whenOrSkip(currentBuild.currentResult == \"SUCCESS\" \u0026\u0026 DEPLOY_FIREFOX) { catchError(buildResult: 'SUCCESS', stageResult: 'FAILURE') { script { // best practice: complex sh files have been created outside // of this jenkinsfile deploy-firefox-extension.sh sh \"\"\" docker run \\ -v \\$(pwd):/deploy \\ -e FIREFOX_JWT_ISSUER='${FIREFOX_CREDS_USR}' \\ -e FIREFOX_JWT_SECRET='${FIREFOX_CREDS_PSW}' \\ -e FIREFOX_APP_ID='${FIREFOX_APP_ID}' \\ --rm '${ECR_BROWSER_EXTENSION_BUILD}' \\ /deploy/build/deploy-firefox-extension.sh \"\"\" sh \"\"\" set -x set -o errexit extensionVersion=\"\\$(jq -r .version \u003c package.json)\" extensionFilename=\"tools-\\${extensionVersion}-an+fx.xpi\" echo \"Upload new extension \\${extensionFilename} to s3 bucket ${FIREFOX_S3_BUCKET}\" aws s3 cp \"\\$(pwd)/packages/\\${extensionFilename}\" \"s3://${FIREFOX_S3_BUCKET}\" aws s3api put-object-acl --bucket \"${FIREFOX_S3_BUCKET}\" --key \"\\${extensionFilename}\" --acl public-read # url is https://tools.s3.eu-west-1.amazonaws.com/tools-2.5.6-an%2Bfx.xpi echo \"Upload new version as current version\" aws s3 cp \"\\$(pwd)/packages/\\${extensionFilename}\" \"s3://${FIREFOX_S3_BUCKET}/tools-an+fx.xpi\" aws s3api put-object-acl --bucket \"${FIREFOX_S3_BUCKET}\" --key \"tools-an+fx.xpi\" --acl public-read # url is https://tools.s3.eu-west-1.amazonaws.com/tools-an%2Bfx.xpi echo \"Upload updates.json file\" aws s3 cp \"\\$(pwd)/packages/updates.json\" \"s3://${FIREFOX_S3_BUCKET}\" aws s3api put-object-acl --bucket \"${FIREFOX_S3_BUCKET}\" --key \"updates.json\" --acl public-read # url is https://tools.s3.eu-west-1.amazonaws.com/updates.json \"\"\" git.updateGithubCommitStatus(\"FIREFOX_DEPLOYED\") } } } } } } } } post { always { script { archiveArtifacts artifacts: 'report/mega-linter.log' archiveArtifacts artifacts: 'report/linters_logs/*' archiveArtifacts artifacts: 'packages/*', fingerprint: true, allowEmptyArchive: true // send email to the builder and culprits of the current commit // culprits are the committers since the last commit successfully built mail.sendConditionalEmail() git.updateConditionalGithubCommitStatus() } } success { script { if (params.targetStore != 'none' \u0026\u0026 env.GIT_BRANCH == 'origin/master') { // send an email to a teams channel so every collaborators knows // when a production ready extension has been deployed mail.sendSuccessfulEmail('teamsChannelId.onmicrosoft.com@amer.teams.ms') } } } } } ","categories":["Jenkins"],"description":"Complex Jenkinsfile scenarios","excerpt":"Complex Jenkinsfile scenarios","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/08-annotated-jenkinsfiles--p4/","tags":["jenkins","jenkinsfile","ci-cd"],"title":"Annotated Jenkinsfiles - Part 4"},{"body":"1. introduction In jenkins library you can create your own directive that allows to generate jenkinsfile code. Here we will use this feature to generate a complete Jenkinsfile.\n2. Annotated Jenkinsfile library identifier: 'jenkins_library@v1.0', retriever: modernSCM([ $class: 'GitSCMSource', remote: 'git@github.com:fchastanet/jenkins-library.git', credentialsId: 'jenkinsCredentialsId' ]) djangoApiPipeline repoUrl: 'git@github.com:fchastanet/django_api_project.git', imageName: 'django_api' 3. Annotated library custom directive In the jenkins library just add a file named vars/djangoApiPipeline.groovy with the following content\n#!/usr/bin/env groovy def call(Map args) { // content of your pipeline } 4. Annotated library custom directive djangoApiPipeline.groovy #!/usr/bin/env groovy def call(Map args) { def gitUtil = new Git(this) def mailUtil = new Mail(this) def dockerUtil = new Docker(this) def kubernetesUtil = new Kubernetes(this) def testUtil = new Tests(this) String workerLabelNonProd = args?.workerLabelNonProd ?: 'eks-nonprod' String workerLabelProd = args?.workerLabelProd ?: 'docker-ubuntu-prod-eks' String awsRegionNonProd = workerLabelNonProd == 'eks-nonprod' ? 'us-east-1' : 'eu-west-1' String awsRegionProd = 'eu-central-1' String regionName = params.targetEnv == 'prod' ? awsRegionProd : awsRegionNonProd String teamsEmail = args?.teamsEmail ?: 'teamsChannel.onmicrosoft.com@amer.teams.ms' String helmDirectory = args?.helmDirectory ?: './helm' Boolean sendCortexMetrics = args?.sendCortexMetrics ?: false Boolean skipTests = args?.skipTests ?: false List environments = args?.environments ?: ['none', 'qa', 'prod'] Short skipBuild = 0 pipeline { agent { node { label params.targetEnv == 'prod' ? workerLabelProd : workerLabelNonProd } } parameters { gitParameter branchFilter: 'origin/(.*)', defaultValue: 'main', quickFilterEnabled: true, sortMode: 'ASCENDING_SMART', name: 'BRANCH', type: 'PT_BRANCH' choice ( name: 'targetEnv', choices: environments, description: 'Where it should be deployed to? (Default: none - No deploy)' ) string ( name: 'instance', defaultValue: '1', description: '''The instance ID to define which QA instance it should be deployed to (Will only apply if targetEnv is qa). Default is 1 for CK and 01 for Darwin''' ) booleanParam( name: 'suspendCron', defaultValue: true, description: 'Suspend cron jobs scheduling' ) choice ( name: 'upStreamImage', choices: ['latest', 'beta'], description: '''Select beta to check if your build works with the future version of the upstream image''' ) } stages { stage('Checkout from SCM') { steps { script { echo \"Checking out from origin/${BRANCH} branch\" gitUtil.branchCheckout( '', 'babee6c1-14fe-4d90-9da0-ffa7068c69af', args.repoUrl, '${BRANCH}' ) wrap([$class: 'BuildUser']) { def String displayName = \"#${currentBuild.number}_${BRANCH}_${BUILD_USER}_${targetEnv}\" if (params.targetEnv == 'qa' || params.targetEnv == 'qe') { displayName = \"${displayName}_${instance}\" } currentBuild.displayName = displayName } env.imageName = env.BUILD_TAG.toLowerCase() env.buildDirectory = args?.buildDirectory ? args.buildDirectory + \"/\" : \"\" env.runCoverage = args?.runCoverage env.shortSha = gitUtil.getShortCommitSha(env.GIT_BRANCH) skipBuild = dockerUtil.checkImage(args.imageName, shortSha) } } } stage('Build') { when { expression { return skipBuild != 0 } } steps { script { String registryUrl = 'dockerRegistryId.dkr.ecr.' + awsRegionNonProd + '.amazonaws.com' String buildDirectory = args?.buildDirectory ?: pwd() if (params.targetEnv == \"prod\") { registryUrl = 'dockerRegistryId.dkr.ecr.' + awsRegionProd + '.amazonaws.com' } dockerUtil.pullBuildImage( registryImageUrl: \"${registryUrl}/${args.imageName}\", pullTags: [ \"${params.targetEnv}\" ], buildDirectory: \"${buildDirectory}\", buildArgs: \"--build-arg UPSTREAM_VERSION=${params.upStreamImage}\", tagPrefix: \"${env.imageName}:\", tags: [ \"${env.shortSha}\" ] ) } } } stage('Test') { when { expression { return skipBuild != 0 \u0026\u0026 skipTests == false } } steps { script { testUtil.execTests(args.imageName) } } } stage('Push') { when { expression { return params.targetEnv != 'none' } } steps { script { //pipeline execution starting time for CD part Map argsMap = [:] if (params.targetEnv == \"prod\") { registryUrl = 'registryIdProd.dkr.ecr.' + awsRegionProd + '.amazonaws.com' } else { registryUrl = 'registryIdNonProd.dkr.ecr.' + awsRegionNonProd + '.amazonaws.com' } argsMap = [ registryImageUrl: \"${registryUrl}/${args.imageName}\", pullTags: [ \"${env.shortSha}\", ], tagPrefix: \"${registryUrl}/${args.imageName}:\", localTagName: \"${env.shortSha}\", tags: [ \"${params.targetEnv}\" ] ] if (skipBuild == 0) { dockerUtil.promoteTag(argsMap) } else { argsMap.remove(\"pullTags\") argsMap.put(\"tagPrefix\", \"${env.imageName}:\") argsMap.put(\"tags\", [\"${env.shortSha}\",\"${params.targetEnv}\"]) dockerUtil.tagPushImage(argsMap) } } } } stage(\"Deploy to Kubernetes\") { when { expression { return params.targetEnv != 'none' } } steps { script { if (params.targetEnv == 'prod') { // not sure it is a good practice as it forces the operator to // wait for build to reach this stage timeout(time: 300, unit: \"SECONDS\") { input( message: \"\"\"Do you want go ahead with ${env.shortSha} image tag for prod helm deploy?\"\"\", ok: 'Yes' ) } } CHART_NAME = (args.imageName).contains(\"_\") ? (args.imageName).replaceAll(\"_\", \"-\") : (args.imageName) if (params.targetEnv == 'qa' || params.targetEnv == 'qe') { helmValueFilePath = \"${helmDirectory}\" + \"/value_files/values-\" + params.targetEnv + params.instance + \".yaml\" NAMESPACE = \"${CHART_NAME}-\" + params.targetEnv + params.instance } else { helmValueFilePath = \"${helmDirectory}\" + \"/value_files/values-\" + params.targetEnv + \".yaml\" NAMESPACE = \"${CHART_NAME}-\" + params.targetEnv } ingressUrl = kubernetesUtil.getIngressUrl(helmValueFilePath) echo \"Deploying into k8s..\" echo \"Helm release: ${CHART_NAME}\" echo \"Target env: ${params.targetEnv}\" echo \"Url: ${ingressUrl}\" echo \"K8s namespace: ${NAMESPACE}\" kubernetesUtil.deployHelmChart( chartName: CHART_NAME, nameSpace: NAMESPACE, imageTag: \"${env.shortSha}\", helmDirectory: \"${helmDirectory}\", helmValueFilePath: helmValueFilePath ) } } } } post { always { script { gitUtil.updateGithubCommitStatus(\"${currentBuild.currentResult}\", \"${env.WORKSPACE}\") mailUtil.sendConditionalEmail() if (params.targetEnv == 'prod') { mailUtil.sendTeamsNotification(teamsEmail) } } } } } } 5. Final thoughts about this technique This technique is really useful when you have a lot of similar projects reusing over and over the same pipeline. It allows:\ncode reuse avoid duplicated code easier maintenance However it has the following drawbacks:\nsome projects using this generic pipeline could have specific needs eg 1: not the same way to run unit tests, to overcome that issue the method testUtil.execTests is used allowing to run a specific sh file if it exists eg 2: more complex way to launch docker environment … be careful, when you upgrade this jenkinsfile as all the projects using it will be upgraded at once it could be seen as an advantage, but it is also a big risk as it could impact all the prod environment at once to overcome that issue I suggest to use library versioning when using the jenkins library in your project pipeline Eg: check Annotated Jenkinsfile @v1.0 when cloning library project I highly suggest to use a unit test framework of the library to avoid at most bad surprises In conclusion, I’m still not sure it is a best practice to generate pipelines like this.\n","categories":["Jenkins"],"description":"Advanced Jenkinsfile techniques","excerpt":"Advanced Jenkinsfile techniques","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/09-annotated-jenkinsfiles--p5/","tags":["jenkins","jenkinsfile","ci-cd"],"title":"Annotated Jenkinsfiles - Part 5"},{"body":" 1. Jenkins snippet generator 2. Declarative pipeline allows you to restart a build from a given stage 3. Replay a pipeline 4. VS code Jenkinsfile validation 5. How to chain pipelines ? 6. Viewing pipelines hierarchy 1. Jenkins snippet generator Use jenkins snippet generator by adding /pipeline-syntax/ to your jenkins pipeline. to allow you to generate jenkins pipeline code easily with inline doc. It also list the available variables.\n2. Declarative pipeline allows you to restart a build from a given stage 3. Replay a pipeline Replaying a pipeline allows you to update your jenkinsfile before replaying the pipeline, easier debugging ! 4. VS code Jenkinsfile validation Please follow this documentation enable jenkins pipeline linter in vscode\n5. How to chain pipelines ? Simply use the build directive followed by the name of the build to launch\nbuild 'OtherBuild' 6. Viewing pipelines hierarchy The downstream-buildview plugin allows to view the full chain of dependent builds.\n","categories":["Jenkins"],"description":"Common recipes and troubleshooting tips","excerpt":"Common recipes and troubleshooting tips","ref":"/my-documents/docs/howtos/howto-write-jenkinsfile/10-jenkins-recipes-and-tips/","tags":["jenkins","jenkinsfile","ci-cd"],"title":"Jenkins Recipes and Tips"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/ai-generated/","tags":"","title":"Ai-Generated"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/authentication/","tags":"","title":"Authentication"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/categories/bash/","tags":"","title":"Bash"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/bash/","tags":"","title":"Bash"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/best-practices/","tags":"","title":"Best-Practices"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/categories/brainstorming/","tags":"","title":"Brainstorming"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/ci-cd/","tags":"","title":"Ci-Cd"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/development-documentation-seo/","tags":"","title":"Development, Documentation, Seo]"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/docsy/","tags":"","title":"Docsy"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/categories/documentation/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/documentation/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/github-actions/","tags":"","title":"Github-Actions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/github-app/","tags":"","title":"Github-App"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/hugo/","tags":"","title":"Hugo"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/categories/jenkins/","tags":"","title":"Jenkins"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/jenkins/","tags":"","title":"Jenkins"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/jenkinsfile/","tags":"","title":"Jenkinsfile"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/multi-site/","tags":"","title":"Multi-Site"},{"body":" Welcome to My Documents, a personal collection of notes, guides, and resources on various topics related to development, testing, and documentation. This site is organized into several sections, each containing detailed content and resources to help you explore and learn about different aspects of software development and documentation.\n","categories":"","description":"","excerpt":" Welcome to My Documents, a personal collection of notes, guides, and …","ref":"/my-documents/","tags":"","title":"My Documents"},{"body":"My Documents - Static Site Generation Using Hugo - Migration Analysis and Implementation My Documents - Static Site Generation Using Hugo - Migration Analysis and Implementation 1. Technical Solutions Evaluated 1.1. Static Site Generator Solutions 1.1.1. Hugo (SELECTED) 1.1.2. Astro 1.1.3. 11ty (Eleventy) 1.1.4. VuePress 2 1.1.5. MkDocs 1.1.6. Next.js and Gatsby 1.1.7. Comparison Summary 1.2. Multi-Site Build Pipeline Solutions 1.2.1. Centralized Orchestrator (my-documents builds all sites) (SELECTED) 1.2.2. Decentralized with Reusable Workflows + Hugo Modules 1.2.3. True Monorepo with Subdirectories 1.2.4. Pipeline Solution Comparison 2. Chosen Solutions \u0026 Rationale 2.1. Static Site Generator: Hugo + Docsy Theme 2.2. Multi-Site Pipeline: Centralized Orchestrator 3. Implementation Details 3.1. Repository Architecture 3.2. Directory Structure 3.2.1. my-documents (Orchestrator) 3.2.2. Dependent Repository (Example: bash-compiler) 3.3. Configuration Merging Strategy 3.4. Build Workflow 3.5. Deployment Approach 3.6. Trigger Mechanism 3.7. Theme Customization 4. Lessons Learned \u0026 Future Considerations 4.1. GitHub App Migration from Deploy Keys 4.2. Trade-offs Discovered 4.2.1. All-Site Rebuild Trade-off 4.2.2. Authentication Complexity 4.2.3. Configuration Flexibility vs Consistency 4.3. Best Practices Identified 4.3.1. Configuration Management 4.3.2. Build Optimization 4.3.3. Dependency Management 4.3.4. Security 4.4. Future Considerations 4.4.1. Potential Optimizations 4.4.2. Scalability Considerations 4.4.3. Alternative Approaches for Future Projects 4.5. Success Metrics 5. Conclusion Project: Migration from Docsify to Hugo with Docsy theme for multiple documentation repositories\nStatus: ✅ Completed\nRepositories:\nfchastanet/my-documents (orchestrator + own documentation) fchastanet/bash-compiler fchastanet/bash-tools fchastanet/bash-tools-framework fchastanet/bash-dev-env Related Documentation: See doc/ai/2026-02-18-migrate-repo-from-docsify-to-hugo.md for detailed migration guide.\n1. Technical Solutions Evaluated 1.1. Static Site Generator Solutions 1.1.1. Hugo (SELECTED) Evaluation: ⭐⭐⭐⭐⭐ Type: Go-based static site generator\nPros:\nExtremely fast compilation (\u003c1s for most documentation sites) Excellent for documentation with purpose-built features Superior SEO support (static HTML, sitemaps, feeds, schemas) - 9/10 SEO score Single binary with no dependency complications Markdown + frontmatter support (natural progression from Docsify) GitHub Actions ready with official actions Large theme ecosystem (500+ themes) including specialized documentation themes Built-in features: search indexes, RSS feeds, hierarchical content organization Output optimization: image processing, minification, CSS purging Active community with frequent updates Multi-language support built-in Cons:\nLearning curve for Go templating (shortcodes, partials) Theme customization requires understanding Hugo’s page model Configuration in YAML/TOML format GitHub CI/CD Integration: Native, simple integration with peaceiris/actions-hugo\nBest For: Technical documentation, multi-site architecture, SEO-critical sites, GitHub Pages, content-heavy sites\n1.1.2. Astro Evaluation: ⭐⭐⭐⭐ Type: JavaScript/TypeScript-based with island architecture\nPros:\nOutstanding SEO support (static HTML, zero JavaScript by default) - 9/10 SEO score Modern JavaScript patterns with TypeScript support Markdown + MDX support (embedded React/Vue components in Markdown) Island architecture minimizes JavaScript shipping Fast performance and build times (\u003c2s) Automatic image optimization (AVIF support) Vite-based with fast HMR Cons:\nNewer ecosystem, less battle-tested than Hugo Requires Node.js and npm dependency management Smaller theme ecosystem MDX adds complexity if not needed Best For: Modern tech stacks, interactive components, TypeScript-heavy teams, blogs + documentation hybrids\n1.1.3. 11ty (Eleventy) Evaluation: ⭐⭐⭐⭐\nType: JavaScript template engine\nPros:\nIncredibly flexible with multiple template language support Lightweight and fast builds JavaScript-based (easier for Node.js teams) Low barrier to entry No framework lock-in Cons:\nLess opinionated, requires more configuration Smaller pre-built theme ecosystem No built-in search (requires plugins) SEO score: 8/10 Best For: Developers wanting full control, JavaScript/Node.js teams, unique design requirements\n1.1.4. VuePress 2 Evaluation: ⭐⭐⭐\nType: Vue 3-based static site generator\nPros:\nDocumentation-first design Built-in search functionality Plugin ecosystem for documentation Vue component integration in Markdown Cons:\nVue.js knowledge required Heavy JavaScript bundle (not as optimized as others) Smaller ecosystem than Hugo SEO score: 6/10 Best For: Vue-centric teams, smaller documentation sites\n1.1.5. MkDocs Evaluation: ⭐⭐⭐\nType: Python-based documentation generator\nPros:\nDocumentation-optimized out of the box Simple configuration Material for MkDocs theme is excellent Fast builds Cons:\nPython dependency management required Smaller ecosystem than Hugo Limited flexibility SEO score: 7/10 Best For: Documentation-only focus, Python-familiar teams, rapid setup\n1.1.6. Next.js and Gatsby Evaluation: ⭐⭐ - Not recommended for static documentation\nReasons:\nOverkill complexity for pure documentation Longer build times (5-30s vs \u003c1s for Hugo) Heavy JavaScript requirements Optimized for different use cases (web apps, not docs) Maintenance burden too high for static documentation 1.1.7. Comparison Summary Criteria Hugo Astro 11ty VuePress MkDocs SEO Score 9/10 9/10 8/10 6/10 7/10 Build Speed ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ Learning Curve ⭐⭐⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐ GitHub Pages ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ Documentation Focus ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ Theme Ecosystem ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ Multi-Site Support ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ 1.2. Multi-Site Build Pipeline Solutions 1.2.1. Centralized Orchestrator (my-documents builds all sites) (SELECTED) Evaluation: ⭐⭐⭐⭐⭐ Architecture:\nmy-documents (orchestrator) ├── .github/workflows/build-all-sites.yml ← Builds all sites ├── configs/ │ ├── _base.yaml ← Shared config │ ├── bash-compiler.yaml ← Site overrides │ ├── bash-tools.yaml │ └── bash-tools-framework.yaml ├── shared/ │ ├── layouts/ ← Shared templates │ ├── assets/ ← Shared styles │ └── archetypes/ ← Content templates └── content/ ← my-documents own docs Dependent repos (minimal): bash-compiler/ ├── .github/workflows/trigger-docs.yml ← Triggers my-documents └── content/en/ ← Documentation only How It Works:\nPush to bash-compiler → triggers my-documents via repository_dispatch my-documents workflow: Checks out ALL repos (my-documents, bash-compiler, bash-tools, bash-tools-framework, bash-dev-env) Builds each site in parallel using GitHub Actions matrix strategy Merges configs (_base.yaml + site-specific overrides) Deploys each site to its respective GitHub Pages Pros:\n✅ All repos under same owner (fchastanet) simplifies permission management ✅ One workflow update fixes all sites immediately ✅ Guaranteed consistency across all documentation sites ✅ Simpler per-repo setup (2 files: trigger workflow + content) ✅ No Hugo modules needed (simpler dependency management) ✅ Centralized theme customization with per-site overrides ✅ Build all sites in ~60s (parallel matrix execution) ✅ Single point of maintenance Cons:\n⚠️ Requires authentication setup (GitHub App or deploy keys) ⚠️ All sites rebuild together (cannot isolate to single site) ⚠️ All-or-nothing failures (one site failure blocks others in same matrix job) ⚠️ Slightly more complex initial setup Best For: Related projects under same organization, shared theme/purpose, centralized maintenance preference\n1.2.2. Decentralized with Reusable Workflows + Hugo Modules Architecture:\nmy-documents (shared resources hub) ├── .github/workflows/hugo-build-deploy-reusable.yml ← Reusable workflow ├── layouts/ (Hugo module export) └── assets/ (Hugo module export) bash-compiler/ (independent) ├── .github/workflows/hugo-build-deploy.yml ← Calls reusable workflow ├── hugo.yaml (imports my-documents module) ├── go.mod └── content/ How It Works:\nEach dependent repo has its own build workflow Workflow calls the reusable workflow from my-documents Hugo modules pull shared resources during build Each site builds and deploys independently Pros:\n✅ Independent deployment (site failures isolated) ✅ Automatic updates when reusable workflow changes ✅ Version control (can pin to @v1.0.0 or @master) ✅ No trigger coordination needed ✅ Faster builds for single-site changes (~30s per site) ✅ Per-repo flexibility if needed Cons:\n⚠️ Hugo modules require Go toolchain ⚠️ More files per repository (6 core files vs 2) ⚠️ Learning curve for Hugo module system ⚠️ Network dependency (modules fetched from GitHub) ⚠️ Potential configuration drift if repos don’t update modules ⚠️ More complex to enforce consistency Best For: Fully independent projects, teams wanting flexibility, isolated failure tolerance\n1.2.3. True Monorepo with Subdirectories Architecture: All content in single repo with subdirectories for each project\nPros:\n✅ Simplest configuration ✅ Single build process ✅ Guaranteed consistency Cons:\n❌ Loses separate GitHub Pages URLs ❌ No independent repository control ❌ Violates existing repository structure ❌ Complicated permission management Evaluation: Not recommended - Conflicts with requirement to maintain separate repository URLs\n1.2.4. Pipeline Solution Comparison Criteria Centralized Orchestrator Decentralized Reusable Monorepo Complexity Low (minimal per-repo) Medium (per-repo setup) Low (single repo) Build Time ~60s all sites ~30s per site ~60s all sites Maintenance Update once Update workflow × N Update once Consistency ✅ Guaranteed Can drift ✅ Guaranteed Failure Isolation All-or-nothing ✅ Independent All-or-nothing Setup Effort 1 workflow + N configs 6 files × N repos Single setup Independent URLs ✅ Yes ✅ Yes ❌ No Hugo Modules ❌ Not needed Required ❌ Not needed 2. Chosen Solutions \u0026 Rationale 2.1. Static Site Generator: Hugo + Docsy Theme Choice: Hugo with Google’s Docsy theme\nRationale:\nSEO Requirements Met:\nStatic HTML pre-rendering (search engines can easily index) Automatic sitemap and robots.txt generation Per-page meta tags and structured data support RSS/Atom feeds Image optimization Performance optimizations (minification, compression) SEO improvement: 2/10 (Docsify) → 9/10 (Hugo) Technical Excellence:\nExtremely fast builds (\u003c1s for typical documentation site) Simple deployment (single Go binary, no dependency hell) GitHub Pages native support Mature, stable, battle-tested (10+ years in production use) Documentation-Specific Features:\nDocsy theme built by Google specifically for documentation Built-in search functionality Responsive design Navigation auto-generation from content structure Version management support Multi-language support Developer Experience:\nMarkdown + frontmatter (minimal migration effort from Docsify) Good documentation and large community Extensive theme ecosystem Active development and updates Multi-Site Architecture Support:\nExcellent support for shared configurations Hugo modules for code reuse Flexible configuration merging Content organization flexibility Alternatives Considered:\nAstro: Excellent option, but newer ecosystem and Node.js dependency management adds complexity 11ty: Good flexibility, but less opinionated structure requires more setup work MkDocs: Python dependencies and smaller ecosystem less ideal VuePress/Next.js/Gatsby: Too heavy for pure documentation needs 2.2. Multi-Site Pipeline: Centralized Orchestrator Choice: Centralized build orchestrator in my-documents repository\nRationale:\nProject Context Alignment:\nAll repositories under same owner (fchastanet) All share same purpose (Bash tooling documentation) All need consistent look and feel Related projects benefit from coordinated updates Maintenance Efficiency:\nSingle workflow update affects all sites immediately One place to fix bugs or add features Guaranteed consistency across all documentation Reduced mental overhead (one system to understand) Simplified Per-Repository Structure:\nOnly 2 essential files per dependent repo: Trigger workflow (10 lines) Content directory No Hugo configuration duplication No Go module management per repo Configuration Management:\nBase configuration shared via configs/_base.yaml Site-specific overrides in configs/{site}.yaml Automatic merging with yq tool No configuration drift possible Build Efficiency:\nParallel matrix execution builds all 5 sites simultaneously Total time ~60s for all sites (vs 30s × 5 = 150s sequential) Resource sharing in CI/CD (single Hugo/Go setup) Deployment Simplification:\nAuthentication centralized in my-documents (GitHub App) Single set of deployment credentials Easier to audit and manage security Trade-offs Accepted:\n⚠️ All sites rebuild together (acceptable for related documentation) ⚠️ More complex initial setup (one-time investment) ⚠️ All-or-nothing failures (mitigated with fail-fast: false in matrix) Alternatives Considered:\nDecentralized Reusable Workflows: Good for truly independent projects, but adds complexity without benefit for our use case where all sites are related and share theme/purpose Monorepo: Would lose independent GitHub Pages URLs, not acceptable 3. Implementation Details 3.1. Repository Architecture Orchestrator Repository: fchastanet/my-documents\nResponsibilities:\nBuild all documentation sites (including its own) Manage shared configurations and theme customizations Deploy to multiple GitHub Pages repositories Coordinate builds triggered from dependent repositories Dependent Repositories:\nfchastanet/bash-compiler fchastanet/bash-tools fchastanet/bash-tools-framework fchastanet/bash-dev-env Responsibilities: Contain documentation content only, trigger builds in orchestrator\n3.2. Directory Structure 3.2.1. my-documents (Orchestrator) /home/wsl/fchastanet/my-documents/ ├── .github/workflows/ │ └── build-all-sites.yml ← Orchestrator workflow ├── configs/ │ ├── _base.yaml ← Shared configuration │ ├── my-documents.yaml ← my-documents overrides │ ├── bash-compiler.yaml ← bash-compiler overrides │ ├── bash-tools.yaml │ ├── bash-tools-framework.yaml │ └── bash-dev-env.yaml ├── shared/ │ ├── layouts/ ← Shared Hugo templates │ ├── assets/ ← Shared SCSS, JS │ └── archetypes/ ← Content templates ├── content/ ← my-documents own content ├── hugo.yaml ← Generated per build └── go.mod ← Hugo modules (Docsy) Key Files:\n.github/workflows/build-all-sites.yml - Orchestrator workflow configs/_base.yaml - Shared Hugo configuration configs/bash-compiler.yaml - Example site-specific config shared/ - Shared theme customizations 3.2.2. Dependent Repository (Example: bash-compiler) fchastanet/bash-compiler/ ├── .github/workflows/ │ └── trigger-docs.yml ← Triggers orchestrator └── content/en/ ← Documentation content only ├── _index.md └── docs/ └── *.md 3.3. Configuration Merging Strategy Approach: Use yq tool for proper YAML deep-merging\nBase Configuration: configs/_base.yaml\nContains:\nHugo module imports (Docsy theme) Common parameters (language, SEO settings) Shared markup configuration Mount points for shared resources Common menu structure Default theme parameters Site-Specific Overrides: Example configs/bash-compiler.yaml\nContains:\nSite title and baseURL Repository-specific links Site-specific theme colors (ui.navbar_bg_color) Custom menu items SEO keywords specific to the project GitHub repository links Merging Process:\nImplemented in .github/workflows/build-all-sites.yml:\nyq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' \\ configs/_base.yaml \\ configs/bash-compiler.yaml \u003e hugo.yaml Result: Clean, merged hugo.yaml with:\nBase configuration as foundation Site-specific overrides applied Proper YAML structure preserved (no duplication) Deep merge of nested objects 3.4. Build Workflow Main Workflow: .github/workflows/build-all-sites.yml\nTriggers:\nworkflow_dispatch - Manual trigger repository_dispatch with type trigger-docs-rebuild - From dependent repos push to master branch affecting: content/** shared/** configs/** .github/workflows/build-all-sites.yml Strategy: Parallel matrix build\nmatrix: site: - name: my-documents repo: fchastanet/my-documents baseURL: https://fchastanet.github.io/my-documents self: true - name: bash-compiler repo: fchastanet/bash-compiler baseURL: https://fchastanet.github.io/bash-compiler self: false # ... other sites Build Steps (Per Site):\nCheckout Orchestrator: Clone my-documents repository Checkout Content: Clone dependent repository content (if not self) Setup Tools: Install Hugo Extended 0.155.3, Go 1.24, yq Prepare Build Directory: For my-documents: Use orchestrator directory For dependent repos: Create build-{site} directory Merge Configurations: Combine _base.yaml + {site}.yaml Copy Shared Resources: Link shared layouts, assets, archetypes Copy Content: Link content directory Initialize Hugo Modules: Run hugo mod init and hugo mod get -u Build Site: Run hugo --minify Deploy: Push to respective GitHub Pages Concurrency: cancel-in-progress: true prevents duplicate builds\nFailure Handling: fail-fast: false allows other sites to build even if one fails\n3.5. Deployment Approach Method: GitHub App authentication (migrated from deploy keys)\nAuthentication Flow:\nGenerate App Token: Use actions/create-github-app-token@v1 Deploy with Token: Use peaceiris/actions-gh-pages@v4 Secrets Required (in my-documents):\nDOC_APP_ID - GitHub App ID DOC_APP_PRIVATE_KEY - GitHub App private key (PEM format) Deployment Step Example:\n- name: Generate GitHub App token id: app-token uses: actions/create-github-app-token@v1 with: app-id: ${{ secrets.DOC_APP_ID }} private-key: ${{ secrets.DOC_APP_PRIVATE_KEY }} owner: fchastanet repositories: bash-compiler - name: Deploy to GitHub Pages uses: peaceiris/actions-gh-pages@v4 with: github_token: ${{ steps.app-token.outputs.token }} external_repository: fchastanet/bash-compiler publish_dir: ./public publish_branch: gh-pages Result URLs:\nhttps://fchastanet.github.io/my-documents/ https://fchastanet.github.io/bash-compiler/ https://fchastanet.github.io/bash-tools/ https://fchastanet.github.io/bash-tools-framework/ https://fchastanet.github.io/bash-dev-env/ 3.6. Trigger Mechanism Dependent Repository Workflow Example: .github/workflows/trigger-docs.yml\nname: Trigger Documentation Rebuild on: push: branches: [master] paths: - 'content/**' - '.github/workflows/trigger-docs.yml' jobs: trigger: runs-on: ubuntu-latest steps: - name: Trigger my-documents build uses: peter-evans/repository-dispatch@v3 with: token: ${{ secrets.DOCS_TRIGGER_PAT }} repository: fchastanet/my-documents event-type: trigger-docs-rebuild client-payload: | { \"repository\": \"${{ github.repository }}\", \"ref\": \"${{ github.ref }}\", \"sha\": \"${{ github.sha }}\" } Required Secret: DOCS_TRIGGER_PAT - Personal Access Token with repo scope\n3.7. Theme Customization Shared Customizations: shared/\nContains:\nLayouts: Custom Hugo templates override Docsy defaults Assets: Custom SCSS variables, additional CSS/JS Archetypes: Content templates for new pages Per-Site Customization: Via configuration overrides in configs/{site}.yaml\nExamples:\nTheme colors: params.ui.navbar_bg_color: '#007bff' (blue for bash-compiler) Custom links in footer or navbar Site-specific SEO keywords and description Logo overrides Mount Strategy: Defined in configs/_base.yaml\nmodule: mounts: - {source: shared/layouts, target: layouts} - {source: shared/assets, target: assets} - {source: shared/archetypes, target: archetypes} - {source: content, target: content} - {source: static, target: static} Result: Shared resources available to all sites, with per-site override capability\n4. Lessons Learned \u0026 Future Considerations 4.1. GitHub App Migration from Deploy Keys Initial Approach: Deploy keys for each repository\nSetup: Generate SSH key pair per repository, store private key in my-documents secrets Secrets Required: DEPLOY_KEY_BASH_COMPILER, DEPLOY_KEY_BASH_TOOLS, etc. (4+ secrets) Management: Per-repository key addition in Settings → Deploy keys Problem: Scalability and management overhead\nMigration to GitHub Apps:\nAdvantages:\n✅ Fine-grained permissions: Only Contents and Pages write access (vs full repo access) ✅ Centralized management: One app for all repositories ✅ Better security: Automatic token expiration and rotation ✅ Audit trail: All actions logged under app identity ✅ No SSH management: HTTPS with tokens instead of SSH keys ✅ Easily revocable: Instant access revocation without key regeneration ✅ Scalable: Add/remove repositories without creating new keys ✅ Secrets reduction: 2 secrets (app ID + private key) vs 4+ deploy keys GitHub Official Recommendation:\n“We recommend using GitHub Apps with permissions scoped to specific repositories for enhanced security and more granular access control.”\nImplementation: See doc/ai/2026-02-18-github-app-migration.md for complete migration guide\nOutcome: Significantly improved security posture and simplified credential management\n4.2. Trade-offs Discovered 4.2.1. All-Site Rebuild Trade-off Trade-off: All sites rebuild together when any site content changes\nMitigation Strategies:\n✅ fail-fast: false in matrix strategy - One site failure doesn’t block others ✅ Parallel execution - All 5 sites build simultaneously (~60s total) ✅ Path-based triggers - Only rebuild when relevant files change ✅ Concurrency control - Cancel duplicate builds Acceptance Rationale:\nRelated documentation sites benefit from synchronized updates Total build time (60s) acceptable for documentation updates Ensures all sites stay consistent with latest shared resources Simpler mental model: one build updates everything 4.2.2. Authentication Complexity Trade-off: Initial setup requires GitHub App creation and secret configuration\nMitigation:\n✅ One-time setup effort well-documented ✅ Improved security worth the complexity ✅ Scales better than deploy keys (no per-repo setup needed for new sites) Outcome: Initial investment pays off with easier ongoing management\n4.2.3. Configuration Flexibility vs Consistency Trade-off: Centralized configuration limits per-site flexibility\nMitigation:\n✅ Site-specific override files in configs/{site}.yaml ✅ Shared base with override capability provides best of both worlds ✅ yq deep-merge preserves flexibility where needed Outcome: Achieved balance between consistency and customization\n4.3. Best Practices Identified 4.3.1. Configuration Management Use YAML deep-merge: yq eval-all properly merges nested structures Separate concerns: Base configuration vs site-specific overrides Version control everything: All configs in git Document override patterns: Clear examples in base config 4.3.2. Build Optimization Parallel matrix builds: Leverage GitHub Actions matrix for speed Minimal checkout: Only fetch what’s needed (depth, paths) Careful path triggers: Avoid unnecessary builds Cancel redundant builds: Use concurrency groups 4.3.3. Dependency Management Pin versions: Hugo 0.155.3, Go 1.24 (reproducible builds) Cache when possible: Hugo modules could be cached (future optimization) Minimal dependencies: yq only additional tool needed 4.3.4. Security GitHub Apps over deploy keys: Better security model Minimal permissions: Only what’s needed (Contents write, Pages write) Secret scoping: Secrets only in orchestrator repo Audit logging: GitHub App actions fully logged 4.4. Future Considerations 4.4.1. Potential Optimizations Hugo Module Caching:\nCurrent: Hugo modules downloaded fresh each build Future: Cache Go modules directory to speed up builds Benefit: Reduce build time by 5-10s per site Conditional Site Builds:\nCurrent: All sites build on any trigger Future: Parse repository_dispatch payload to build only affected site Benefit: Faster feedback for single-site changes Trade-off: More complex logic, potential consistency issues Build Artifact Reuse:\nCurrent: Each site built independently Future: Share Hugo module downloads across matrix jobs Benefit: Reduced redundant network calls 4.4.2. Scalability Considerations Adding New Documentation Sites:\nCreate new repository with content Add trigger workflow (2-minute setup) Add site config to my-documents/configs/{new-site}.yaml Add site to matrix in build-all-sites.yml Install GitHub App on new repository Done - automatic builds immediately available Estimated effort: 15-30 minutes per new site\n4.4.3. Alternative Approaches for Future Projects When Decentralized Makes Sense:\nTruly independent projects (not related documentation) Different teams with different update schedules Need for isolated failure handling Different Hugo/Docsy versions per project When to Reconsider:\nMore than 10 sites (build time may become issue) Sites diverge significantly in requirements Team structure changes (separate maintainers per site) Different deployment targets (not all GitHub Pages) 4.5. Success Metrics Achieved:\n✅ SEO Improvement: 2/10 (Docsify) → 9/10 (Hugo with Docsy) ✅ Build Time: ~60s for all 5 sites (parallel) ✅ Maintenance Reduction: One workflow update vs 5× separate updates ✅ Consistency: 100% - All sites use same base configuration ✅ Security: GitHub App authentication with fine-grained permissions ✅ Deployment: Automatic on content changes ✅ Developer Experience: Simplified per-repo structure (2 files vs 6) ✅ Independent URLs: All 5 repositories maintain separate GitHub Pages URLs ✅ Theme Sharing: Shared Docsy theme customizations across all sites Continuous Improvement:\nMonitor build times as content grows Gather feedback on developer experience Iterate on shared vs per-site customizations Evaluate caching opportunities Consider additional SEO optimization (structured data, etc.) 5. Conclusion The Hugo migration successfully addressed the SEO limitations of Docsify while establishing a scalable, maintainable multi-site documentation architecture. The centralized orchestrator approach provides the right balance of consistency and flexibility for related Bash tooling documentation projects.\nKey Success Factors:\nRight tool for the job: Hugo’s documentation focus and SEO capabilities Architectural alignment: Centralized approach matches project relationships Security improvement: GitHub App migration enhanced security posture Maintainability: Single-point updates reduce ongoing effort Flexibility preserved: Configuration overrides allow per-site customization Documentation maintained and current as of: 2026-02-18\nRelated Resources:\nMigration Guide GitHub App Migration Details Build Workflow Configuration Examples ","categories":["Brainstorming"],"description":"Comprehensive documentation of the Hugo migration for multi-site documentation","excerpt":"Comprehensive documentation of the Hugo migration for multi-site …","ref":"/my-documents/docs/my-documents/my-documents-static-site-generation-using-hugo/","tags":["hugo","docsy","multi-site","documentation","static-site-generator","github-actions","ai-generated"],"title":"My Documents - Static Site Generation Using Hugo"},{"body":"My Documents - Technical Architecture and Documentation Site Implementation My Documents - Technical Architecture and Documentation Site Implementation 1. Documentation Site Built with Hugo 2. Building Locally 2.1. Prerequisites 2.2. Quick Start 2.2.1. Install Hugo 2.2.2. Clone and Setup 2.2.3. Run Local Server 2.3. Building for Production 2.4. Checking Site Statistics 3. Multi-Site Orchestrator Architecture 3.1. Architecture Overview 3.2. Managed Documentation Sites 3.3. Shared vs Site-Specific Resources 3.4. How It Works 3.5. Key Benefits 3.6. Testing Multi-Site Locally 3.7. Makefile Commands 4. Creating a New Documentation Site 4.1. Prerequisites 4.2. Step-by-Step Guide 4.2.1. Step 1: Prepare the New Repository 4.2.2. Step 2: Add Trigger Workflow 4.2.3. Step 3: Create Site Configuration in my-documents 4.2.4. Step 4: Update Build Workflow Matrix 4.2.5. Step 5: Configure GitHub (See Section 6 for details) 4.2.6. Step 6: Test Locally 4.2.7. Step 7: Commit and Deploy 4.3. Post-Creation Checklist 5. GitHub Configuration 5.1. Architecture: GitHub App Authentication 5.2. Required Secrets 5.2.1. In my-documents Repository 5.2.2. In Each Dependent Repository 5.3. Creating the GitHub App 5.3.1. Step 1: Create GitHub App 5.3.2. Step 2: Note the App ID 5.3.3. Step 3: Generate Private Key 5.3.4. Step 4: Install App on Repositories 5.3.5. Step 5: Add Secrets to my-documents 5.4. Creating Personal Access Token (PAT) 5.4.1. Step 1: Create PAT 5.4.2. Step 2: Add to Each Dependent Repository 5.5. Repository Settings 5.5.1. GitHub Pages Configuration 5.5.2. Branch Protection (Optional) 6. Repository Integration 6.1. Files and Folder Structure 6.1.1. Required Structure in Dependent Repository 6.1.2. Required Structure in my-documents (Orchestrator) 6.2. Migration Guide 6.3. Content Organization Best Practices 6.4. Repository Configuration in GitHub 6.4.1. Step 1: Add Required Secrets 6.4.2. Step 2: Install GitHub App 6.4.3. Step 3: Configure GitHub Pages 6.4.4. Step 4: Verify Workflow Permissions 6.4.5. Step 5: Test the Integration 6.4.6. Common Integration Issues 7. Authentication Setup Details 7.1. GitHub App vs Deploy Keys Comparison 7.2. GitHub App Complete Setup 7.3. Personal Access Token (PAT) Setup 7.4. Token Lifecycle and Rotation 7.4.1. GitHub App Tokens 7.4.2. Personal Access Tokens 7.5. Troubleshooting Authentication 8. Troubleshooting Multi-Site Builds 8.1. Build Failures 8.1.1. Issue: Matrix Build Fails for One Site 8.1.2. Issue: All Sites Fail to Build 8.2. Configuration Merge Issues 8.2.1. Issue: Site-Specific Config Not Applied 8.2.2. Issue: Configuration Merge Produces Invalid YAML 8.3. Deployment Failures 8.3.1. Issue: GitHub App Authentication Fails 8.3.2. Issue: Deploy Succeeds but Site Not Updated 8.4. Trigger Issues 8.4.1. Issue: Repository Dispatch Not Triggering Build 8.4.2. Issue: Unwanted Builds (Too Many Triggers) 8.5. Content and Link Issues 8.5.1. Issue: Internal Links Return 404 8.5.2. Issue: Images Not Displaying 8.6. Performance Issues 8.6.1. Issue: Builds Taking Too Long 8.7. Debugging Checklist 9. Advanced Configuration Topics 9.1. Configuration Merging Strategy 9.1.1. How yq Deep-Merge Works 9.2. Per-Site Theme Customization 9.2.1. Theme Colors 9.2.2. Custom Logos 9.2.3. Custom CSS/SCSS 9.3. SEO and Metadata Customization 9.3.1. Per-Site SEO Keywords 9.3.2. Custom Metadata 9.3.3. Structured Data (JSON-LD) 9.4. Menu Customization 9.4.1. Custom Navigation Menu 9.4.2. Footer Links 9.5. Performance Optimization 9.5.1. Image Optimization 9.5.2. Minification 9.5.3. HTML Rendering Optimization 9.6. Multi-Language Support 9.7. Search Configuration 10. Contributing to the Orchestrator 10.1. Types of Contributions 10.2. Development Workflow 10.2.1. Step 1: Fork and Clone 10.2.2. Step 2: Make Changes 10.2.3. Step 3: Test Across All Sites 10.2.4. Step 4: Commit with Conventional Commits 10.3. Testing Changes Locally 10.3.1. Test Shared Layout Changes 10.3.2. Test Configuration Changes 10.3.3. Test Workflow Changes 10.4. Best Practices for Contributors 10.4.1. Shared Components 10.4.2. Configuration Changes 10.4.3. Workflow Improvements 10.5. Code Review Process 10.6. Adding New Shared Features 10.6.1. Example: Adding a New Partial 10.7. Release Process 10.7.1. Versioning 10.7.2. Deprecation Policy 10.8. Getting Help 11. Documentation Structure 11.1. Adding New Documentation 12. Content Guidelines 13. SEO Features 14. CI/CD Pipelines 14.1. Build All Sites (build-all-sites.yml) 14.2. Hugo Build \u0026 Deploy (hugo-build-deploy.yml) 14.3. Pre-commit Linting (lint.yml) 1. Documentation Site Built with Hugo This repository contains documentation built with Hugo static site generator and the Docsy theme. All content is in Markdown format and automatically published to GitHub Pages.\n2. Building Locally 2.1. Prerequisites Hugo Extended version 0.110+ Go version 1.18+ 2.2. Quick Start 2.2.1. Install Hugo Linux:\nCGO_ENABLED=1 go install -tags extended github.com/gohugoio/hugo@latest Or download from Hugo “extended” releases\n2.2.2. Clone and Setup git clone https://github.com/fchastanet/my-documents.git cd my-documents # Download Hugo theme and dependencies hugo mod get -u 2.2.3. Run Local Server hugo server -D The site will be available at http://localhost:1313/my-documents/\n-D flag includes draft pages Site auto-reloads on file changes Press Ctrl+C to stop the server 2.3. Building for Production hugo --minify Output is generated in the public/ directory.\n2.4. Checking Site Statistics hugo --printI18nWarnings --printPathWarnings --printUnusedTemplates 3. Multi-Site Orchestrator Architecture This repository serves as a centralized orchestrator that builds and deploys multiple documentation sites in a coordinated fashion.\n3.1. Architecture Overview ┌─────────────────────────────────────────────────────────────────┐ │ my-documents (Orchestrator) │ ├─────────────────────────────────────────────────────────────────┤ │ • Shared Hugo theme and layouts (shared/) │ │ • Base configuration (configs/_base.yaml) │ │ • Site-specific configs (configs/*.yaml) │ │ • Build workflow (.github/workflows/build-all-sites.yml) │ │ • Deployment orchestration (GitHub App authentication) │ └────┬────────────┬────────────┬────────────┬─────────────────────┘ │ │ │ │ ▼ ▼ ▼ ▼ ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ bash- │ │ bash- │ │ bash- │ │ bash- │ │compiler │ │ tools │ │ tools- │ │ dev- │ │ │ │ │ │framework│ │ env │ └─────────┘ └─────────┘ └─────────┘ └─────────┘ │ │ │ │ └────┬───────┴────────┬───┴────────┬───┘ │ │ │ ▼ ▼ ▼ Push to master → Trigger orchestrator → Build all sites in parallel │ │ └────────────────────────────────────────┘ │ ▼ Deploy to GitHub Pages (fchastanet.github.io/bash-compiler/, etc.) 3.2. Managed Documentation Sites Centralized orchestrator and documentation site My Documents deployment My Documents GitHub Repository Documentation for the Bash Compiler project Bash Compiler deployment Bash Compiler GitHub Repository Documentation for the Bash Tools project Bash Tools deployment Bash Tools GitHub Repository Documentation for the Bash Tools Framework project Bash Tools Framework deployment Bash Tools Framework GitHub Repository Documentation for the Bash Dev Env project Bash Dev Env deployment Bash Dev Env GitHub Repository 3.3. Shared vs Site-Specific Resources Shared Resources (All Sites):\nHugo theme (Docsy) and version Common layouts and partials (shared/layouts/) Base SCSS variables and assets (shared/assets/) Content archetypes (shared/archetypes/) SEO structured data and meta tags Base configuration (configs/_base.yaml) Site-Specific Resources:\nDocumentation content (content/) Site configuration overrides (configs/[site].yaml) Theme colors and branding Navigation menu items Repository links and metadata Static assets specific to the site 3.4. How It Works Content Change: Developer pushes changes to any managed repository (e.g., bash-compiler) Trigger: Repository trigger workflow sends repository_dispatch event to my-documents orchestrator Checkout: Orchestrator checks out all repositories Parallel Build: GitHub Actions matrix builds all 5 sites simultaneously (~60s total) Config Merge: Each site gets base config + site-specific overrides merged with yq Deploy: Sites deployed to respective GitHub Pages using GitHub App authentication Result: All documentation sites are updated and consistent 3.5. Key Benefits ✅ Guaranteed Consistency: All sites use the same Hugo theme version and shared components ✅ Single-Point Updates: Fix a bug once, deploy to all sites automatically ✅ Simplified Per-Repo Setup: Dependent repos need only 2 files (trigger workflow + content) ✅ Fast Parallel Builds: All 5 sites build simultaneously in ~60 seconds ✅ Centralized Authentication: One GitHub App manages deployments to all repositories ✅ Configuration Flexibility: Per-site customization while maintaining consistency 3.6. Testing Multi-Site Locally To test multiple sites locally using symlinks:\nStep 1: Clone related repositories\ncd /path/to/your/workspace git clone https://github.com/fchastanet/my-documents.git git clone https://github.com/fchastanet/bash-compiler.git git clone https://github.com/fchastanet/bash-tools.git git clone https://github.com/fchastanet/bash-tools-framework.git git clone https://github.com/fchastanet/bash-dev-env.git Step 2: Install dependencies\ncd my-documents make install # Installs Hugo, yq, npm packages, Go modules Step 3: Link repositories\nmake link-repos # Creates symlinks in sites/ directory This creates:\nmy-documents/sites/ ├── bash-compiler -\u003e ../../bash-compiler ├── bash-tools -\u003e ../../bash-tools ├── bash-tools-framework -\u003e ../../bash-tools-framework └── bash-dev-env -\u003e ../../bash-dev-env Step 4: Build and test\n# Build all sites make build-all # Test all sites with curl make test-all # Build a specific site make build-site SITE=bash-compiler Step 5: Clean up\n# Remove symlinks make unlink-repos # Clean build artifacts make clean 3.7. Makefile Commands make help # Show all available commands make install # Install all dependencies make link-repos # Create symlinks to other repos make unlink-repos # Remove symlinks make build-all # Build all sites locally make build-site # Build specific site (SITE=name) make test-all # Build and test all sites with curl make start # Start Hugo dev server (my-documents) make build # Build my-documents only make clean # Remove build artifacts 4. Creating a New Documentation Site This section guides you through adding a new documentation site to the orchestrator.\n4.1. Prerequisites Before adding a new site, ensure:\nRepository exists and contains documentation to migrate You have admin access to both my-documents and the new repository Hugo Extended and Go are installed locally for testing You understand the orchestrator architecture (see Section 4) 4.2. Step-by-Step Guide 4.2.1. Step 1: Prepare the New Repository Clone the repository:\ngit clone https://github.com/fchastanet/your-new-repo.git cd your-new-repo Create content structure:\nmkdir -p content/en/docs Create homepage (content/en/_index.md):\n--- title: Your Project Name description: Brief project description --- Welcome to the documentation! ## Features - Feature 1 - Feature 2 Create docs landing page (content/en/docs/_index.md):\n--- title: Documentation description: Complete documentation for Your Project weight: 1 --- Migrate existing content (see Section 7.2 for detailed migration guide)\n4.2.2. Step 2: Add Trigger Workflow Create .github/workflows/trigger-docs.yml:\n--- name: Trigger Documentation Rebuild on: push: branches: [master] paths: - 'content/**' - 'static/**' - '.github/workflows/trigger-docs.yml' workflow_dispatch: jobs: trigger: runs-on: ubuntu-latest steps: - name: Trigger my-documents orchestrator uses: peter-evans/repository-dispatch@v3 with: token: ${{ secrets.DOCS_BUILD_TOKEN }} repository: fchastanet/my-documents event-type: trigger-docs-rebuild client-payload: | { \"repository\": \"${{ github.repository }}\", \"ref\": \"${{ github.ref }}\", \"sha\": \"${{ github.sha }}\", \"triggered_by\": \"${{ github.actor }}\" } - name: Build triggered run: | echo \"✅ Documentation build triggered in fchastanet/my-documents\" echo \"🔗 Check status: https://github.com/fchastanet/my-documents/actions\" 4.2.3. Step 3: Create Site Configuration in my-documents Clone my-documents:\ncd /path/to/workspace git clone https://github.com/fchastanet/my-documents.git cd my-documents Create site config (configs/your-new-repo.yaml):\nbaseURL: https://fchastanet.github.io/your-new-repo title: Your Project Name description: Brief description for SEO and meta tags params: description: Brief description for SEO and meta tags keywords: - keyword1 - keyword2 - keyword3 # Theme color (hex color code) ui: navbar_bg_color: '#ff6600' # Choose a unique color # Repository links github_repo: https://github.com/fchastanet/your-new-repo github_project_repo: https://github.com/fchastanet/your-new-repo # Footer links links: user: - name: GitHub Repository url: https://github.com/fchastanet/your-new-repo icon: fab fa-github 4.2.4. Step 4: Update Build Workflow Matrix Edit .github/workflows/build-all-sites.yml and add your site to the matrix:\nmatrix: site: # ... existing sites ... - name: your-new-repo repo: fchastanet/your-new-repo baseURL: 'https://fchastanet.github.io/your-new-repo' self: false 4.2.5. Step 5: Configure GitHub (See Section 6 for details) Add Personal Access Token to new repository:\nGo to new repository → Settings → Secrets → Actions Add secret: DOCS_BUILD_TOKEN (see Section 6.2) Install GitHub App on new repository (see Section 6.3)\n4.2.6. Step 6: Test Locally cd my-documents # Link your new repository ln -s ../../your-new-repo sites/your-new-repo # Build the site make build-site SITE=your-new-repo # Test the output cd build/your-new-repo hugo server -D --port 1314 # Visit http://localhost:1314/your-new-repo/ 4.2.7. Step 7: Commit and Deploy Commit changes to my-documents:\ncd my-documents git add configs/your-new-repo.yaml .github/workflows/build-all-sites.yml git commit -m \"feat: add your-new-repo to orchestrator - Add site configuration with unique theme color - Update build matrix to include your-new-repo - Enable automated documentation deployment \" git push origin master Commit trigger workflow to new repository:\ncd your-new-repo git add .github/workflows/trigger-docs.yml content/ git commit -m \"feat: add Hugo content structure and orchestrator trigger - Migrate content to Hugo format - Add trigger workflow for my-documents orchestrator - Configure content structure for Docsy theme \" git push origin master Verify deployment:\nCheck my-documents Actions: https://github.com/fchastanet/my-documents/actions Wait for build to complete (~60s) Visit your site: https://fchastanet.github.io/your-new-repo/ 4.3. Post-Creation Checklist Site builds successfully in ci/CD GitHub Pages deployment completes without errors Live site is accessible at expected URL Navigation works correctly Internal links resolve properly Images and static assets display correctly SEO meta tags are present (view source) Sitemap is generated: https://fchastanet.github.io/your-new-repo/sitemap.xml Mobile responsive layout works Search functionality works (Docsy local search) 5. GitHub Configuration This section covers authentication, secrets, and repository settings required for the orchestrator.\n5.1. Architecture: GitHub App Authentication The orchestrator uses a GitHub App for secure, fine-grained authentication when deploying to dependent repositories.\nWhy GitHub App (vs Deploy Keys)?\n✅ Fine-grained permissions: Only Contents and Pages write access ✅ Centralized management: One app deploys to all repositories ✅ Better security: Automatic token expiration and rotation ✅ Audit trail: All actions logged under app identity ✅ Easily scalable: Add/remove repositories without generating new keys ✅ Revocable: Instantly revoke access from app settings 5.2. Required Secrets 5.2.1. In my-documents Repository Secret Name Purpose How to Obtain DOC_APP_ID GitHub App ID for deployments Step 3 of GitHub App creation (Section 6.3) DOC_APP_PRIVATE_KEY GitHub App private key (PEM format) Step 4 of GitHub App creation (Section 6.3) 5.2.2. In Each Dependent Repository Secret Name Purpose How to Obtain DOCS_BUILD_TOKEN Personal Access Token to trigger builds Create PAT with repo scope (Section 6.4) 5.3. Creating the GitHub App If the GitHub App doesn’t exist yet, follow these steps:\n5.3.1. Step 1: Create GitHub App Navigate to https://github.com/settings/apps/new (or Organization → Settings → GitHub Apps → New)\nFill in app details:\nName: My Documents Site Deployer Description: Deploys documentation sites to GitHub Pages for fchastanet projects Homepage URL: https://github.com/fchastanet/my-documents Callback URL: (leave blank) Webhook: ✗ Uncheck \"Active\" Set repository permissions:\nRepository permissions: - Contents: Read and write ← Deploy to gh-pages branch - Pages: Read and write ← Trigger GitHub Pages build - Metadata: Read-only ← (automatic, required) Where can this app be installed?\n○ Only on this account Click “Create GitHub App”\n5.3.2. Step 2: Note the App ID App ID is displayed at the top of the app settings page Example: App ID: 123456 You’ll need this for DOC_APP_ID secret 5.3.3. Step 3: Generate Private Key Scroll to “Private keys” section Click “Generate a private key” Download the .pem file (e.g., my-documents-site-deployer.2026-02-19.private-key.pem) Store securely (password manager or encrypted storage) ⚠️ Security Warning: Private key provides write access to repositories. Never commit it to git.\n5.3.4. Step 4: Install App on Repositories In app settings → “Install App” (left sidebar) Click “Install” next to your account (fchastanet) Select “Only select repositories” Choose repositories: bash-compiler bash-tools bash-tools-framework bash-dev-env (Add your new repository here) Click “Install” 5.3.5. Step 5: Add Secrets to my-documents Go to https://github.com/fchastanet/my-documents/settings/secrets/actions Click “New repository secret” Secret 1: DOC_APP_ID\nName: DOC_APP_ID Value: 123456 # Your App ID from Step 2 Secret 2: DOC_APP_PRIVATE_KEY\nName: DOC_APP_PRIVATE_KEY Value: # Paste ENTIRE content of .pem file, including BEGIN/END lines 5.4. Creating Personal Access Token (PAT) Each dependent repository needs a Personal Access Token to trigger builds in my-documents.\n5.4.1. Step 1: Create PAT Go to https://github.com/settings/tokens\nClick “Generate new token” → “Generate new token (classic)”\nFill in details:\nNote: Documentation Build Trigger Expiration: No expiration (or 1 year) Scopes: ✅ repo (Full control of private repositories) ✅ repo:status ✅ repo_deployment ✅ public_repo Click “Generate token”\nCopy the token immediately (you won’t see it again)\n5.4.2. Step 2: Add to Each Dependent Repository For each repository (bash-compiler, bash-tools, etc.):\nGo to repository → Settings → Secrets and variables → Actions\nClick “New repository secret”\nAdd:\nName: DOCS_BUILD_TOKEN Value: ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx # Your PAT 5.5. Repository Settings 5.5.1. GitHub Pages Configuration For each dependent repository:\nGo to repository → Settings → Pages Source: Deploy from a branch Branch: gh-pages / (root) Custom domain: (leave blank unless you have one) Enforce HTTPS: ✅ Checked The orchestrator will create and manage the gh-pages branch automatically.\n5.5.2. Branch Protection (Optional) For master branch in dependent repositories:\nGo to repository → Settings → Branches Add rule for master: ✅ Require a pull request before merging ✅ Require status checks to pass (Documentation build) ✅ Require branches to be up to date 6. Repository Integration This section covers integrating an existing or new repository into the orchestrator.\n6.1. Files and Folder Structure 6.1.1. Required Structure in Dependent Repository your-new-repo/ ├── .github/ │ └── workflows/ │ └── trigger-docs.yml ← Triggers orchestrator (required) ├── content/ │ └── en/ │ ├── _index.md ← Homepage (required) │ └── docs/ │ ├── _index.md ← Docs landing page (required) │ └── *.md ← Documentation pages ├── static/ ← Static assets (optional) │ ├── images/ │ └── downloads/ ├── README.md ← Repository README (recommended) └── LICENSE ← License file (recommended) Not Required in Dependent Repository:\n❌ hugo.yaml (generated by orchestrator from configs/) ❌ go.mod / go.sum (managed by orchestrator) ❌ Hugo build workflows (orchestrator handles this) ❌ Theme files (shared from my-documents) 6.1.2. Required Structure in my-documents (Orchestrator) my-documents/ ├── configs/ │ ├── _base.yaml ← Shared base config │ └── your-new-repo.yaml ← Site-specific overrides ├── shared/ │ ├── layouts/ ← Shared Hugo templates │ ├── assets/ ← Shared SCSS, JS │ └── archetypes/ ← Content templates └── .github/workflows/ └── build-all-sites.yml ← Orchestrator workflow (update matrix) 6.2. Migration Guide For detailed migration from Docsify or other static site generators to Hugo, see:\n📄 Complete Migration Guide\nThe migration guide covers:\nAnalyzing current documentation structure Converting Docsify/other formats to Hugo content structure Content migration strategies (markdown conversion) Adding proper frontmatter to all pages Updating internal links and image references Testing migration locally Validation checklist Quick Migration Overview:\nAnalyze current structure - Identify all documentation files and organization Create Hugo content structure - content/en/docs/ hierarchy Add frontmatter - Title, description, weight to all pages Convert content - Update syntax, links, shortcodes Migrate assets - Move to static/ directory Test locally - Build with orchestrator, verify output Deploy - Commit and push to trigger automated build 6.3. Content Organization Best Practices Directory Structure Example:\ncontent/en/docs/ ├── _index.md # Docs landing page (weight: 1) ├── getting-started/ │ ├── _index.md # Section landing (weight: 10) │ ├── installation.md # (weight: 10) │ ├── quickstart.md # (weight: 20) │ └── configuration.md # (weight: 30) ├── guides/ │ ├── _index.md # (weight: 20) │ ├── basic-usage.md │ └── advanced-topics.md └── reference/ ├── _index.md # (weight: 30) ├── api.md └── cli.md Frontmatter Template:\n--- title: Page Title description: Brief description for SEO (100-160 characters recommended) weight: 10 # Lower numbers appear first in navigation categories: [documentation] # Optional categories tags: [bash, linux, tutorial] # Optional tags --- Navigation Ordering:\nHugo uses weight to order pages (ascending order) Lower weight = higher in menu (10 before 20) Same weight = alphabetical order by title Section _index.md establishes section weight 6.4. Repository Configuration in GitHub After creating content and workflows, configure the repository:\n6.4.1. Step 1: Add Required Secrets See Section 6.2 and 6.4 for details.\nAdd DOCS_BUILD_TOKEN secret to dependent repository Ensure my-documents has DOC_APP_ID and DOC_APP_PRIVATE_KEY 6.4.2. Step 2: Install GitHub App See Section 6.3, Step 4 for details.\nGo to https://github.com/settings/installations Click “Configure” on “My Documents Site Deployer” Add your repository to “Repository access” Click “Save” 6.4.3. Step 3: Configure GitHub Pages Go to repository → Settings → Pages Source: Deploy from a branch Branch: gh-pages / (root) Click “Save” ⚠️ Note: The gh-pages branch will be created automatically on first deployment. Don’t create it manually.\n6.4.4. Step 4: Verify Workflow Permissions Go to repository → Settings → Actions → General Workflow permissions: ✅ Read and write permissions (for creating gh-pages branch) ✅ Allow GitHub Actions to create and approve pull requests (optional) 6.4.5. Step 5: Test the Integration Make a test commit to content:\ncd your-new-repo echo \"Test update\" \u003e\u003e content/en/docs/_index.md git add content/en/docs/_index.md git commit -m \"test: trigger documentation build\" git push origin master Verify trigger workflow runs in your repository:\nGo to https://github.com/fchastanet/your-new-repo/actions Check “Trigger Documentation Rebuild” workflow succeeded Verify orchestrator build in my-documents:\nGo to https://github.com/fchastanet/my-documents/actions Check “Build All Documentation Sites” workflow running Look for your site in the matrix build jobs Verify deployment:\nWait for build to complete (~60s) Visit https://fchastanet.github.io/your-new-repo/ Check that test update is visible 6.4.6. Common Integration Issues Issue Solution Trigger workflow fails Check DOCS_BUILD_TOKEN secret exists and is valid Build workflow fails Check GitHub App installed on repository Deployment fails Check gh-pages branch permissions and Pages settings Site not accessible Wait 2-3 minutes for GitHub Pages propagation 404 on site Check baseURL in configs/your-new-repo.yaml 7. Authentication Setup Details This section provides comprehensive details on authentication mechanisms.\n7.1. GitHub App vs Deploy Keys Comparison Feature GitHub App (Recommended) Deploy Keys Secrets Required 2 (App ID + Private Key) N (one per repository) Permissions Fine-grained (Contents, Pages) Full repository access Token Expiration Automatic (1 hour) Never (manual rotation needed) Scalability Easy (add repos to app) Creates new key pair per repo Audit Trail Full (logged as app) Limited (logged as deploy key) Revocation Instant (uninstall app) Per-key (delete each key) Setup Complexity Medium (one-time) Low (per repo) GitHub Recommendation ✅ Yes ❌ Deprecated for this use case Conclusion: GitHub App is the better choice for the orchestrator architecture.\n7.2. GitHub App Complete Setup See Section 6.3 for step-by-step instructions.\nQuick Reference:\nCreate app at https://github.com/settings/apps/new Set permissions: Contents (write), Pages (write) Generate and download private key (.pem file) Note the App ID Install app on target repositories Add DOC_APP_ID and DOC_APP_PRIVATE_KEY to my-documents secrets 7.3. Personal Access Token (PAT) Setup See Section 6.4 for step-by-step instructions.\nQuick Reference:\nCreate PAT at https://github.com/settings/tokens Scopes: repo (full control) Expiration: No expiration or 1 year Add DOCS_BUILD_TOKEN to each dependent repository Security Note: PAT has full repo access. It’s used only to trigger repository_dispatch events, which is a safe operation. Still, protect it like a password.\n7.4. Token Lifecycle and Rotation 7.4.1. GitHub App Tokens Lifetime: 1 hour (automatic) Rotation: Automatic (new token generated each build) Manual Rotation: Regenerate private key in app settings if compromised 7.4.2. Personal Access Tokens Lifetime: As configured (no expiration or 1 year) Rotation: Manual Best Practice: Rotate annually or when team members leave To Rotate PAT:\nCreate new PAT (Section 6.4) Update DOCS_BUILD_TOKEN in all dependent repositories Delete old PAT from https://github.com/settings/tokens 7.5. Troubleshooting Authentication Issue Diagnosis Solution “Bad credentials” error GitHub App token expired/invalid Check DOC_APP_ID and DOC_APP_PRIVATE_KEY “Resource not accessible” error GitHub App not installed on repo Install app on repository (Section 6.3.4) Trigger workflow unauthorized PAT invalid or insufficient scope Regenerate PAT with repo scope (Section 6.4) Deploy fails with 403 GitHub App lacks Pages permission Check app permissions (Section 6.3.1) Token generation fails Private key malformed Re-download .pem file, ensure complete copy 8. Troubleshooting Multi-Site Builds This section covers common issues and solutions specific to the orchestrator.\n8.1. Build Failures 8.1.1. Issue: Matrix Build Fails for One Site Symptoms:\nOne site fails while others succeed Error message specific to site content or configuration Diagnosis:\nCheck GitHub Actions logs for the failing site Look for errors in the matrix job output Identify if it’s a content issue or configuration issue Solutions:\n# Test site locally to reproduce error cd my-documents make build-site SITE=failing-site-name # Check configuration merge yq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' \\ configs/_base.yaml \\ configs/failing-site.yaml # Validate content frontmatter grep -r \"^---$\" sites/failing-site/content/ 8.1.2. Issue: All Sites Fail to Build Symptoms:\nEntire matrix fails Error occurs before individual site builds Common Causes:\nHugo version incompatibility yq installation failure Go module resolution issues Base configuration syntax error Solutions:\nCheck Hugo version in workflow matches local: 0.155.3\nVerify yq installation step succeeded\nCheck configs/_base.yaml for YAML syntax errors:\nyq eval configs/_base.yaml Review workflow logs for specific error messages\n8.2. Configuration Merge Issues 8.2.1. Issue: Site-Specific Config Not Applied Symptoms:\nSite uses base configuration instead of overrides Theme color or title not correct Diagnosis:\n# Test configuration merge locally yq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' \\ configs/_base.yaml \\ configs/your-site.yaml # Check if keys are being merged (should be combined, not duplicated) Solutions:\nEnsure site config uses correct YAML syntax (2-space indentation) Verify override keys match base config structure Use yq deep-merge syntax (not simple concatenation) 8.2.2. Issue: Configuration Merge Produces Invalid YAML Symptoms:\nyq merge succeeds but Hugo fails to parse hugo.yaml YAML syntax errors in workflow Solutions:\n# Validate both configs individually yq eval configs/_base.yaml yq eval configs/your-site.yaml # Test merge and validate result yq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' \\ configs/_base.yaml \\ configs/your-site.yaml | yq eval - # Common issues: # - Missing quotes around URLs # - Incorrect indentation (use 2 spaces, not tabs) # - Invalid YAML anchors or aliases 8.3. Deployment Failures 8.3.1. Issue: GitHub App Authentication Fails Symptoms:\nError: “Could not authenticate with GitHub App” Deploy step fails with 401 or 403 Solutions:\nVerify secrets exist in my-documents:\n# Check in GitHub UI # https://github.com/fchastanet/my-documents/settings/secrets/actions # Ensure DOC_APP_ID and DOC_APP_PRIVATE_KEY are set Verify GitHub App is installed on target repository:\n# Check installations # https://github.com/settings/installations # Click \"Configure\" → Verify repository is in access list Verify app permissions (Contents: write, Pages: write)\n8.3.2. Issue: Deploy Succeeds but Site Not Updated Symptoms:\nDeploy step succeeds in workflow GitHub Pages still shows old content Solutions:\nCheck gh-pages branch:\n# Clone repository git clone https://github.com/fchastanet/your-site.git cd your-site git checkout gh-pages git log -1 # Check latest commit timestamp Clear GitHub Actions cache:\nGo to repository → Actions → Caches Delete all caches Re-run workflow Wait for propagation:\nGitHub Pages can take 2-3 minutes to update Check deployment status in repository → Settings → Pages Verify Pages settings:\nSource: Deploy from a branch Branch: gh-pages / (root) 8.4. Trigger Issues 8.4.1. Issue: Repository Dispatch Not Triggering Build Symptoms:\nCommit to dependent repository doesn’t trigger my-documents build Trigger workflow succeeds but orchestrator doesn’t run Solutions:\nVerify PAT has correct scope:\n# PAT must have 'repo' scope # Check: https://github.com/settings/tokens # Regenerate if necessary (Section 6.4) Check event type matches:\n# In trigger workflow (dependent repo) event-type: trigger-docs-rebuild # Must match orchestrator workflow on: repository_dispatch: types: [trigger-docs-rebuild] # Must match exactly Verify secret name:\n# In trigger workflow token: ${{ secrets.DOCS_BUILD_TOKEN }} # Must be this exact name Check workflow logs in dependent repository for API errors\n8.4.2. Issue: Unwanted Builds (Too Many Triggers) Symptoms:\nBuilds trigger on every commit Builds trigger even when only README changes Solutions:\nUpdate paths filter in trigger workflow:\non: push: branches: [master] paths: - 'content/**' # Only content changes - 'static/**' # Only static asset changes # Remove other paths if too many triggers 8.5. Content and Link Issues 8.5.1. Issue: Internal Links Return 404 Symptoms:\nNavigation works but internal links broken Links work locally but not on deployed site Solutions:\nUse Hugo shortcodes for internal links: escaped here to prevent rendering by hugo\n\\{\\{\u003c ref \"/docs/section/page\" \u003e\\}\\} Use relative paths without .md extension:\n[Link text](/docs/section/page/) Avoid absolute URLs for internal links:\n❌ [Bad](https://fchastanet.github.io/site/docs/page/) ✅ [Good](/docs/page/) 8.5.2. Issue: Images Not Displaying Symptoms:\nImages work locally but 404 on deployed site Solutions:\nUse correct path from static directory:\n# File: static/images/diagram.png ![Diagram](/images/diagram.png) ✅ ![Diagram](images/diagram.png) ❌ Verify image exists in static directory:\nls -la sites/your-site/static/images/ Check build output includes images:\nls -la build/your-site/public/images/ 8.6. Performance Issues 8.6.1. Issue: Builds Taking Too Long Symptoms:\nBuild time exceeds 5 minutes Workflow times out Solutions:\nEnable matrix parallelization (already enabled):\nstrategy: fail-fast: false # Ensures parallel execution Reduce content size:\nOptimize images (compress, resize) Remove unused static assets Archive old documentation versions Cache Hugo modules (future optimization):\n# Add caching step - name: Cache Hugo modules uses: actions/cache@v4 with: path: | ~/.cache/hugo_cache /tmp/hugo_cache key: hugo-modules-${{ hashFiles('**/go.sum') }} 8.7. Debugging Checklist When troubleshooting, follow this systematic approach:\nIdentify the failure point:\nTrigger workflow (dependent repo) Orchestrator workflow started Specific site build failed Deployment failed Site not accessible Gather information:\nCheck GitHub Actions logs Review error messages Check workflow run details Visit repository deployment status Reproduce locally:\nClone repository Run make build-site SITE=failing-site Check local build output Verify configuration:\nYAML syntax valid Secrets exist and are correct GitHub App installed Permissions configured Test incrementally:\nTest config merge Test Hugo build Test with minimal content Add content gradually 9. Advanced Configuration Topics This section covers advanced customization and optimization techniques.\n9.1. Configuration Merging Strategy 9.1.1. How yq Deep-Merge Works The orchestrator uses yq for proper YAML deep-merging (not simple concatenation):\nyq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' \\ configs/_base.yaml \\ configs/site-specific.yaml \u003e hugo.yaml Behavior:\nScalar values: Site-specific overrides base (complete replacement) Objects: Deep merge (combine keys from both) Arrays: Site-specific replaces base entirely (not appended) Example:\n# configs/_base.yaml params: ui: navbar_bg_color: '#333333' sidebar_width: 250px keywords: - documentation - hugo # configs/bash-compiler.yaml params: ui: navbar_bg_color: '#007bff' keywords: - bash - compiler # Merged result params: ui: navbar_bg_color: '#007bff' # Overridden sidebar_width: 250px # Inherited from base keywords: # Array replaced entirely - bash - compiler 9.2. Per-Site Theme Customization 9.2.1. Theme Colors Each site can have unique branding:\n# configs/your-site.yaml params: ui: navbar_bg_color: '#ff6600' # Navbar background navbar_text_color: '#ffffff' # Navbar text sidebar_bg_color: '#f8f9fa' # Sidebar background Color Palette Recommendations:\nmy-documents: #663399 (Deep Purple) bash-compiler: #007bff (Blue) bash-tools: #28a745 (Green) bash-tools-framework: #dc3545 (Red) bash-dev-env: #fd7e14 (Orange) 9.2.2. Custom Logos Override logo per site:\n# configs/your-site.yaml params: ui: navbar_logo: /images/logo.svg sidebar_logo: /images/logo-sidebar.svg Place logos in dependent repository:\n# your-site/static/images/logo.svg # your-site/static/images/logo-sidebar.svg 9.2.3. Custom CSS/SCSS Option 1: Site-specific SCSS in shared/\n// shared/assets/scss/_variables_project.scss $custom-your-site-color: #ff6600; Option 2: Site-specific CSS in dependent repo\n/* your-site/static/css/custom.css */ .navbar { background-color: #ff6600 !important; } Reference in config:\n# configs/your-site.yaml params: custom_css: - /css/custom.css 9.3. SEO and Metadata Customization 9.3.1. Per-Site SEO Keywords # configs/your-site.yaml params: description: Comprehensive documentation for Your Project Name keywords: - your - specific - keywords 9.3.2. Custom Metadata Add custom meta tags via site config:\nparams: meta_tags: - name: \"author\" content: \"Your Name\" - name: \"og:type\" content: \"website\" 9.3.3. Structured Data (JSON-LD) Shared structured data is in shared/layouts/partials/hooks/head-end.html. Customize per site:\n\u003c!-- Create: sites/your-site/layouts/partials/hooks/head-end.html --\u003e \u003cscript type=\"application/ld+json\"\u003e { \"@context\": \"https://schema.org\", \"@type\": \"SoftwareApplication\", \"name\": \"Your Project\", \"description\": \"Project description\", \"url\": \"{{ .Site.BaseURL }}\" } \u003c/script\u003e 9.4. Menu Customization 9.4.1. Custom Navigation Menu Override default menu per site:\n# configs/your-site.yaml menu: main: - name: Documentation url: /docs/ weight: 10 - name: Guides url: /docs/guides/ weight: 20 - name: API url: /docs/api/ weight: 30 - name: GitHub url: https://github.com/fchastanet/your-site weight: 40 9.4.2. Footer Links params: links: user: - name: GitHub url: https://github.com/fchastanet/your-site icon: fab fa-github - name: Issues url: https://github.com/fchastanet/your-site/issues icon: fas fa-bug developer: - name: Contribute url: https://github.com/fchastanet/your-site/blob/master/CONTRIBUTING.md icon: fas fa-code 9.5. Performance Optimization 9.5.1. Image Optimization Hugo can process and optimize images:\n# configs/_base.yaml (already configured) imaging: resampleFilter: CatmullRom quality: 75 anchor: smart Use responsive images in content:\n9.5.2. Minification Already enabled in build workflow:\nhugo --minify This minifies:\nHTML CSS JavaScript JSON XML 9.5.3. HTML Rendering Optimization # configs/_base.yaml markup: goldmark: renderer: unsafe: false # Security: disable raw HTML highlight: anchorLineNos: false # Faster: no line number anchors lineNos: false # Faster: no line numbers lineNumbersInTable: false 9.6. Multi-Language Support Hugo/Docsy supports multiple languages. To add:\n# configs/your-site.yaml languages: en: title: Your Project description: English description languageName: English weight: 1 fr: title: Votre Project description: Description française languageName: Français weight: 2 Create content:\ncontent/en/docs/guide.md content/fr/docs/guide.md 9.7. Search Configuration Docsy includes local search (already configured). To customize:\n# configs/your-site.yaml params: search: algolia: enabled: false # Use local search (default) offlineSearch: true # Enable offline search index offlineSearchMaxResults: 20 offlineSearchSummaryLength: 200 10. Contributing to the Orchestrator This section guides contributors who want to improve the orchestrator itself.\n10.1. Types of Contributions Shared Components:\nLayouts and partials (shared/layouts/) SCSS variables and styles (shared/assets/) Content archetypes (shared/archetypes/) SEO enhancements (shared/layouts/partials/hooks/) Configuration:\nBase configuration improvements (configs/_base.yaml) New site additions (site-specific configs) Workflows:\nBuild optimizations (.github/workflows/build-all-sites.yml) New optional workflows (testing, validation) Documentation:\nREADME improvements Troubleshooting guide additions Migration guide updates 10.2. Development Workflow 10.2.1. Step 1: Fork and Clone git clone https://github.com/fchastanet/my-documents.git cd my-documents git checkout -b feature/your-improvement 10.2.2. Step 2: Make Changes Edit shared components in shared/ Test changes locally (see Section 11.3) Verify all sites still build correctly 10.2.3. Step 3: Test Across All Sites # Link all repositories make link-repos # Build all sites make build-all # Test all sites make test-all 10.2.4. Step 4: Commit with Conventional Commits git add shared/layouts/partials/your-change.html git commit -m \"feat: add new partial for feature X - Add partial to enhance functionality - Update documentation - Test across all sites \" 10.3. Testing Changes Locally 10.3.1. Test Shared Layout Changes # Build specific site to test layout make build-site SITE=bash-compiler # Start local server cd build/bash-compiler hugo server -D --port 1314 # Visit http://localhost:1314/bash-compiler/ 10.3.2. Test Configuration Changes # Test config merge yq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' \\ configs/_base.yaml \\ configs/bash-compiler.yaml | tee /tmp/test-config.yaml # Build with test config hugo --config /tmp/test-config.yaml --minify 10.3.3. Test Workflow Changes Push to your fork:\ngit push origin feature/your-improvement Create PR to fchastanet/my-documents\nWorkflow will run on PR (test build before merge)\n10.4. Best Practices for Contributors 10.4.1. Shared Components Test across all sites before committing Keep changes backward compatible (don’t break existing sites) Document new features in comments and README Use descriptive variable names in layouts/partials 10.4.2. Configuration Changes Preserve base config integrity (don’t remove existing keys) Document new parameters in comments Test config merge with all site-specific configs Validate YAML syntax before committing 10.4.3. Workflow Improvements Test workflow changes in fork before PR Ensure backward compatibility with existing triggers Document workflow parameters in comments Keep build times efficient (avoid unnecessary steps) 10.5. Code Review Process Create PR with descriptive title and description Ensure CI passes (lint workflow) Request review from maintainers Address feedback promptly Squash and merge after approval 10.6. Adding New Shared Features 10.6.1. Example: Adding a New Partial Create partial:\n\u003c!-- shared/layouts/partials/custom-feature.html --\u003e \u003cdiv class=\"custom-feature\"\u003e {{ .Inner }} \u003c/div\u003e Reference in base layout:\n\u003c!-- shared/layouts/partials/hooks/head-end.html --\u003e {{ partial \"custom-feature.html\" . }} Test across all sites:\nmake build-all Document usage:\nUpdate README with usage instructions\n10.7. Release Process 10.7.1. Versioning The orchestrator doesn’t use semantic versioning (continuous deployment), but significant changes should be documented:\nUpdate CHANGELOG.md (create if doesn’t exist)\nTag release for major changes:\ngit tag -a v2.0.0 -m \"Release v2.0.0: Multi-site orchestrator\" git push origin v2.0.0 10.7.2. Deprecation Policy When deprecating features:\nAnnounce deprecation in README and PR Keep deprecated feature for 1 month minimum Provide migration path in documentation Remove after grace period 10.8. Getting Help Issues: https://github.com/fchastanet/my-documents/issues Discussions: https://github.com/fchastanet/my-documents/discussions Documentation: This README and related guides 11. Documentation Structure The documentation is organized as follows:\ncontent/en/ ├── _index.html # Homepage └── docs/ ├── _index.md # Docs landing page ├── bash-scripts/ # Bash scripting guides ├── howtos/ # How-to guides ├── lists/ # Reference lists └── other-projects/ # Links to related projects 11.1. Adding New Documentation Create a Markdown file in the appropriate content/en/docs/ subdirectory Add frontmatter with title, description, and weight (for ordering) Save and Hugo will automatically rebuild the site Example:\n--- title: My New Page description: Brief description of the page weight: 10 --- Your content here... 12. Content Guidelines Keep Markdown files focused and well-organized Use ATX-style headers (#, ##, etc.) Line length: 120 characters maximum (enforced by mdformat) Line endings: LF only Use relative links for internal navigation Code blocks should specify language: ```bash ```, yaml, etc. 13. SEO Features This site includes the following SEO optimizations:\nStatic HTML pre-rendering for all content Automatic XML sitemap generation Responsive design and mobile-first approach Optimized page load performance Per-page metadata and structured data (JSON-LD) RSS feeds for content distribution Canonical URLs to prevent duplication Open Graph and Twitter card support Breadcrumb navigation with schema markup 14. CI/CD Pipelines 14.1. Build All Sites (build-all-sites.yml) Centralized orchestrator that:\nBuilds all documentation sites in parallel Merges base + site-specific configs using yq Deploys each site to its own GitHub Pages Triggers on push to master or via repository_dispatch 14.2. Hugo Build \u0026 Deploy (hugo-build-deploy.yml) Builds my-documents site only Validates build output Deploys to GitHub Pages automatically 14.3. Pre-commit Linting (lint.yml) Runs Markdown and code quality checks Auto-fixes formatting/linting issues Runs MegaLinter validation ","categories":["Documentation"],"description":"Overview of the technical architecture and implementation details of the My Documents documentation site built with Hugo","excerpt":"Overview of the technical architecture and implementation details of …","ref":"/my-documents/docs/my-documents/technical-architecture/","tags":["hugo","docsy","multi-site","documentation","static-site-generator","github-actions","ai-generated"],"title":"My Documents - Technical Architecture and Documentation Site Implementation"},{"body":" Trigger My-Documents Reusable Workflow 1. Overview 2. Quick Start 2.1. Basic Usage 3. How It Works 3.1. Architecture 3.2. Authentication Flow 4. Configuration 4.1. Input Parameters 4.2. Advanced Usage Examples 4.2.1. Custom Documentation URL 4.2.2. Different Target Repository 4.2.3. Manual Trigger with Custom Event Type 5. Complete Example 6. Secrets Configuration 6.1. In my-documents Repository 6.2. In Dependent Repositories 7. Understanding Secrets: Inherit and Access Control 7.1. What is secrets: inherit? 7.2. How Does It Work for Dependent Repositories? 7.3. Secret Access Hierarchy 7.4. Why This Workflow Can’t Be Used by Others 7.4.1. Reason 1: GitHub App is Organization-Specific 7.4.2. Reason 2: GitHub App Has No Access to Other Organizations 7.4.3. Reason 3: Secrets Are Repository-Specific 7.5. Practical Example: Why It Fails 7.6. How Someone Else Could Create Their Own Version 7.7. Summary: Why This Workflow is Fchastanet-Only 7.8. Conclusion 8. Workflow Outputs 8.1. Console Output 8.2. GitHub Actions Summary 9. Troubleshooting 9.1. Build Not Triggered 9.2. Authentication Failures 9.3. Workflow Not Found 9.4. Debug Mode 10. Migration Guide 10.1. From Old Trigger Workflow 11. Best Practices 11.1. Trigger Paths 11.2. Concurrency Control 11.3. Conditional Triggers 12. FAQ 12.1. Q: Do I need to configure secrets in my dependent repository? 12.2. Q: Can I test the workflow before merging to master? 12.3. Q: How long does documentation deployment take? 12.4. Q: Can I use this with my own organization? 12.5. Q: What if the build fails? 12.6. Q: Can I trigger builds for multiple repositories? 13. Related Documentation 14. Support Trigger My-Documents Reusable Workflow 1. Overview The trigger-docs-reusable.yml workflow is a reusable GitHub Actions workflow that enables dependent repositories (bash-compiler, bash-tools, bash-tools-framework, bash-dev-env) to trigger documentation builds in the centralized my-documents orchestrator.\nBenefits:\nNo secrets required in dependent repositories (GitHub handles authentication automatically) Centralized configuration - All authentication handled by GitHub App in my-documents Configurable - Override defaults for organization, repository, URLs, etc. Secure - Uses GitHub App authentication with automatic token expiration Simple integration - Just a few lines in dependent repo workflows 2. Quick Start 2.1. Basic Usage Create .github/workflows/trigger-docs.yml in your dependent repository:\n--- name: Trigger Documentation Build on: push: branches: [master] paths: - 'content/**' - 'static/**' - 'go.mod' - 'go.sum' workflow_dispatch: jobs: trigger-docs: uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master secrets: inherit That’s it! No secrets to configure, no tokens to manage.\n3. How It Works 3.1. Architecture ┌─────────────────────────┐ │ Dependent Repository │ │ (e.g., bash-compiler) │ │ │ │ Push to master branch │ │ ├─ content/** │ │ └─ static/** │ └────────────┬────────────┘ │ │ workflow_call ▼ ┌─────────────────────────────────────┐ │ my-documents Repository │ │ │ │ .github/workflows/ │ │ trigger-docs-reusable.yml │ │ │ │ ┌────────────────────────────────┐ │ │ │ 1. Generate GitHub App Token │ │ │ │ (using DOC_APP_ID secret) │ │ │ └────────────┬───────────────────┘ │ │ │ │ │ ┌────────────▼───────────────────┐ │ │ │ 2. Trigger repository_dispatch │ │ │ │ event in my-documents │ │ │ └────────────┬───────────────────┘ │ └───────────────┼─────────────────────┘ │ │ repository_dispatch ▼ ┌─────────────────────────────────────┐ │ my-documents Repository │ │ │ │ .github/workflows/ │ │ build-all-sites.yml │ │ │ │ Builds all 5 documentation sites │ │ Deploys to GitHub Pages │ └─────────────────────────────────────┘ 3.2. Authentication Flow Calling workflow runs in dependent repository context Reusable workflow executes in my-documents repository context GitHub App token generated using my-documents secrets: DOC_APP_ID - GitHub App ID DOC_APP_PRIVATE_KEY - GitHub App private key Token used to trigger repository_dispatch event Build workflow starts automatically in my-documents Security Benefits:\nNo PAT tokens needed in dependent repositories No secrets management in dependent repos Automatic token expiration (1 hour) Fine-grained permissions (Contents: write, Pages: write) Centralized audit trail 4. Configuration 4.1. Input Parameters All inputs are optional with sensible defaults:\nInput Description Default target_org Target organization/user fchastanet target_repo Target repository name my-documents event_type Repository dispatch event type trigger-docs-rebuild docs_url_base Documentation URL base https://fchastanet.github.io workflow_filename Workflow filename to monitor build-all-sites.yml source_repo Source repository ${{ github.repository }} (auto-detected if not provided) 4.2. Advanced Usage Examples 4.2.1. Custom Documentation URL jobs: trigger-docs: uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master with: docs_url_base: 'https://docs.example.com' secrets: inherit 4.2.2. Different Target Repository jobs: trigger-docs: uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master with: target_org: 'myOrg' target_repo: 'my-docs' workflow_filename: 'build-docs.yml' secrets: inherit 4.2.3. Manual Trigger with Custom Event Type jobs: trigger-docs: uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master with: event_type: 'custom-docs-rebuild' secrets: inherit 5. Complete Example Here’s a complete example for a dependent repository:\n--- name: Trigger Documentation Build on: # Trigger on content changes push: branches: [master] paths: - 'content/**' # Hugo content - 'static/**' # Static assets - 'go.mod' # Hugo modules - 'go.sum' # Hugo module checksums - 'configs/**' # If using custom configs # Allow manual triggering workflow_dispatch: # Trigger on releases release: types: [published] jobs: trigger-docs: name: Trigger Documentation Build uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master secrets: inherit 6. Secrets Configuration 6.1. In my-documents Repository The reusable workflow requires these secrets to be configured in the my-documents repository:\nSecret Description How to Get DOC_APP_ID GitHub App ID From GitHub App settings DOC_APP_PRIVATE_KEY GitHub App private key (PEM format) Generated when creating GitHub App Setting up secrets:\nGo to https://github.com/fchastanet/my-documents/settings/secrets/actions Add DOC_APP_ID with your GitHub App ID Add DOC_APP_PRIVATE_KEY with the private key content 6.2. In Dependent Repositories No secrets needed! The secrets: inherit directive allows the reusable workflow to access my-documents secrets when running.\n7. Understanding Secrets: Inherit and Access Control 7.1. What is secrets: inherit? secrets: inherit is a GitHub Actions feature that allows a reusable workflow to access repository secrets from the calling workflow’s repository when in the same repository context.\nImportant distinction:\nWhen a dependent repository (like bash-compiler) calls this reusable workflow with secrets: inherit:\njobs: trigger-docs: uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master secrets: inherit It means:\n“Pass any secrets from bash-compiler repository to the reusable workflow”\nNOT:\n“Pass secrets from my-documents to bash-compiler”\n7.2. How Does It Work for Dependent Repositories? The key to understanding this is the execution context:\nWorkflow file location: .github/workflows/trigger-docs-reusable.yml lives in my-documents Calling workflow location: .github/workflows/trigger-docs.yml lives in bash-compiler (or other dependent repo) Execution context: When bash-compiler calls the reusable workflow, the reusable workflow still runs in the my-documents context This means:\nThe reusable workflow has access to my-documents’ secrets, not bash-compiler’s secrets secrets: inherit tells the reusable workflow “use my (the calling repo’s) secrets if needed” But since the workflow runs in my-documents context, it automatically has access to my-documents’ secrets anyway 7.3. Secret Access Hierarchy GitHub Actions processes reusable workflows within the repository where they’re defined:\n┌────────────────────────────────────────────────────────────────────────────────────┐ │ bash-compiler repo │ │ │ │ .github/workflows/ │ │ trigger-docs.yml │ │ │ │ calls: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master │ │ secrets: inherit │ └───────────────────────────────────────┬────────────────────────────────────────────┘ │ │ workflow_call (context: my-documents) │ ▼ ┌─────────────────────────────────┐ │ my-documents repo │ │ (workflow context) │ │ │ │ .github/workflows/ │ │ trigger-docs-reusable.yml │ │ │ │ ✅ Can access: │ │ - DOC_APP_ID │ │ - DOC_APP_PRIVATE_KEY │ │ (my-documents secrets) │ │ │ │ ❌ Cannot directly access: │ │ - bash-compiler secrets │ └─────────────────────────────────┘ 7.4. Why This Workflow Can’t Be Used by Others This workflow is tightly coupled to the my-documents infrastructure:\n7.4.1. Reason 1: GitHub App is Organization-Specific The workflow uses DOC_APP_ID and DOC_APP_PRIVATE_KEY secrets that are:\nConfigured only in the my-documents repository Created from a GitHub App installed only on: fchastanet/my-documents fchastanet/bash-compiler fchastanet/bash-tools fchastanet/bash-tools-framework fchastanet/bash-dev-env If someone from outside this organization tries to use the workflow:\n# In their-org/their-repo jobs: trigger-docs: uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master secrets: inherit What happens:\nWorkflow starts in their-org/their-repo context (calling workflow) Reusable workflow executes in fchastanet/my-documents context Reusable workflow tries to access DOC_APP_ID and DOC_APP_PRIVATE_KEY These secrets don’t exist in their-repo, so secrets: inherit doesn’t provide them The workflow fails with authentication error Error: The variable has not been set, or it has been set to an empty string. Evaluating: secrets.DOC_APP_ID 7.4.2. Reason 2: GitHub App Has No Access to Other Organizations The GitHub App is installed only on specific fchastanet repositories:\nWhen workflow tries to trigger repository_dispatch in my-documents using the app token The token is only valid for repositories where the app is installed If someone tries to point it to their own my-documents fork, the app has no permission Error example:\nError: Resource not accessible by integration at https://api.github.com/repos/their-org/their-docs/dispatches 7.4.3. Reason 3: Secrets Are Repository-Specific GitHub Actions secrets are stored at three levels:\nLevel Scope Accessible By Repository Single repository Workflows in that repository only Environment Specific deployment environment Workflows targeting that environment Organization All repositories in organization All workflows in the organization My-documents secrets are stored at the repository level:\nOnly accessible to workflows executing in my-documents context Not accessible to workflows in other organizations Not inherited by other repositories, even if they call the reusable workflow 7.5. Practical Example: Why It Fails Scenario: User john forks my-documents to john/my-documents-fork and tries to use the workflow:\n# In john/bash-compiler (dependent repo fork) jobs: trigger-docs: uses: john/my-documents-fork/.github/workflows/trigger-docs-reusable.yml@master secrets: inherit Execution flow:\n1. bash-compiler workflow starts (context: john) ❌ john/my-documents-fork doesn't have DOC_APP_ID or DOC_APP_PRIVATE_KEY secrets 2. Reusable workflow starts (context: john/my-documents-fork) ❌ Tries to access secrets.DOC_APP_ID ❌ Secrets don't exist in john/my-documents-fork ❌ secrets: inherit doesn't help (no secrets in john/bash-compiler either) 3. GitHub App access attempt ❌ GitHub App not installed on john/my-documents-fork ❌ Authentication fails with 403 error 7.6. How Someone Else Could Create Their Own Version If someone wanted to use this pattern for their own orchestrator:\nCreate their own GitHub App\nIn their organization settings With Contents: write and Pages: write permissions Install on their repositories Set up secrets in their my-documents repository\nDOC_APP_ID = their-app-id DOC_APP_PRIVATE_KEY = their-private-key Create their own reusable workflow\nCopy and adapt the trigger-docs-reusable.yml Reference their own secrets Change target_org default to their organization Update dependent repositories\nPoint to their reusable workflow Use secrets: inherit in their calls Example for their-org:\n# In their-org/bash-compiler jobs: trigger-docs: uses: their-org/my-docs-orchestrator/.github/workflows/trigger-docs-reusable.yml@master secrets: inherit # This now references their-org's secrets, not fchastanet's 7.7. Summary: Why This Workflow is Fchastanet-Only Component Why It’s Fchastanet-Specific Can Be Generalized? Workflow logic Generic, reusable for any workflow ✅ Yes, with different inputs DOC_APP_ID secret Specific to fchastanet’s GitHub App ❌ No, organization-specific DOC_APP_PRIVATE_KEY secret Specific to fchastanet’s GitHub App ❌ No, organization-specific Target repository (default) Hardcoded to my-documents ✅ Yes, via target_repo input Target organization (default) Hardcoded to fchastanet ✅ Yes, via target_org input GitHub App installation Only on fchastanet repositories ❌ No, would need own app 7.8. Conclusion The secrets: inherit mechanism is elegant for internal workflows within an organization because:\nFor dependent repos in fchastanet: They can call the workflow without managing secrets (works perfectly) For external users: They cannot use this workflow as-is because the GitHub App and secrets are organization-specific This is intentional: It provides security and prevents unauthorized access to the build orchestration This is not a limitation but a security feature - the workflow is designed to work only within the fchastanet organization where the GitHub App is installed.\n8. Workflow Outputs The workflow provides rich output and summaries:\n8.1. Console Output 🔔 Triggering documentation build in fchastanet/my-documents... ✅ Successfully triggered docs build in fchastanet/my-documents 📖 Documentation will be updated at: https://fchastanet.github.io/bash-compiler/ ℹ️ Note: Documentation deployment may take 2-5 minutes 8.2. GitHub Actions Summary The workflow creates a detailed summary visible in the Actions UI:\n### 8.3. ✅ Documentation build triggered **Source Repository:** `fchastanet/bash-compiler` **Target Repository:** `fchastanet/my-documents` **Commit:** `abc123def456` **Triggered by:** `fchastanet` 🔗 [View build status](https://github.com/fchastanet/my-documents/actions/workflows/build-all-sites.yml) 📖 [View documentation](https://fchastanet.github.io/bash-compiler/) 9. Troubleshooting 9.1. Build Not Triggered Symptoms:\nWorkflow runs successfully but build doesn’t start HTTP 204 response but no activity in my-documents Possible Causes:\nGitHub App not installed on target repository\nSolution: Install the GitHub App on my-documents repository GitHub App permissions insufficient\nSolution: Ensure app has Contents: write permission Event type mismatch\nSolution: Verify event_type input matches what build-all-sites.yml expects 9.2. Authentication Failures Symptoms:\nHTTP 401 (Unauthorized) or 403 (Forbidden) errors “Resource not accessible by integration” error Possible Causes:\nSecrets not configured in my-documents\nSolution: Add DOC_APP_ID and DOC_APP_PRIVATE_KEY secrets GitHub App private key incorrect\nSolution: Regenerate private key in GitHub App settings GitHub App permissions revoked\nSolution: Reinstall GitHub App on repositories 9.3. Workflow Not Found Symptoms:\n“Unable to resolve action” error “Workflow file not found” error Possible Causes:\nWrong branch reference\nSolution: Use @master not @main (my-documents uses master branch) Workflow file renamed or moved\nSolution: Verify file exists at .github/workflows/trigger-docs-reusable.yml 9.4. Debug Mode Enable debug logging in dependent repository:\njobs: trigger-docs: uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master secrets: inherit Then enable debug logs in repository settings:\nGo to repository settings → Secrets and variables → Actions Add repository variable: ACTIONS_STEP_DEBUG = true Add repository variable: ACTIONS_RUNNER_DEBUG = true 10. Migration Guide 10.1. From Old Trigger Workflow If you’re migrating from the old PAT-based trigger workflow:\nOld approach (deprecated):\njobs: trigger: runs-on: ubuntu-latest steps: - name: Trigger my-documents build run: | curl -X POST \\ -H \"Authorization: token ${{ secrets.DOCS_BUILD_TOKEN }}\" \\ ... New approach (recommended):\njobs: trigger-docs: uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master secrets: inherit Benefits of migration:\n✅ Remove DOCS_BUILD_TOKEN secret from dependent repository ✅ Simpler workflow (3 lines vs 50+ lines) ✅ Centralized authentication ✅ Automatic token management ✅ Better security (GitHub App vs PAT) 11. Best Practices 11.1. Trigger Paths Only trigger on content changes to avoid unnecessary builds:\non: push: branches: [master] paths: - 'content/**' # Documentation content - 'static/**' # Static assets - 'go.mod' # Hugo modules (theme updates) - 'go.sum' Don’t trigger on:\nTest files CI configuration changes Source code changes (unless they affect docs) README updates (unless it’s documentation content) 11.2. Concurrency Control Prevent multiple concurrent builds:\njobs: trigger-docs: uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master secrets: inherit concurrency: group: docs-build-${{ github.ref }} cancel-in-progress: true 11.3. Conditional Triggers Only trigger for certain branches:\njobs: trigger-docs: if: github.ref == 'refs/heads/master' uses: fchastanet/my-documents/.github/workflows/trigger-docs-reusable.yml@master secrets: inherit 12. FAQ 12.1. Q: Do I need to configure secrets in my dependent repository? A: No! When using secrets: inherit, the reusable workflow can access secrets from my-documents repository.\n12.2. Q: Can I test the workflow before merging to master? A: Yes, add workflow_dispatch trigger and manually run it from the Actions tab.\n12.3. Q: How long does documentation deployment take? A: Typically 2-5 minutes:\nTrigger: ~5 seconds Build (all sites): ~60 seconds Deployment: ~1-3 minutes (GitHub Pages propagation) 12.4. Q: Can I use this with my own organization? A: Yes, override target_org and target_repo inputs. You’ll need to set up your own GitHub App.\n12.5. Q: What if the build fails? A: Check the build status link in the workflow summary. The trigger workflow will still succeed; failures happen in the build workflow.\n12.6. Q: Can I trigger builds for multiple repositories? A: Yes, create multiple jobs in your workflow, each calling the reusable workflow with different source_repo values.\n13. Related Documentation Multi-Site Orchestrator Architecture GitHub App Migration Guide Build All Sites Workflow 14. Support For issues or questions:\nCheck Troubleshooting section Review GitHub Actions logs Create an issue in my-documents repository ","categories":["Documentation"],"description":"Overview of the technical architecture and implementation details of the My Documents reusable workflow for triggering documentation builds","excerpt":"Overview of the technical architecture and implementation details of …","ref":"/my-documents/docs/my-documents/trigger-my-documents-workflow/","tags":["documentation","github-actions","reusable-workflow","github-app","authentication","secrets-management","ai-generated"],"title":"My Documents - Trigger Reusable Workflow Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/reusable-workflow/","tags":"","title":"Reusable-Workflow"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/scripts/","tags":"","title":"Scripts"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/secrets-management/","tags":"","title":"Secrets-Management"},{"body":"Static Site Generator Migration Analysis Date: 2026-02-17 Project: my-documents repository migration and multi-site consolidation Goal: Migrate from Docsify to an SEO-optimized static site generator while maintaining simplicity and GitHub CI compatibility\n1. Executive Summary This document evaluates the current Docsify setup and recommends alternative static site generators that provide superior SEO performance while maintaining the simplicity and ease of deployment that made Docsify attractive.\nCurrent Challenge: Docsify renders content client-side, which severely limits SEO capabilities and page load performance. This is critical for a documentation site seeking organic search visibility.\n2. Current Solution Analysis: Docsify 2.1. Current Configuration Type: Client-side SPA (Single Page Application) Deployment: Direct to GitHub Pages (no build step) Content Format: Markdown Theme: Simple Dark (customized) Search: Built-in search plugin Navigation: Manual sidebar and navbar configuration 2.2. Docsify Pros ✅ Advantage Impact Zero build step required Instant deployment, minimal CI/CD complexity Simple file structure Easy to add new documentation files No dependencies to manage Fewer security concerns, simpler setup Client-side rendering Works directly with GitHub Pages Lightweight theme system Easy customization with CSS Good for technical audience Fast navigation for users familiar with SPAs Markdown-first Natural for technical documentation 2.3. Docsify Cons ❌ Limitation Impact Client-side rendering Poor SEO - Search engines struggle to index content No static HTML No pre-rendered pages for crawlers JavaScript dependent Requires JS in browser (security consideration) Limited meta tags control Difficult to optimize individual pages for SEO Slow initial page load JavaScript bundle must load first No built-in sitemap Manual sitemap generation needed No RSS/feed support Hard to distribute content Search plugin limitations Site search not indexed by external search engines No static asset optimization All images referenced as relative paths Outdated dependency stack Uses Vue 2 (Vue 3 available), jQuery, legacy patterns 2.4. Docsify SEO Score Current Estimate: 2/10 ⛔\n❌ No static pre-rendered HTML ❌ Robot.txt and sitemap not automatically generated ❌ Limited per-page meta tag control ❌ No automatic JSON-LD schema generation ❌ Poor mobile-first Core Web Vitals (JS-heavy) ⚠️ Possible crawl budget waste ⚠️ Delayed indexing (content hidden until JS loads) 3. Recommended Migration Path 3.1. Phase 1: Evaluation (This Phase) Compare alternatives against criteria Identify best fit for multi-site architecture Plan migration strategy 3.2. Phase 2: Pilot Set up new solution with one repository Migrate content and test Validate SEO improvements 3.3. Phase 3: Full Migration Migrate remaining repositories Set up CI/CD pipeline Monitor performance metrics 3.4. Phase 4: Optimization Fine-tune SEO settings Implement analytics Monitor search engine indexing 4. Alternative Solutions Comparison 4.1. Option 1: Hugo ⭐⭐⭐⭐⭐ (RECOMMENDED) Type: Go-based static site generator Build Time: \u003c1s for most sites Theme System: Flexible with 500+ themes\n4.1.1. Pros ✅ Extremely fast compilation - Processes 1000+ pages in milliseconds Excellent for documentation - Purpose-built with documentation sites in mind Superior SEO support - Generates static HTML, sitemaps, feeds, schemas Simple setup - Single binary, no dependency hell Markdown + frontmatter - Natural upgrade from Docsify GitHub Actions ready - Hugo orb/actions available for CI/CD Responsive themes - Many documentation-specific themes (Docsy, Relearn, Book) Built-in features - Search indexes, RSS feeds, JSON-LD support Content organization - Hierarchical content structure with archetypes Output optimization - Image processing, minification, CSS purging Flexible routing - Customize URLs, create custom taxonomies Active community - Large ecosystem, frequent updates Multi-language support - Built-in i18n capability 4.1.2. Cons ❌ Learning curve for Go templating (shortcodes, partials) Theme customization requires understanding Hugo’s page model Configuration in TOML/YAML (minor, but different from Docsify) Less visual for live preview compared to Docsify 4.1.3. SEO Score: 9/10 ✅ ✅ Static HTML pre-rendering ✅ Automatic sitemap generation ✅ Per-page meta tags and structured data ✅ RSS/Atom feeds ✅ Canonical URLs ✅ Image optimization ✅ Performance optimizations (minification, compression) ⚠️ JSON-LD not automated (requires theme customization) 4.1.4. GitHub CI/CD Integration # .github/workflows/deploy.yml example - uses: peaceiris/actions-hugo@v2 with: hugo-version: 'latest' extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public 4.1.5. Migration Effort Content: Minimal - Markdown stays same, just add frontmatter Structure: Organize into content sections (easy mapping from Docsify) Navigation: Automatic from directory structure or config Customization: Moderate - Theme customization required 4.1.6. Recommended Themes Docsy - Google-created, excellent documentation theme, built-in search Relearn - MkDocs-inspired, sidebar navigation like Docsify Book - Clean, minimal, perfect for tutorials Geek Docs - Modern, fast, developer-friendly 4.1.7. Best For ✅ Technical documentation ✅ Multi-site architecture ✅ SEO-critical sites ✅ GitHub Pages deployment ✅ Content-heavy sites (1000+ pages)\n4.2. Option 2: Astro ⭐⭐⭐⭐ Type: JavaScript/TypeScript-based, island architecture Build Time: \u003c2s typical Theme System: Component-based\n4.2.1. Pros ✅ Outstanding SEO support - Static HTML generation, built-in meta tag management Zero JavaScript by default - Only JS needed for interactive components Modern stack - Latest JavaScript patterns, TypeScript support Markdown + MDX support - Markdown with embedded React/Vue components Component imports - Use React, Vue, Svelte components in Markdown Fast performance - Island architecture means minimal JS shipping Great for blogs/docs - Built-in content collections API Image optimization - Automatic image processing and responsive images Built-in integrations - Readily available for analytics, fonts, CSS Flexible deployment - Works with any static host or serverless TypeScript first - Better tooling and IDE support Vite-based - Fast HMR and builds 4.2.2. Cons ❌ Newer ecosystem (less battle-tested than Hugo) Small learning curve with Astro-specific patterns Requires Node.js and npm (dependency management) Theme ecosystem smaller than Hugo MDX adds complexity if not needed 4.2.3. SEO Score: 9/10 ✅ ✅ Static HTML pre-rendering ✅ Fine-grained meta tag control ✅ JSON-LD schema support ✅ Automatic sitemap generation ✅ RSS/feed support ✅ Image optimization with AVIF ✅ Open Graph and Twitter cards ✅ Performance metrics built-in 4.2.4. GitHub CI/CD Integration # .github/workflows/deploy.yml example - name: Install dependencies run: npm ci - name: Build run: npm run build - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./dist 4.2.5. Migration Effort Content: Minimal - Markdown compatible with optional frontmatter Structure: Convert to Astro collections (straightforward) Navigation: Can use auto-generated from file structure Customization: Moderate - Components offer more flexibility than Hugo 4.2.6. Recommended Themes/Templates Starlight - Official Astro docs template, excellent for documentation Docs Kit - Tailored for technical documentation Astro Paper - Blog-focused, highly customizable 4.2.7. Best For ✅ Modern tech stack preference ✅ Need for interactive components ✅ TypeScript-heavy teams ✅ Blogs + Documentation hybrid ✅ SEO + Performance critical\n4.3. Option 3: 11ty (Eleventy) ⭐⭐⭐⭐ Type: JavaScript template engine Build Time: \u003c1s typical Theme System: Template-based\n4.3.1. Pros ✅ Incredibly flexible - Supports multiple template languages (Markdown, Nunjucks, Liquid, etc.) Lightweight - Minimal opinion on structure, you decide Fast builds - Blazingly fast incremental builds JavaScript-based - Easier for Node.js teams than Go Markdown-first - Natural Markdown support with plugins No locked-in framework - Use vanilla HTML/CSS or any framework Great community - Excellent documentation and starter projects Simple config - .eleventy.js is readable JavaScript Content collections - Flexible ways to organize content Image processing - Built-in with popular plugins GitHub Pages friendly - Easy integration with GitHub Actions Low barrier to entry - Understand JavaScript, you understand Eleventy 4.3.2. Cons ❌ Less opinionated (requires more configuration) Smaller pre-built theme ecosystem Requires JavaScript knowledge for customization No built-in search (needs separate solution) Learning curve steeper if unfamiliar with template languages 4.3.3. SEO Score: 8/10 ✅ ✅ Static HTML generation ✅ Manual sitemap generation (simple plugin) ✅ Per-page meta tag control ✅ Feed/RSS support (via plugins) ✅ Image optimization (via plugins) ⚠️ Schema/JSON-LD (requires custom implementation) 4.3.4. GitHub CI/CD Integration # .github/workflows/deploy.yml example - name: Install dependencies run: npm ci - name: Build run: npm run build - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./_site 4.3.5. Migration Effort Content: Minimal - Markdown files work as-is Structure: Very flexible, custom folder organization Navigation: Can auto-generate from structure or manually configure Customization: High - Maximum control but more work 4.3.6. Recommended Starters 11ty Base Blog - Simple starting point Eleventy High Performance Blog - Performance-focused Slinkity - Hybrid with component support 4.3.7. Best For ✅ Developers who want full control ✅ Simple, focused documentation ✅ JavaScript/Node.js teams ✅ Performance optimization focus ✅ Unique design requirements\n4.4. Option 4: VuePress 2 ⭐⭐⭐ Type: Vue 3-based static site generator Build Time: 1-2s typical Theme System: Vue components\n4.4.1. Pros ✅ Vue ecosystem - Use Vue components directly in Markdown Documentation-first - Built specifically for docs Markdown extensions - Plugin system for custom Markdown syntax Built-in search - Local search with Algolia option Plugin ecosystem - Rich ecosystem for docs sites Good themes - VuePress Theme Default is solid PWA support - Can work offline (if configured) Git history - Can show last edited time from git i18n built-in - Multi-language support Flexible routing - Customizable URL structure 4.4.2. Cons ❌ Vue knowledge required Smaller ecosystem than Hugo Heavy JavaScript bundle (not as optimized as Astro) Less mature than Hugo Configuration can be verbose Search indexing still client-side primarily 4.4.3. SEO Score: 6/10 ⚠️ ✅ Static HTML generation ✅ Per-page meta tags ✅ Sitemap support (via plugin) ⚠️ Search still somewhat client-side ⚠️ Performance not optimized (Vue overhead) ⚠️ JSON-LD requires manual setup 4.4.4. GitHub CI/CD Integration - name: Install dependencies run: npm ci - name: Build run: npm run build - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./dist 4.4.5. Migration Effort Content: Minimal - Markdown compatible Structure: Organized in .vuepress/config.js Navigation: Configured in sidebar/navbar config Customization: Moderate - Vue components for complex needs 4.4.6. Best For ✅ Vue-centric teams ✅ Need interactive components ✅ Plugin-heavy customization ✅ Smaller documentation sites ✅ Already using Vue ecosystem\n4.5. Option 5: MkDocs ⭐⭐⭐ Type: Python-based documentation generator Build Time: \u003c1s typical Theme System: Python template-based\n4.5.1. Pros ✅ Documentation-optimized - Built by documentation enthusiasts Simple configuration - mkdocs.yml is straightforward Markdown-native - Pure Markdown with extensions Great themes - Material for MkDocs is excellent Low overhead - Minimal learning curve Python-based - Good for Python-heavy teams Fast builds - Quick incremental rebuilds Search integration - Good local search, Algolia-ready Git integration - Edit on GitHub features Active community - Good documentation and examples 4.5.2. Cons ❌ Python dependency management Smaller ecosystem than Hugo Theme customization requires Python knowledge Less flexibility than some alternatives Setup requires Python environment 4.5.3. SEO Score: 7/10 ✅ ✅ Static HTML generation ✅ Per-page meta tags ✅ Sitemap support (via plugin) ⚠️ Schema/JSON-LD minimal ⚠️ Image optimization requires external tools 4.5.4. GitHub CI/CD Integration - name: Set up Python uses: actions/setup-python@v4 with: python-version: '3.11' - name: Install dependencies run: pip install mkdocs mkdocs-material - name: Build run: mkdocs build - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./site 4.5.5. Migration Effort Content: Minimal - Markdown files work directly Structure: Configured in mkdocs.yml Navigation: Simple hierarchical structure Customization: Easy for theming, harder for core customization 4.5.6. Best For ✅ Documentation-only focus ✅ Python-familiar teams ✅ Minimal configuration needed ✅ Material design preference ✅ Rapid setup priority\n4.6. Option 6: Next.js / Vercel ⭐⭐ Type: React meta-framework Build Time: 5-10s typical Theme System: React components\n4.6.1. Pros ✅ Powerful frameworks - React + Node.js backend possibility Vercel optimization - Vercel specialist optimization React ecosystem - Access to millions of components SSR capable - Server-side rendering if needed API routes - Can add serverless functions Image optimization - Automatic image optimization Incremental Static Regeneration - Change content without full rebuild TypeScript native - First-class TypeScript support Performance monitoring - Web vitals built-in 4.6.2. Cons ❌ Overkill for static docs - Too much complexity Learning curve steep - React + Next.js knowledge required Build times longer - Slower than purpose-built SSGs More dependencies - Dependency management complexity GitHub Pages less ideal - Optimized for Vercel deployment Maintenance burden - React team required to maintain 4.6.3. SEO Score: 8/10 ✅ ✅ Static generation capability ✅ Per-page meta tags via next/head ✅ Sitemap and robots.txt support ✅ Image optimization ⚠️ Requires more configuration ⚠️ Slower builds than dedicated SSGs 4.6.4. GitHub CI/CD Integration (Docsify level: Complex) - name: Install dependencies run: npm ci - name: Build run: npm run build - name: Static Export run: npm run export - name: Deploy uses: peaceiris/actions-gh-pages@v3 4.6.5. Migration Effort Content: Moderate - Convert to Next.js structure Structure: Pages directory structure required Navigation: Custom component creation Customization: High complexity 4.6.6. Best For ✅ React-centric teams ✅ Need dynamic functionality ✅ Willing to deploy on Vercel ✅ Complex sites with interactive elements ❌ NOT recommended for pure documentation\n4.7. Option 7: Gatsby ⭐⭐ Type: React-based static site generator Build Time: 10-30s typical Theme System: React components + theme shadowing\n4.7.1. Pros ✅ Powerful plugin system - Huge ecosystem GraphQL querying - Flexible content queries Performance optimization - Good performance features React components - Full React power available CMS integration - Works with many headless CMS 4.7.2. Cons ❌ Heavy and slow - Longest build times of alternatives High complexity - Steep learning curve Dependency bloat - Many dependencies to maintain Not ideal for docs - Over-engineered for simple documentation GitHub Pages unfriendly - Best with Netlify Overkill - Too much power for static docs 4.7.3. SEO Score: 7/10 ✅ ✅ Static generation ✅ Good plugin ecosystem for SEO ⚠️ Heavy JavaScript overhead ⚠️ Slower builds 4.7.4. Best For ❌ NOT recommended for documentation migration\n5. Comparison Matrix Criteria Hugo Astro 11ty VuePress MkDocs Next.js Gatsby SEO Score 9/10 9/10 8/10 6/10 7/10 8/10 7/10 Build Speed ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐ Learning Curve ⭐⭐⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐ ⭐⭐⭐⭐ ⭐ ⭐ Customization ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ GitHub Pages ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐ Static Output ✅ ✅ ✅ ✅ ✅ ✅ ✅ Documentation Focus ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐ ⭐⭐ Theme Ecosystem ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ Community Size ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ GitHub Pages Native ✅ ✅ ✅ ✅ ✅ ⚠️ ❌ Multiple Sites ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐ ⭐ 6. Improvements for New Solutions Regardless of which SSG is chosen, implement these SEO improvements:\n6.1. Technical SEO Baseline Generate robots.txt automatically Generate XML sitemap automatically Implement per-page meta tags (title, description) Add canonical URLs to prevent duplication Implement JSON-LD schema (Article, BreadcrumbList, Organization) Open Graph and Twitter card meta tags Mobile-first responsive design Fast page load (Core Web Vitals: LCP, CLS, FID) Image optimization and lazy loading Minify and compress assets 6.2. Content Structure Implement breadcrumb navigation (visual + schema) Hierarchical heading structure (H1, H2, H3) Internal linking strategy Related content suggestions Table of contents for long articles Reading time estimates Last updated timestamps 6.3. Performance Optimizations Code splitting and lazy loading Image optimization (WebP, AVIF formats) CSS/JS minification Critical CSS inline Service worker for offline access Asset caching strategies Compression (gzip/brotli) CDN integration 6.4. Search and Indexing Submit sitemap to Google Search Console Monitor indexing status Fix crawl errors Optimize Core Web Vitals Monitor search appearance (ratings, rich results) Use Google Search Console to identify improvements 6.5. Advanced SEO Implement full-text search with ranking Add RSS/Atom feeds for content discovery Implement structured data for articles Add FAQ schema for common questions Breadcrumb schema implementation Organization/website schema Add “edit on GitHub” links for engagement signals 6.6. Analytics and Monitoring Google Analytics 4 integration Search Console monitoring Core Web Vitals tracking Error tracking (Sentry/similar) Performance monitoring dashboard Keyword ranking tracking Traffic Analysis 6.7. GitHub CI/CD Improvements Semantic versioning for releases Link checker in CI pipeline SEO audit in CI (Lighthouse, lighthouse-ci) Spell checker (already implemented) Broken internal link detection Mobile-first testing Accessibility testing (a11y) Build time monitoring Automated lighthouse reports 7. Hugo-Specific Recommendations If Hugo is chosen (recommended), implement:\n# config.yaml example improvements params: description: \"Collection of my documents on various subjects\" keywords: \"bash,best practices,learn,docker,jenkins\" openGraph: enabled: true twitterCards: enabled: true jsonLD: enabled: true outputs: home: - HTML - JSON - RSS section: - HTML - JSON - RSS taxonomies: category: categories tag: tags mediaTypes: application/json: suffixes: - json outputFormats: JSON: isPlainText: true mediaType: application/json 8. Astro-Specific Recommendations If Astro is chosen, implement:\n// astro.config.mjs example improvements export default defineConfig({ integrations: [ sitemap(), robotsTxt(), react(), vue(), ], image: { remotePatterns: [{ protocol: \"https\" }], }, vite: { plugins: [ sitemap(), ], }, }); 9. Migration Strategy for Multiple Sites 9.1. With Hugo (Recommended Approach) github-sites-monorepo/ ├── myDocuments/ │ ├── content/ │ ├── themes/ │ └── config.yaml ├── bashToolsFramework/ │ ├── content/ │ ├── themes/ │ └── config.yaml ├── bashTools/ │ ├── content/ │ ├── themes/ │ └── config.yaml └── bashCompiler/ ├── content/ ├── themes/ └── config.yaml CI/CD Strategy:\nSingle workflow builds all sites Each site has separate output directory Deploy to respective GitHub Pages branches Shared theme for consistency (git submodule or package) Single dependency management file 10. Risk Assessment and Mitigation Risk Hugo Astro 11ty MkDocs VuePress Breaking changes ⚠️ Low ⚠️ Medium ✅ Low ✅ Low ⚠️ Medium Ecosystem longevity ✅ Very High ⚠️ High ✅ Very High ✅ High ⚠️ Medium Theme support ✅ Excellent ⚠️ Good ⚠️ Good ✅ Good ⚠️ Good GitHub Pages ✅ Perfect ✅ Perfect ✅ Perfect ✅ Perfect ⚠️ Works Team skills ⚠️ Go required ⚠️ JS required ✅ JS (low level) ✅ Python/Markdown ⚠️ Vue required Maintenance burden ✅ Low ⚠️ Medium ⚠️ Medium ✅ Low ⚠️ Medium 11. Final Recommendation: Hugo 11.1. Justification SEO Excellence - 9/10 score meets all objectives Simplicity - Single Go binary, no dependency management Performance - \u003c1s builds, scales to thousands of pages Documentation-First - Built for exactly this use case GitHub Pages Native - Zero friction deployment Multi-Site Scalability - Perfect for multiple repositories Community - Largest documentation site generator community Proven - 1000+ major documentation sites use it Themes - Docsy, Relearn excellent for technical docs Future-Proof - Stable, active development 11.2. Hugo Implementation Plan Phase 1: Setup (1-2 weeks)\nInstall Hugo and select Docsy or Relearn theme Create content structure Configure SEO baseline Set up GitHub Actions workflow Test locally Phase 2: Migration (2-3 weeks)\nConvert Markdown files (minimal changes) Migrate sidebar structure to Hugo config Update internal links Test all links and navigation Performance testing Phase 3: SEO Optimization (1-2 weeks)\nImplement schema markup Configure sitemaps and feeds Submit to Google Search Console Baseline performance metrics Optimize Core Web Vitals Phase 4: Deployment (1 week)\nValidate all tests pass Deploy to production Monitor indexing and performance Gather feedback 12. Alternative: Astro for Modern Setup If your team prefers JavaScript/TypeScript and wants maximum flexibility with modern tooling, Astro with Starlight is the secondary recommendation:\nExcellent SEO (equal to Hugo) More flexible for custom components Modern JavaScript ecosystem Better DX with TypeScript Slightly longer build times acceptable GitHub Pages deployment straightforward 13. NOT Recommended ❌ Docsify - Keep for simple internal documentation only, not public sites ❌ Next.js - Overcomplicated for documentation, not ideal for GitHub Pages ❌ Gatsby - Slow builds, high complexity, deprecated 14. Conclusion Migrate to Hugo with Docsy theme for optimal balance of simplicity, SEO performance, and documentation focus. This will:\nImprove SEO from 2/10 to 9/10 Reduce page load times significantly Provide static pre-rendered pages for crawlers Scale to multiple sites easily Maintain simplicity in CI/CD Future-proof your documentation infrastructure Next Steps:\nReview this analysis with relevant stakeholders Set up pilot Hugo site with one repository Validate SEO improvements with Search Console Plan full migration timeline Document Hugo best practices for team ","categories":["Brainstorming"],"description":"Analysis of migrating from Docsify to an SEO-optimized static site generator","excerpt":"Analysis of migrating from Docsify to an SEO-optimized static site …","ref":"/my-documents/docs/brainstorming/static-site-generation/","tags":"development, documentation, seo]","title":"Static Site Generation Migration Analysis"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/static-site-generator/","tags":"","title":"Static-Site-Generator"},{"body":"","categories":"","description":"","excerpt":"","ref":"/my-documents/tags/","tags":"","title":"Tags"}]